,title,abstract,journal,link,category
312,Public Health and access to medicine. Pharmaceutical industry's role,"Every year, 10 million people die from lack of access to treatment for
curable diseases, specially in developing countries. Meanwhile, legal but
unsafe drugs cause 130 thousand deaths per year. How can this be happening in
21st Century? What role does the pharmaceutical industry play in this tragedy?
In this research, WHO reports are analyzed and primary information gathered so
as to answer this questions.",ACM Transactions on Economics and Computation,http://arxiv.org/abs/1901.02384v1,cs.AI
584,"Coevolution of deception and preferences: Darwin and Nash meet
  Machiavelli","We develop a framework in which individuals' preferences coevolve with their
abilities to deceive others about their preferences and intentions.
Specifically, individuals are characterised by (i) a level of cognitive
sophistication and (ii) a subjective utility function. Increased cognition is
costly, but higher-level individuals have the advantage of being able to
deceive lower-level opponents about their preferences and intentions in some of
the matches. In the remaining matches, the individuals observe each other's
preferences. Our main result shows that, essentially, only efficient outcomes
can be stable. Moreover, under additional mild assumptions, we show that an
efficient outcome is stable if and only if the gain from unilateral deviation
is smaller than the effective cost of deception in the environment.","ACM Transactions on Knowledge Discovery from Data
 ",http://arxiv.org/abs/2006.15308v1,cs.LG
277,"Characterizing Shadow Price via Lagrangian Multiplier for Nonsmooth
  Problem","In this paper, a relation between shadow price and the Lagrangian multiplier
for nonsmooth problem is explored. It is shown that the Lagrangian Multiplier
is the upper bound of shadow price for convex optimization and a class of
Lipschtzian optimizations. This work can be used in shadow pricing for
nonsmooth situation. The several nonsmooth functions involved in this class of
Lipschtzian optimizations is listed. Finally, an application to electricity
pricing is discussed.",ACS Energy Letters,http://arxiv.org/abs/1905.13622v2,econ.EM
573,Model instability in predictive exchange rate regressions,"In this paper we aim to improve existing empirical exchange rate models by
accounting for uncertainty with respect to the underlying structural
representation. Within a flexible Bayesian non-linear time series framework,
our modeling approach assumes that different regimes are characterized by
commonly used structural exchange rate models, with their evolution being
driven by a Markov process. We assume a time-varying transition probability
matrix with transition probabilities depending on a measure of the monetary
policy stance of the central bank at the home and foreign country. We apply
this model to a set of eight exchange rates against the US dollar. In a
forecasting exercise, we show that model evidence varies over time and a model
approach that takes this empirical evidence seriously yields improvements in
accuracy of density forecasts for most currency pairs considered.",Acta Physica Polonica A,http://arxiv.org/abs/1811.08818v2,physics.data-an
317,"Econophysics of Asset Price, Return and Multiple Expectations","This paper describes asset price and return disturbances as result of
relations between transactions and multiple kinds of expectations. We show that
disturbances of expectations can cause fluctuations of trade volume, price and
return. We model price disturbances for transactions made under all types of
expectations as weighted sum of partial price and trade volume disturbances for
transactions made under separate kinds of expectations. Relations on price
allow present return as weighted sum of partial return and trade volume
""return"" for transactions made under separate expectations. Dependence of price
disturbances on trade volume disturbances as well as dependence of return on
trade volume ""return"" cause dependence of volatility and statistical
distributions of price and return on statistical properties of trade volume
disturbances and trade volume ""return"" respectively.",Actual Problems of Economics,http://arxiv.org/abs/1901.05024v2,econ.GN
443,Game Theoretic Consequences of Resident Matching,"The resident matching algorithm, Gale-Shapley, currently used by SF Match and
the National Residency Match Program, has been in use for over 50 years without
fundamental alteration. The algorithm is a 'stable-marriage' method that favors
applicant outcomes. However, in these 50 years, there has been a big shift in
the supply and demand of applicants and programs. These changes along with the
way the Match is implemented have induced a costly race among applicants to
apply and interview at as many programs as possible. Meanwhile programs also
incur high costs as they maximize their probability of matching by interviewing
as many candidates as possible.",Actual Problems of Economics,http://arxiv.org/abs/2003.07205v1,econ.GN
119,Efficiency in Micro-Behaviors and FL Bias,"In this paper, we propose a model which simulates odds distributions of
pari-mutuel betting system under two hypotheses on the behavior of bettors: 1.
The amount of bets increases very rapidly as the deadline for betting comes
near. 2. Each bettor bets on a horse which gives the largest expectation value
of the benefit. The results can be interpreted as such efficient behaviors do
not serve to extinguish the FL bias but even produce stronger FL bias.","Actual Problems of Economics 
 ",http://arxiv.org/abs/1805.04225v1,econ.GN
178,"Modeling the residential electricity consumption within a restructured
  power market","The United States' power market is featured by the lack of judicial power at
the federal level. The market thus provides a unique testing environment for
the market organization structure. At the same time, the econometric modeling
and forecasting of electricity market consumption become more challenging.
Import and export, which generally follow simple rules in European countries,
can be a result of direct market behaviors. This paper seeks to build a general
model for power consumption and using the model to test several hypotheses.",Advances in Complex Systems,http://arxiv.org/abs/1805.11138v2,cs.GT
431,Bilateral Tariffs Under International Competition,"This paper explores the gain maximization problem of two nations engaging in
non-cooperative bilateral trade. Probabilistic model of an exchange of
commodities under different price systems is considered. Volume of commodities
exchanged determines the demand each nation has over the counter party's
currency. However, each nation can manipulate this quantity by imposing a
tariff on imported commodities. As long as the gain from trade is determined by
the balance between imported and exported commodities, such a scenario results
in a two party game where Nash equilibrium tariffs are determined for various
foreign currency demand functions and ultimately, the exchange rate based on
optimal tariffs is obtained.",Advances in Econometrics ,http://arxiv.org/abs/2001.02426v1,math.ST
302,"A possible alternative evaluation method for the non-use and nonmarket
  values of ecosystem services","Monetization of the non-use and nonmarket values of ecosystem services is
important especially in the areas of environmental cost-benefit analysis,
management and environmental impact assessment. However, the reliability of
valuation estimations has been criticized due to the biases that associated
with methods like the popular contingent valuation method (CVM). In order to
provide alternative valuation results for comparison purpose, we proposed the
possibility of using a method that incorporates fact-based costs and contingent
preferences for evaluating non-use and nonmarket values, which we referred to
as value allotment method (VAM). In this paper, we discussed the economic
principles of VAM, introduced the performing procedure, analyzed assumptions
and potential biases that associated with the method and compared VAM with CVM
through a case study in Guangzhou, China. The case study showed that the VAM
gave more conservative estimates than the CVM, which could be a merit since CVM
often generates overestimated values. We believe that this method can be used
at least as a referential alternative to CVM and might be particularly useful
in assessing the non-use and nonmarket values of ecosystem services from
human-invested ecosystems, such as restored ecosystems, man-made parks and
croplands.",Advances in Econometrics Volume B ,http://arxiv.org/abs/1811.08376v1,econ.EM
610,"Generational political dynamics of retirement pensions systems: An agent
  based model","The increasing difficulties in financing the welfare state and in particular
public retirement pensions have been one of the outcomes both of the decrease
of fertility and birth rates combined with the increase of life expectancy. The
dynamics of retirement pensions are usually studied in Economics using
overlapping generation models. These models are based on simplifying
assumptions like the use of a representative agent to ease the problem of
tractability. Alternatively, we propose to use agent-based modelling (ABM),
relaxing the need for those assumptions and enabling the use of interacting and
heterogeneous agents assigning special importance to the study of
inter-generational relations. We treat pension dynamics both in economics and
political perspectives. The model we build, following the ODD protocol, will
try to understand the dynamics of choice of public versus private retirement
pensions resulting from the conflicting preferences of different agents but
also from the cooperation between them. The aggregation of these individual
preferences is done by voting. We combine a microsimulation approach following
the evolution of synthetic populations along time, with the ABM approach
studying the interactions between the different agent types. Our objective is
to depict the conditions for the survival of the public pensions system
emerging from the relation between egoistic and altruistic individual and
collective behaviours.",Advances in Neural Information Processing Systems,http://arxiv.org/abs/1909.08706v1,cs.GT
487,"Pink Work: Same-Sex Marriage, Employment and Discrimination","This paper analyzes how the legalization of same-sex marriage in the U.S.
affected gay and lesbian couples in the labor market. Results from a
difference-in-difference model show that both partners in same-sex couples were
more likely to be employed, to have a full-time contract, and to work longer
hours in states that legalized same-sex marriage. In line with a theoretical
search model of discrimination, suggestive empirical evidence supports the
hypothesis that marriage equality led to an improvement in employment outcomes
among gays and lesbians and lower occupational segregation thanks to a decrease
in discrimination towards sexual minorities.","Advances in Neural Information Processing Systems
",http://arxiv.org/abs/1807.06698v1,stat.ML
84,Selling Wind,"We offer a parsimonious model to investigate how strategic wind producers
sell energy under stochastic production constraints, where the extent of
heterogeneity of wind energy availability varies according to wind farm
locations. The main insight of our analysis is that increasing heterogeneity in
resource availability improves social welfare, as a function of its effects
both on improving diversification and on reducing withholding by firms. We show
that this insight is quite robust for any concave and downward-sloping inverse
demand function. The model is also used to analyze the effect of heterogeneity
on firm profits and opportunities for collusion. Finally, we analyze the
impacts of improving public information and weather forecasting; enhanced
public forecasting increases welfare, but it is not always in the best
interests of strategic producers.",American Economic Journal of Microeconomics,http://arxiv.org/abs/1812.11420v1,econ.TH
165,"Sion's mini-max theorem and Nash equilibrium in a five-players game with
  two groups which is zero-sum and symmetric in each group","We consider the relation between Sion's minimax theorem for a continuous
function and a Nash equilibrium in a five-players game with two groups which is
zero-sum and symmetric in each group. We will show the following results.
  1. The existence of Nash equilibrium which is symmetric in each group implies
Sion's minimax theorem for a pair of playes in each group. 2. Sion's minimax
theorem for a pair of playes in each group imply the existence of a Nash
equilibrium which is symmetric in each group.
  Thus, they are equivalent. An example of such a game is a relative profit
maximization game in each group under oligopoly with two groups such that firms
in each group have the same cost functions and maximize their relative profits
in each group, and the demand functions are symmetric for the firms in each
group.",American Economic Review ,http://arxiv.org/abs/1809.02466v1,econ.EM
445,Bemerkungen zum paarweisen Vergleich,"The simple pairwise comparison is a method to provide different criteria with
weights. We show that the values of those weights (in particular the maximum)
depend just on the number of criteria. Additionally, it is shown that the
distance between the weights is always the same.
  -----
  Der einfache paarweise Vergleich ist ein Verfahren verschiedene Kriterien mit
einer Gewichtung zu versehen. Wir zeigen, dass die Werte dieser Gewichte
(insbesondere auch der maximale Wert) ausschlie{\ss}lich von der Anzahl der
Kriterien abh\""angt. Dar\""uber hinaus wird gezeigt, dass der Abstand der
Gewichtungen stets gleich ist.",American Journal of Criminal Justice,http://arxiv.org/abs/2003.10978v1,stat.OT
316,Designing An Industrial Policy For Developing Countries: A New Approach,"In this study, the prevalent methodology for design of the industrial policy
in developing countries was critically assessed, and it was shown that the
mechanism and content of classical method is fundamentally contradictory to the
goals and components of the endogenous growth theories. This study, by
proposing a new approach, along settling Schumpeter's economic growth theory as
a policy framework, designed the process of entering, analyzing and processing
data as the mechanism of the industrial policy in order to provide ""theoretical
consistency"" and ""technical and Statistical requirements"" for targeting the
growth stimulant factor effectively.",American Mathematical Monthly,http://arxiv.org/abs/1901.04265v1,econ.TH
320,The Economic Complexity of US Metropolitan Areas,"We calculate measures of economic complexity for US metropolitan areas for
the years 2007-2015 based on industry employment data. We show that the concept
of economic complexity translates well from the cross-country to the regional
setting, and is able to incorporate local as well as traded industries. The
largest cities and the Northeast of the US have the highest average complexity,
while traded industries are more complex than local-serving ones on average,
but with some exceptions. On average, regions with higher complexity have a
higher income per capita, but those regions also were more affected by the
financial crisis. Finally, economic complexity is a significant predictor of
within-decreases in income per capita and population. Our findings highlight
the importance of subnational regions, and particularly metropolitan areas, as
units of economic geography.",Annals of Contemporary Developments in Management & HR,http://arxiv.org/abs/1901.08112v1,econ.GN
441,"Compromise, Don't Optimize: Generalizing Perfect Bayesian Equilibrium to
  Allow for Ambiguity","We introduce a solution concept for extensive-form games of incomplete
information in which players need not assign likelihoods to what they do not
know about the game. This is embedded in a model in which players can hold
multiple priors. Players make choices by looking for compromises that yield a
good performance under each of their updated priors. Our solution concept is
called perfect compromise equilibrium. It generalizes perfect Bayesian
equilibrium. We show how it deals with ambiguity in Cournot and Bertrand
markets, public good provision, Spence's job market signaling, bilateral trade
with common value, and forecasting.",Annals of Operations Research,http://arxiv.org/abs/2003.02539v2,econ.GN
156,"How much income inequality is fair? Nash bargaining solution and its
  connection to entropy","The question about fair income inequality has been an important open question
in economics and in political philosophy for over two centuries with only
qualitative answers such as the ones suggested by Rawls, Nozick, and Dworkin.
We provided a quantitative answer recently, for an ideal free-market society,
by developing a game-theoretic framework that proved that the ideal inequality
is a lognormal distribution of income at equilibrium. In this paper, we develop
another approach, using the Nash Bargaining Solution (NBS) framework, which
also leads to the same conclusion. Even though the conclusion is the same, the
new approach, however, reveals the true nature of NBS, which has been of
considerable interest for several decades. Economists have wondered about the
economic meaning or purpose of the NBS. While some have alluded to its fairness
property, we show more conclusively that it is all about fairness. Since the
essence of entropy is also fairness, we see an interesting connection between
the Nash product and entropy for a large population of rational economic
agents.",Annals of Operations Research ,http://arxiv.org/abs/1806.05262v1,econ.EM
415,"Estimating Trade-Related Adjustment Costs in the Agricultural Sector in
  Iran","Tariff liberalization and its impact on tax revenue is an important
consideration for developing countries, because they are increasingly facing
the difficult task of implementing and harmonizing regional and international
trade commitments. The tariff reform and its costs for Iranian government is
one of the issues that are examined in this study. Another goal of this paper
is, estimating the cost of trade liberalization. On this regard, imports value
of agricultural sector in Iran in 2010 was analyzed according to two scenarios.
For reforming nuisance tariff, a VAT policy is used in both scenarios. In this
study, TRIST method is used. In the first scenario, imports' value decreased to
a level equal to the second scenario and higher tariff revenue will be created.
The results show that reducing the average tariff rate does not always result
in the loss of tariff revenue. This paper is a witness that different forms of
tariff can generate different amount of income when they have same level of
liberalization and equal effect on producers. Therefore, using a good tariff
regime can help a government to generate income when increases social welfare
by liberalization.",Annals of Statistics,http://arxiv.org/abs/1806.04238v1,math.ST
435,All-Pay Auctions with Different Forfeits,"In an auction each party bids a certain amount and the one which bids the
highest is the winner. Interestingly, auctions can also be used as models for
other real-world systems. In an all pay auction all parties must pay a forfeit
for bidding. In the most commonly studied all pay auction, parties forfeit
their entire bid, and this has been considered as a model for expenditure on
political campaigns. Here we consider a number of alternative forfeits which
might be used as models for different real-world competitions, such as
preparing bids for defense or infrastructure contracts.",Annals of Statistics,http://arxiv.org/abs/2002.02599v1,math.ST
460,"Labor Market Outcomes and Early Schooling: Evidence from School Entry
  Policies Using Exact Date of Birth","We use a rich, census-like Brazilian dataset containing information on
spatial mobility, schooling, and income in which we can link children to
parents to assess the impact of early education on several labor market
outcomes. Brazilian public primary schools admit children up to one year
younger than the national minimum age to enter school if their birthday is
before an arbitrary threshold, causing an exogenous variation in schooling at
adulthood. Using a Regression Discontinuity Design, we estimate one additional
year of schooling increases labor income in 25.8% - almost twice as large as
estimated using mincerian models. Around this cutoff there is also a gap of
9.6% on the probability of holding a college degree in adulthood, with which we
estimate the college premium and find a 201% increase in labor income. We test
the robustness of our estimates using placebo variables, alternative model
specifcations and McCrary Density Tests.",Annals of Statistics,http://arxiv.org/abs/1905.13281v1,math.ST
469,A Model of Presidential Debates,"Presidential debates are viewed as providing an important public good by
revealing information on candidates to voters. We consider an endogenous model
of presidential debates in which an incumbent and a challenger (who is
privately informed about her own quality) publicly announce whether they are
willing to participate in a public debate, taking into account that a voter's
choice of candidate depends on her beliefs regarding the candidates' qualities
and on the state of nature.It is found that in equilibrium a debate occurs or
does not occur independently of the challenger's quality and therefore the
candidates' announcements are uninformative. This is because opting-out is
perceived to be worse than losing a debate and therefore the challenger never
refuses to participate.",Annals of Statistics ,http://arxiv.org/abs/1907.01362v10,stat.ME
244,"Nighttime Light, Superlinear Growth, and Economic Inequalities at the
  Country Level","Research has highlighted relationships between size and scaled growth across
a large variety of biological and social organisms, ranging from bacteria,
through animals and plants, to cities an companies. Yet, heretofore,
identifying a similar relationship at the country level has proven challenging.
One reason is that, unlike the former, countries have predefined borders, which
limit their ability to grow ""organically."" This paper addresses this issue by
identifying and validating an effective measure of organic growth at the
country level: nighttime light emissions, which serve as a proxy of energy
allocations where more productive activity takes place. This indicator is
compared to population size to illustrate that while nighttime light emissions
are associated with superlinear growth, population size at the country level is
associated with sublinear growth. These relationships and their implications
for economic inequalities are then explored using high-resolution geospatial
datasets spanning the last three decades.",Annual Review of Economics,http://arxiv.org/abs/1810.12996v1,stat.AP
420,Semiparametrically Point-Optimal Hybrid Rank Tests for Unit Roots,"We propose a new class of unit root tests that exploits invariance properties
in the Locally Asymptotically Brownian Functional limit experiment associated
to the unit root model. The invariance structures naturally suggest tests that
are based on the ranks of the increments of the observations, their average,
and an assumed reference density for the innovations. The tests are
semiparametric in the sense that they are valid, i.e., have the correct
(asymptotic) size, irrespective of the true innovation density. For a correctly
specified reference density, our test is point-optimal and nearly efficient.
For arbitrary reference densities, we establish a Chernoff-Savage type result,
i.e., our test performs as well as commonly used tests under Gaussian
innovations but has improved power under other, e.g., fat-tailed or skewed,
innovation distributions. To avoid nonparametric estimation, we propose a
simplified version of our test that exhibits the same asymptotic properties,
except for the Chernoff-Savage result that we are only able to demonstrate by
means of simulations.",Annual Review of Economics,http://arxiv.org/abs/1806.09304v1,math.ST
191,A $t$-test for synthetic controls,"We propose a practical and robust method for making inferences on average
treatment effects estimated by synthetic controls. We develop a $K$-fold
cross-fitting procedure for bias-correction. To avoid the difficult estimation
of the long-run variance, inference is based on a self-normalized
$t$-statistic, which has an asymptotically pivotal $t$-distribution. Our
$t$-test is easy to implement, provably robust against misspecification, valid
with non-stationary data, and demonstrates an excellent small sample
performance. Compared to difference-in-differences, our method often yields
more than 50% shorter confidence intervals and is robust to violations of
parallel trends assumptions. An R-package for implementing our methods is
available.",Annual Review of Resource Economics ,http://arxiv.org/abs/1812.10820v6,econ.GN
136,Sorting and filtering as effective rational choice procedures,"Many online shops offer functionality that help their customers navigate the
available alternatives. For instance, options to filter and to sort goods are
wide-spread. In this paper we show that sorting and filtering can be used by
rational consumers to find their most preferred choice -- quickly. We
characterize the preferences which can be expressed through filtering and
sorting and show that these preferences exhibit a simple and intuitive logical
structure.","Applied Economics
",http://arxiv.org/abs/1809.06766v3,econ.GN
99,Unravelling the forces underlying urban industrial agglomeration,"As early as the 1920's Marshall suggested that firms co-locate in cities to
reduce the costs of moving goods, people, and ideas. These 'forces of
agglomeration' have given rise, for example, to the high tech clusters of San
Francisco and Boston, and the automobile cluster in Detroit. Yet, despite its
importance for city planners and industrial policy-makers, until recently there
has been little success in estimating the relative importance of each
Marshallian channel to the location decisions of firms.
  Here we explore a burgeoning literature that aims to exploit the co-location
patterns of industries in cities in order to disentangle the relationship
between industry co-agglomeration and customer/supplier, labour and idea
sharing. Building on previous approaches that focus on across- and
between-industry estimates, we propose a network-based method to estimate the
relative importance of each Marshallian channel at a meso scale. Specifically,
we use a community detection technique to construct a hierarchical
decomposition of the full set of industries into clusters based on
co-agglomeration patterns, and show that these industry clusters exhibit
distinct patterns in terms of their relative reliance on individual Marshallian
channels.",Applied Economics and Finance,http://arxiv.org/abs/1903.09279v2,econ.GN
115,Endogenous growth - A dynamic technology augmentation of the Solow model,"In this paper, I endeavour to construct a new model, by extending the classic
exogenous economic growth model by including a measurement which tries to
explain and quantify the size of technological innovation ( A ) endogenously. I
do not agree technology is a ""constant"" exogenous variable, because it is
humans who create all technological innovations, and it depends on how much
human and physical capital is allocated for its research. I inspect several
possible approaches to do this, and then I test my model both against sample
and real world evidence data. I call this method ""dynamic"" because it tries to
model the details in resource allocations between research, labor and capital,
by affecting each other interactively. In the end, I point out which is the new
residual and the parts of the economic growth model which can be further
improved.",Applied Economics and Finance,http://arxiv.org/abs/1805.00668v1,econ.GN
116,Optimal Linear Instrumental Variables Approximations,"This paper studies the identification and estimation of the optimal linear
approximation of a structural regression function. The parameter in the linear
approximation is called the Optimal Linear Instrumental Variables Approximation
(OLIVA). This paper shows that a necessary condition for standard inference on
the OLIVA is also sufficient for the existence of an IV estimand in a linear
model. The instrument in the IV estimand is unknown and may not be identified.
A Two-Step IV (TSIV) estimator based on Tikhonov regularization is proposed,
which can be implemented by standard regression routines. We establish the
asymptotic normality of the TSIV estimator assuming neither completeness nor
identification of the instrument. As an important application of our analysis,
we robustify the classical Hausman test for exogeneity against misspecification
of the linear structural model. We also discuss extensions to weighted least
squares criteria. Monte Carlo simulations suggest an excellent finite sample
performance for the proposed inferences. Finally, in an empirical application
estimating the elasticity of intertemporal substitution (EIS) with US data, we
obtain TSIV estimates that are much larger than their standard IV counterparts,
with our robust Hausman test failing to reject the null hypothesis of
exogeneity of real interest rates.",Applied Economics and Finance,http://arxiv.org/abs/1805.03275v3,econ.GN
248,"Health Care Expenditures, Financial Stability, and Participation in the
  Supplemental Nutrition Assistance Program (SNAP)","This paper examines the association between household healthcare expenses and
participation in the Supplemental Nutrition Assistance Program (SNAP) when
moderated by factors associated with financial stability of households. Using a
large longitudinal panel encompassing eight years, this study finds that an
inter-temporal increase in out-of-pocket medical expenses increased the
likelihood of household SNAP participation in the current period. Financially
stable households with precautionary financial assets to cover at least 6
months worth of household expenses were significantly less likely to
participate in SNAP. The low income households who recently experienced an
increase in out of pocket medical expenses but had adequate precautionary
savings were less likely than similar households who did not have precautionary
savings to participate in SNAP. Implications for economists, policy makers, and
household finance professionals are discussed.",Applied Economics and Finance,http://arxiv.org/abs/1811.05421v1,econ.GN
483,"Factor models with many assets: strong factors, weak factors, and the
  two-pass procedure","This paper re-examines the problem of estimating risk premia in linear factor
pricing models. Typically, the data used in the empirical literature are
characterized by weakness of some pricing factors, strong cross-sectional
dependence in the errors, and (moderately) high cross-sectional dimensionality.
Using an asymptotic framework where the number of assets/portfolios grows with
the time span of the data while the risk exposures of weak factors are
local-to-zero, we show that the conventional two-pass estimation procedure
delivers inconsistent estimates of the risk premia. We propose a new estimation
procedure based on sample-splitting instrumental variables regression. The
proposed estimator of risk premia is robust to weak included factors and to the
presence of strong unaccounted cross-sectional error dependence. We derive the
many-asset weak factor asymptotic distribution of the proposed estimator, show
how to construct its standard errors, verify its performance in simulations,
and revisit some empirical studies.",Applied Economics and Finance,http://arxiv.org/abs/1807.04094v2,econ.GN
457,The role of pawnshops in risk coping in early twentieth-century Japan,"This study examines the role of pawnshops as a risk-coping device in prewar
Japan. Using data on pawnshop loans for more than 250 municipalities and
exploiting the 1918-1920 influenza pandemic as a natural experiment, we find
that the adverse health shock increased the total amount of loans from
pawnshops. This is because those who regularly relied on pawnshops borrowed
more money from them than usual to cope with the adverse health shock, and not
because the number of people who used pawnshops increased.",Applied Economics and Finance ,http://arxiv.org/abs/1905.04419v2,econ.GN
22,"Apropiación privada de renta de recursos naturales? El caso del cobre
  en Chile","Unexpected increases of natural resource prices can generate rents, value
that should be recovered by the State to minimize inefficiencies, avoid
arbitrary discrimination between citizens and keep a sustainable trajectory. As
a case study about private appropriation of natural resource rent, this work
explores the case of copper in Chile since 1990, empirically analyzing if the
12 main private mining companies have recovered in present value more than
their investment during their life cycle. The results of this exercise,
applicable to other natural resources, indicate that some actually have,
capturing about US$ 40 billion up to 2012. Elaborating an adequate
institutional framework for future deposits remain important challenges for
Chile to plentifully take advantage of its mining potential, as well as for any
country with an abundant resource base to better enjoy its natural wealth. For
that purpose, a concession known as Least Present Value Revenue (LPVR) is
proposed.",Applied Energy,http://arxiv.org/abs/1812.05093v1,econ.GN
160,CAP and Monetary Policy,"Despite the importance of CAP-related agricultural market regulation
mechanisms within Europe, the agricultural sectors in European countries retain
a degree of sensitivity to macroeconomic activity and policies. This reality
now raises the question of the effects to be expected from the implementation
of the single monetary policy on these agricultural sectors within the Monetary
Union.",Applied Energy,http://arxiv.org/abs/1807.09475v1,econ.GN
506,A Theory of the Saving Rate of the Rich,"Empirical evidence suggests that the rich have higher propensity to save than
do the poor. While this observation may appear to contradict the homotheticity
of preferences, we theoretically show that that is not the case. Specifically,
we consider an income fluctuation problem with homothetic preferences and
general shocks and prove that consumption functions are asymptotically linear,
with an exact analytical characterization of asymptotic marginal propensities
to consume (MPC). We provide necessary and sufficient conditions for the
asymptotic MPCs to be zero. We calibrate a model with standard constant
relative risk aversion utility and show that zero asymptotic MPCs are
empirically plausible, implying that our mechanism has the potential to
accommodate a large saving rate of the rich and high wealth inequality (small
Pareto exponent) as observed in the data.",Applied Energy,http://arxiv.org/abs/2005.02379v5,eess.SY
581,Social Welfare in Search Games with Asymmetric Information,"We consider games in which players search for a hidden prize, and they have
asymmetric information about the prize location. We study the social payoff in
equilibria of these games. We present sufficient conditions for the existence
of an equilibrium that yields the first-best payoff (i.e., the highest social
payoff under any strategy profile), and we characterize the first-best payoff.
The results have interesting implications for innovation contests and R&D
races.",Applied Energy,http://arxiv.org/abs/2006.14860v1,econ.GN
601,CO2 mitigation model for China's residential building sector,"This paper aims to investigate the factors that can mitigate carbon-dioxide
(CO2) intensity and further assess CMRBS in China based on a household scale
via decomposition analysis. Here we show that: Three types of housing economic
indicators and the final emission factor significantly contributed to the
decrease in CO2 intensity in the residential building sector. In addition, the
CMRBS from 2001-2016 was 1816.99 MtCO2, and the average mitigation intensity
during this period was 266.12 kgCO2/household/year. Furthermore, the
energy-conservation and emission-mitigation strategy caused CMRBS to
effectively increase and is the key to promoting a more significant emission
mitigation in the future. Overall, this paper covers the CMRBS assessment gap
in China, and the proposed assessment model can be regarded as a reference for
other countries and cities for measuring the retrospective CO2 mitigation
effect in residential buildings.",Applied Energy,http://arxiv.org/abs/1909.01249v1,q-fin.ST
382,"Crowdfunding Public Projects: Collaborative Governance for Achieving
  Citizen Co-funding of Public Goods","This study explores the potential of crowdfunding as a tool for achieving
citizen co-funding of public projects. Focusing on philanthropic crowdfunding,
we examine whether collaborative projects between public and private
organizations are more successful in fundraising than projects initiated solely
by private organizations. We argue that government involvement in crowdfunding
provides some type of accreditation or certification that attests to a project
aim to achieve public rather than private goals, thereby mitigating information
asymmetry and improving mutual trust between creators (i.e., private sector
organizations) and funders (i.e., crowd). To support this argument, we show
that crowdfunding projects with government involvement achieved a greater
success rate and attracted a greater amount of funding than comparable projects
without government involvement. This evidence shows that governments may take
advantage of crowdfunding to co-fund public projects with the citizenry for
addressing the complex challenges that we face in the twenty-first century.",Applied Energy ,http://arxiv.org/abs/1902.02480v1,econ.GN
578,"Mechanism of Instrumental Game Theory in The Legal Process via
  Stochastic Options Pricing Induction","Economic theory has provided an estimable intuition in understanding the
perplexing ideologies in law, in the areas of economic law, tort law, contract
law, procedural law and many others. Most legal systems require the parties
involved in a legal dispute to exchange information through a process called
discovery. The purpose is to reduce the relative optimisms developed by
asymmetric information between the parties. Like a head or tail phenomenon in
stochastic processes, uncertainty in the adjudication affects the decisions of
the parties in a legal negotiation. This paper therefore applies the principles
of aleatory analysis to determine how negotiations fail in the legal process,
introduce the axiological concept of optimal transaction cost and formulates a
numerical methodology based on backwards induction and stochastic options
pricing economics in estimating the reasonable and fair bargain in order to
induce settlements thereby increasing efficiency and reducing social costs.",Applied Mathematics & Optimization ,http://arxiv.org/abs/2006.11061v1,math.NA
52,Existence of Equilibrium Prices: A Pedagogical Proof,"Under the same assumptions made by Mas-Colell et al. (1995), I develop a
short, simple, and complete proof of existence of equilibrium prices based on
excess demand functions. The result is obtained by applying the Brouwer fixed
point theorem to a trimmed simplex which does not contain prices equal to zero.
The mathematical techniques are based on some results obtained in Neuefeind
(1980) and Geanakoplos (2003).",Applied Network Science,http://arxiv.org/abs/1808.03129v2,econ.GN
256,Ensemble Methods for Causal Effects in Panel Data Settings,"This paper studies a panel data setting where the goal is to estimate causal
effects of an intervention by predicting the counterfactual values of outcomes
for treated units, had they not received the treatment. Several approaches have
been proposed for this problem, including regression methods, synthetic control
methods and matrix completion methods. This paper considers an ensemble
approach, and shows that it performs better than any of the individual methods
in several economic datasets. Matrix completion methods are often given the
most weight by the ensemble, but this clearly depends on the setting. We argue
that ensemble methods present a fruitful direction for further research in the
causal panel data setting.",Artificial Intelligence,http://arxiv.org/abs/1903.10079v1,cs.GT
350,Scoring Strategic Agents,"I introduce a model of predictive scoring. A receiver wants to predict a
sender's quality. An intermediary observes multiple features of the sender and
aggregates them into a score. Based on the score, the receiver takes a
decision. The sender wants the most favorable decision, and she can distort
each feature at a privately known cost. I characterize the most accurate
scoring rule. This rule underweights some features to deter sender distortion,
and overweights other features so that the score is correct on average. The
receiver prefers this scoring rule to full disclosure because information
aggregation mitigates his commitment problem.",Asian Economic Papers,http://arxiv.org/abs/1909.01888v3,econ.GN
105,How Smart Are `Water Smart Landscapes'?,"Understanding the effectiveness of alternative approaches to water
conservation is crucially important for ensuring the security and reliability
of water services for urban residents. We analyze data from one of the
longest-running ""cash for grass"" policies - the Southern Nevada Water
Authority's Water Smart Landscapes program, where homeowners are paid to
replace grass with xeric landscaping. We use a twelve year long panel dataset
of monthly water consumption records for 300,000 households in Las Vegas,
Nevada. Utilizing a panel difference-in-differences approach, we estimate the
average water savings per square meter of turf removed. We find that
participation in this program reduced the average treated household's
consumption by 18 percent. We find no evidence that water savings degrade as
the landscape ages, or that water savings per unit area are influenced by the
value of the rebate. Depending on the assumed time horizon of benefits from
turf removal, we find that the WSL program cost the water authority about $1.62
per thousand gallons of water saved, which compares favorably to alternative
means of water conservation or supply augmentation.",Asian Social Science ,http://arxiv.org/abs/1803.04593v1,econ.GN
54,$k$th price auctions and Catalan numbers,"This paper establishes an interesting link between $k$th price auctions and
Catalan numbers by showing that for distributions that have linear density, the
bid function at any symmetric, increasing equilibrium of a $k$th price auction
with $k\geq 3$ can be represented as a finite series of $k-2$ terms whose
$\ell$th term involves the $\ell$th Catalan number. Using an integral
representation of Catalan numbers, together with some classical combinatorial
identities, we derive the closed form of the unique symmetric, increasing
equilibrium of a $k$th price auction for a non-uniform distribution.",Bank of Israel Discussion Paper Series ,http://arxiv.org/abs/1808.05996v1,econ.EM
418,Shift-Share Designs: Theory and Inference,"We study inference in shift-share regression designs, such as when a regional
outcome is regressed on a weighted average of sectoral shocks, using regional
sector shares as weights. We conduct a placebo exercise in which we estimate
the effect of a shift-share regressor constructed with randomly generated
sectoral shocks on actual labor market outcomes across U.S. Commuting Zones.
Tests based on commonly used standard errors with 5\% nominal significance
level reject the null of no effect in up to 55\% of the placebo samples. We use
a stylized economic model to show that this overrejection problem arises
because regression residuals are correlated across regions with similar
sectoral shares, independently of their geographic location. We derive novel
inference methods that are valid under arbitrary cross-regional correlation in
the regression residuals. We show using popular applications of shift-share
designs that our methods may lead to substantially wider confidence intervals
in practice.",Biometrika,http://arxiv.org/abs/1806.07928v5,stat.ME
444,Keeping the Listener Engaged: a Dynamic Model of Bayesian Persuasion,"We consider a dynamic model of Bayesian persuasion in which information takes
time and is costly for the sender to generate and for the receiver to process,
and neither player can commit to their future actions. Persuasion may totally
collapse in a Markov perfect equilibrium (MPE) of this game. However, for
persuasion costs sufficiently small, a version of a folk theorem holds:
outcomes that approximate Kamenica and Gentzkow (2011)'s sender-optimal
persuasion as well as full revelation and everything in between are obtained in
MPE, as the cost vanishes.",Brain Behavior and Immunity ,http://arxiv.org/abs/2003.07338v3,cs.CY
267,Inference for VARs Identified with Sign Restrictions,"There is a fast growing literature that set-identifies structural vector
autoregressions (SVARs) by imposing sign restrictions on the responses of a
subset of the endogenous variables to a particular structural shock
(sign-restricted SVARs). Most methods that have been used to construct
pointwise coverage bands for impulse responses of sign-restricted SVARs are
justified only from a Bayesian perspective. This paper demonstrates how to
formulate the inference problem for sign-restricted SVARs within a
moment-inequality framework. In particular, it develops methods of constructing
confidence bands for impulse response functions of sign-restricted SVARs that
are valid from a frequentist perspective. The paper also provides a comparison
of frequentist and Bayesian coverage bands in the context of an empirical
application - the former can be substantially wider than the latter.",Central European Journal of Economic Modelling and Econometrics,http://arxiv.org/abs/1709.10196v2,econ.EM
303,"The transmission of liquidity shocks via China's segmented money market:
  evidence from recent market events","This is the first study to explore the transmission paths for liquidity
shocks in China's segmented money market. We examine how money market
transactions create such pathways between China's closely-guarded banking
sector and the rest of its financial system, and empirically capture the
transmission of liquidity shocks through these pathways during two recent
market events. We find strong indications that money market transactions allow
liquidity shocks to circumvent certain regulatory restrictions and financial
market segmentation in China. Our findings suggest that a widespread
illiquidity contagion facilitated by money market transactions can happen in
China and new policy measures are needed to prevent such contagion.",Chaos ,http://arxiv.org/abs/1811.08949v1,math.OC
509,"Information Validates the Prior: A Theorem on Bayesian Updating and
  Applications","We develop a result on expected posteriors for Bayesians with heterogenous
priors, dubbed information validates the prior (IVP). Under familiar ordering
requirements, Anne expects a (Blackwell) more informative experiment to bring
Bob's posterior mean closer to Anne's prior mean. We apply the result in two
contexts of games of asymmetric information: voluntary testing or
certification, and costly signaling or falsification. IVP can be used to
determine how an agent's behavior responds to additional exogenous or
endogenous information. We discuss economic implications.",Chaos ,http://arxiv.org/abs/2005.05714v3,q-fin.ST
530,"Hiring in the substance use disorder treatment related sector during the
  first five years of Medicaid expansion","Effective treatment strategies exist for substance use disorder (SUD),
however severe hurdles remain in ensuring adequacy of the SUD treatment (SUDT)
workforce as well as improving SUDT affordability, access and stigma. Although
evidence shows recent increases in SUD medication access from expanding
Medicaid availability under the Affordable Care Act, it is yet unknown whether
these policies also led to a growth in the changes in the nature of hiring in
SUDT related workforce, partly due to poor data availability. Our study uses
novel data to shed light on recent trends in a fast-evolving and
policy-relevant labor market, and contributes to understanding the current SUDT
related workforce and the effect of Medicaid expansion on hiring attempts in
this sector. We examine attempts over 2010-2018 at hiring in the SUDT and
related behavioral health sector as background for estimating the causal effect
of the 2014-and-beyond state Medicaid expansion on these outcomes through
""difference-in-difference"" econometric models. We use Burning Glass
Technologies (BGT) data covering virtually all U.S. job postings by employers.
Nationally, we find little growth in the sector's hiring attempts in 2010-2018
relative to the rest of the economy or to health care as a whole. However, this
masks diverging trends in subsectors, which saw reduction in hospital based
hiring attempts, increases towards outpatient facilities, and changes in
occupational hiring demand shifting from medical personnel towards counselors
and social workers. Although Medicaid expansion did not lead to any
statistically significant or meaningful change in overall hiring attempts,
there was a shift in the hiring landscape.",Chaos ,http://arxiv.org/abs/1908.00216v1,physics.soc-ph
598,Signaling with Private Monitoring,"We study dynamic signaling when the informed party does not observe the
signals generated by her actions. A long-run player signals her type
continuously over time to a myopic second player who privately monitors her
behavior; in turn, the myopic player transmits his private inferences back
through an imperfect public signal of his actions. Preferences are
linear-quadratic and the information structure is Gaussian. We construct linear
Markov equilibria using belief states up to the long-run player's
$\textit{second-order belief}$. Because of the private monitoring, this state
is an explicit function of the long-run player's past play. A novel separation
effect then emerges through this second-order belief channel, altering the
traditional signaling that arises when beliefs are public. Applications to
models of leadership, reputation, and trading are examined.",Chaos Solitons and Fractals ,http://arxiv.org/abs/2007.15514v1,physics.soc-ph
97,"Variety, Complexity and Economic Development","We propose a combinatorial model of economic development. An economy develops
by acquiring new capabilities allowing for the production of an ever greater
variety of products of increasingly complex products. Taking into account that
economies abandon the least complex products as they develop over time, we show
that variety first increases and then decreases in the course of economic
development. This is consistent with the empirical pattern known as 'the hump'.
Our results question the common association of variety with complexity. We
further discuss the implications of our model for future research.",China Urban Economy ,http://arxiv.org/abs/1903.07997v1,econ.TH
394,"ICT Capital-Skill Complementarity and Wage Inequality: Evidence from
  OECD Countries","Although wage inequality has evolved in advanced countries over recent
decades, it remains unknown the extent to which changes in wage inequality and
their differences across countries are attributable to specific capital and
labor quantities. We examine this issue by estimating a sector-level production
function extended to allow for capital-skill complementarity and factor-biased
technological change using cross-country and cross-industry panel data. Our
results indicate that most of the changes in the skill premium are attributable
to the relative quantities of ICT equipment, skilled labor, and unskilled labor
in the goods and service sectors of the majority of advanced countries.",Chinese Physics B,http://arxiv.org/abs/1904.09857v5,physics.soc-ph
388,"FDI, banking crisis and growth: direct and spill over effects","This study suggests a new decomposition of the effect of Foreign Direct
Investment (FDI) on long-term growth in developing countries. It reveals that
FDI not only have a positive direct effect on growth, but also increase the
latter by reducing the recessionary effect resulting from a banking crisis.
Even more, they reduce its occurrence. JEL: F65, F36, G01, G15",Cogent Economics & Finance,http://arxiv.org/abs/1904.04911v1,q-fin.PM
604,"Climate Policy under Spatial Heat Transport: Cooperative and
  Noncooperative Regional Outcomes","We build a novel stochastic dynamic regional integrated assessment model
(IAM) of the climate and economic system including a number of important
climate science elements that are missing in most IAMs. These elements are
spatial heat transport from the Equator to the Poles, sea level rise,
permafrost thaw and tipping points. We study optimal policies under cooperation
and noncooperation between two regions (the North and the Tropic-South) in the
face of risks and recursive utility. We introduce a new general computational
algorithm to find feedback Nash equilibrium. Our results suggest that when the
elements of climate science are ignored, important policy variables such as the
optimal regional carbon tax and adaptation could be seriously biased. We also
find the regional carbon tax is significantly smaller in the feedback Nash
equilibrium than in the social planner's problem in each region, and the North
has higher carbon taxes than the Tropic-South.",Cogent Economics & Finance ,http://arxiv.org/abs/1909.04009v1,q-fin.PM
452,Do Informational Cascades Happen with Non-myopic Agents?,"We consider an environment where players need to decide whether to buy a
certain product (or adopt a technology) or not. The product is either good or
bad but its true value is not known to the players. Instead, each player has
her own private information on its quality. Each player can observe the
previous actions of other players and estimate the quality of the product. A
classic result in the literature shows that in similar settings information
cascades occur where learning stops for the whole network and players repeat
the actions of their predecessors. In contrast to the existing literature on
informational cascades, in this work, players get more than one opportunity to
act. In each turn, a player is chosen uniformly at random and can decide to buy
the product and leave the market or to wait. We provide a characterization of
structured perfect Bayesian equilibria (sPBE) with forward-looking strategies
through a fixed-point equation of dimensionality that grows only quadratically
with the number of players. In particular, a sufficient state for players'
strategies at each time instance is a pair of two integers, the first
corresponding to the estimated quality of the good and the second indicating
the number of players that cannot offer additional information about the good
to the rest of the players. Based on this characterization we study
informational cascades in two regimes. First, we show that for a discount
factor strictly smaller than one, informational cascades happen with high
probability as the number of players increases. Furthermore, only a small
portion of the total information in the system is revealed before a cascade
occurs. Secondly, and more surprisingly, we show that for a fixed number of
players, as the discount factor approaches one, bad informational cascades are
benign when the product is bad, and are completely eliminated when the discount
factor equals one.",Computational Statistics & Data Analysis ,http://arxiv.org/abs/1905.01327v3,stat.ME
186,Approximate State Space Modelling of Unobserved Fractional Components,"We propose convenient inferential methods for potentially nonstationary
multivariate unobserved components models with fractional integration and
cointegration. Based on finite-order ARMA approximations in the state space
representation, maximum likelihood estimation can make use of the EM algorithm
and related techniques. The approximation outperforms the frequently used
autoregressive or moving average truncation, both in terms of computational
costs and with respect to approximation quality. Monte Carlo simulations reveal
good estimation properties of the proposed methods for processes of different
complexity and dimension.",Computers and Operations Research ,http://arxiv.org/abs/1812.09142v3,econ.GN
159,"An Impossibility Theorem for Wealth in Heterogeneous-agent Models with
  Limited Heterogeneity","It has been conjectured that canonical Bewley--Huggett--Aiyagari
heterogeneous-agent models cannot explain the joint distribution of income and
wealth. The results stated below verify this conjecture and clarify its
implications under very general conditions. We show in particular that if (i)
agents are infinitely-lived, (ii) saving is risk-free, and (iii) agents have
constant discount factors, then the wealth distribution inherits the tail
behavior of income shocks (e.g., light-tailedness or the Pareto exponent). Our
restrictions on utility require only that relative risk aversion is bounded,
and a large variety of income processes are admitted. Our results show
conclusively that it is necessary to go beyond standard models to explain the
empirical fact that wealth is heavier-tailed than income. We demonstrate
through examples that relaxing any of the above three conditions can generate
Pareto tails.",Covid Economics,http://arxiv.org/abs/1807.08404v3,econ.GN
184,"What Is the Value Added by Using Causal Machine Learning Methods in a
  Welfare Experiment Evaluation?","Recent studies have proposed causal machine learning (CML) methods to
estimate conditional average treatment effects (CATEs). In this study, I
investigate whether CML methods add value compared to conventional CATE
estimators by re-evaluating Connecticut's Jobs First welfare experiment. This
experiment entails a mix of positive and negative work incentives. Previous
studies show that it is hard to tackle the effect heterogeneity of Jobs First
by means of CATEs. I report evidence that CML methods can provide support for
the theoretical labor supply predictions. Furthermore, I document reasons why
some conventional CATE estimators fail and discuss the limitations of CML
methods.",Covid Economics,http://arxiv.org/abs/1812.06533v3,econ.GN
82,Revisiting Transformation and Directional Technology Distance Functions,"In the first part of the paper, we prove the equivalence of the unsymmetric
transformation function and an efficient joint production function (JPF) under
strong monotonicity conditions imposed on input and output correspondences.
Monotonicity, continuity, and convexity properties sufficient for a symmetric
transformation function to be an efficient JPF are also stated. In the second
part, we show that the most frequently used functional form for the directional
technology distance function (DTDF), the quadratic, does not satisfy
homogeneity of degree $-1$ in the direction vector. This implies that the
quadratic function is not the directional technology distance function. We
provide derivation of the DTDF from a symmetric transformation function and
show how this approach can be used to obtain functional forms that satisfy both
translation property and homogeneity of degree $-1$ in the direction vector if
the optimal solution of an underlying optimization problem can be expressed in
closed form.",Covid Economics ,http://arxiv.org/abs/1812.10108v1,econ.TH
505,Dynamic Reserves in Matching Markets,"We study a school choice problem under affirmative action policies where
authorities reserve a certain fraction of the slots at each school for specific
student groups, and where students have preferences not only over the schools
they are matched to but also the type of slots they receive. Such reservation
policies might cause waste in instances of low demand from some student groups.
To propose a solution to this issue, we construct a family of choice functions,
dynamic reserves choice functions, for schools that respect within-group
fairness and allow the transfer of otherwise vacant slots from low-demand
groups to high-demand groups. We propose the cumulative offer mechanism (COM)
as an allocation rule where each school uses a dynamic reserves choice function
and show that it is stable with respect to schools' choice functions, is
strategy-proof, and respects improvements. Furthermore, we show that
transferring more of the otherwise vacant slots leads to strategy-proof Pareto
improvement under the COM.",Crime Science,http://arxiv.org/abs/2005.01103v1,econ.GN
554,"Prices, Profits, Proxies, and Production","This paper studies nonparametric identification and counterfactual bounds for
heterogeneous firms that can be ranked in terms of productivity. Our approach
works when quantities and prices are latent rendering standard approaches
inapplicable. Instead, we require observation of profits or other
optimizing-values such as costs or revenues, and either prices or price proxies
of flexibly chosen variables. We extend classical duality results for
price-taking firms to a setup with discrete heterogeneity, endogeneity, and
limited variation in possibly latent prices. Finally, we show that convergence
results for nonparametric estimators may be directly converted to convergence
results for production sets.",Decision Analysis ,http://arxiv.org/abs/1810.04697v3,econ.GN
546,"Future competitive bioenergy technologies in the German heat sector:
  Findings from an economic optimization approach","Meeting the defined greenhouse gas (GHG) reduction targets in Germany is only
possible by switching to renewable technologies in the energy sector. A major
share of that reduction needs to be covered by the heat sector, which accounts
for ~35% of the energy based emissions in Germany. Biomass is the renewable key
player in the heterogeneous heat sector today. Its properties such as weather
independency, simple storage and flexible utilization open up a wide field of
applications for biomass. However, in a future heat sector fulfilling GHG
reduction targets and energy sectors being increasingly connected: which
bioenergy technology concepts are competitive options against other renewable
heating systems? In this paper, the cost optimal allocation of the limited
German biomass potential is investigated under longterm scenarios using a
mathematical optimization approach. The model results show that bioenergy can
be a competitive option in the future. Especially the use of biomass from
residues can be highly competitive in hybrid combined heat and power (CHP)
pellet combustion plants in the private household sector. However, towards
2050, wood based biomass use in high temperature industry applications is found
to be the most cost efficient way to reduce heat based emissions by 95% in
2050.",Decision Support Systems ,http://arxiv.org/abs/1908.10065v2,econ.EM
285,Ordinal Imitative Dynamics,"This paper introduces an evolutionary dynamics based on imitate the better
realization (IBR) rule. Under this rule, agents in a population game imitate
the strategy of a randomly chosen opponent whenever the opponent`s realized
payoff is higher than their own. Such behavior generates an ordinal mean
dynamics which is polynomial in strategy utilization frequencies. We
demonstrate that while the dynamics does not possess Nash stationarity or
payoff monotonicity, under it pure strategies iteratively strictly dominated by
pure strategies are eliminated and strict equilibria are locally stable. We
investigate the relationship between the dynamics based on the IBR rule and the
replicator dynamics. In trivial cases, the two dynamics are topologically
equivalent. In Rock-Paper-Scissors games we conjecture that both dynamics
exhibit the same types of behavior, but the partitions of the game set do not
coincide. In other cases, the IBR dynamics exhibits behaviors that are
impossible under the replicator dynamics.",Development Compilation ,http://arxiv.org/abs/1907.04272v1,econ.GN
118,"Density Forecasts in Panel Data Models: A Semiparametric Bayesian
  Perspective","This paper constructs individual-specific density forecasts for a panel of
firms or households using a dynamic linear model with common and heterogeneous
coefficients as well as cross-sectional heteroskedasticity. The panel
considered in this paper features a large cross-sectional dimension N but short
time series T. Due to the short T, traditional methods have difficulty in
disentangling the heterogeneous parameters from the shocks, which contaminates
the estimates of the heterogeneous parameters. To tackle this problem, I assume
that there is an underlying distribution of heterogeneous parameters, model
this distribution nonparametrically allowing for correlation between
heterogeneous parameters and initial conditions as well as individual-specific
regressors, and then estimate this distribution by combining information from
the whole panel. Theoretically, I prove that in cross-sectional homoskedastic
cases, both the estimated common parameters and the estimated distribution of
the heterogeneous parameters achieve posterior consistency, and that the
density forecasts asymptotically converge to the oracle forecast.
Methodologically, I develop a simulation-based posterior sampling algorithm
specifically addressing the nonparametric density estimation of unobserved
heterogeneous parameters. Monte Carlo simulations and an empirical application
to young firm dynamics demonstrate improvements in density forecasts relative
to alternative approaches.",Discover Sustainability ,http://arxiv.org/abs/1805.04178v3,econ.GN
314,"Fuzzy Profit Shifting: A Model for Optimal Tax-induced Transfer Pricing
  with Fuzzy Arm's Length Parameter","This paper proposes a model of optimal tax-induced transfer pricing with a
fuzzy arm's length parameter. Fuzzy numbers provide a suitable structure for
modelling the ambiguity that is intrinsic to the arm's length parameter. For
the usual conditions regarding the anti-shifting mechanisms, the optimal
transfer price becomes a maximising $\alpha$-cut of the fuzzy arm's length
parameter. Nonetheless, we show that it is profitable for firms to choose any
maximising transfer price if the probability of tax audit is sufficiently low,
even if the chosen price is considered a completely non-arm's length price by
tax authorities. In this case, we derive the necessary and sufficient
conditions to prevent this extreme shifting strategy",Dynamic Games and Applications,http://arxiv.org/abs/1901.03843v1,math.DS
122,Happy family of stable marriages,"Some aspects of the problem of stable marriage are discussed. There are two
distinguished marriage plans: the fully transferable case, where money can be
transferred between the participants, and the fully non transferable case where
each participant has its own rigid preference list regarding the other gender.
We continue to discuss intermediate partial transferable cases. Partial
transferable plans can be approached as either special cases of cooperative
games using the notion of a core, or as a generalization of the cyclical
monotonicity property of the fully transferable case (fake promises). We shall
introduced these two approaches, and prove the existence of stable marriage for
the fully transferable and non-transferable plans.",Ecological Economics,http://arxiv.org/abs/1805.06687v1,econ.GN
152,"Determining Fundamental Supply and Demand Curves in a Wholesale
  Electricity Market","In this paper we develop a novel method of wholesale electricity market
modeling. Our optimization-based model decomposes wholesale supply and demand
curves into buy and sell orders of individual market participants. In doing so,
the model detects and removes arbitrage orders. As a result, we construct an
innovative fundamental model of a wholesale electricity market. First, our
fundamental demand curve has a unique composition. The demand curve lies in
between the wholesale demand curve and a perfectly inelastic demand curve.
Second, our fundamental supply and demand curves contain only actual (i.e.
non-arbitrage) transactions with physical assets on buy and sell sides. Third,
these transactions are designated to one of the three groups of wholesale
electricity market participants: retailers, suppliers, or utility companies. To
evaluate the performance of our model, we use the German wholesale market data.
Our fundamental model yields a more precise approximation of the actual load
values than a model with perfectly inelastic demand. Moreover, we conduct a
study of wholesale demand elasticities. The obtained conclusions regarding
wholesale demand elasticity are consistent with the existing academic
literature.",Ecological Economics ,http://arxiv.org/abs/1903.11383v2,econ.GN
465,"A New Solution to Market Definition: An Approach Based on
  Multi-dimensional Substitutability Statistics","Market definition is an important component in the premerger investigation,
but the models used in the market definition have not developed much in the
past three decades since the Critical Loss Analysis (CLA) was proposed in 1989.
The CLA helps the Hypothetical Monopolist Test to determine whether the
hypothetical monopolist is going to profit from the small but significant and
non-transitory increase in price (SSNIP). However, the CLA has long been
criticized by academic scholars for its tendency to conclude a narrow market.
Although the CLA was adopted by the 2010 Horizontal Merger Guidelines (the 2010
Guidelines), the criticisms are likely still valid. In this dissertation, we
discussed the mathematical deduction of CLA, the data used, and the SSNIP
defined by the Agencies. Based on our research, we concluded that the narrow
market conclusion was due to the incorrect implementation of the CLA; not the
model itself. On the other hand, there are other unresolvable problems in the
CLA and the Hypothetical Monopolist Test. The SSNIP test and the CLA are bright
resolutions for market definition problem during their time, but we have more
advanced tools to solve the task nowadays. In this dissertation, we propose a
model which is based directly on the multi-dimensional substitutability between
the products and is capable of maximizing the substitutability of product
features within each group. Since the 2010 Guidelines does not exclude the use
of models other than the ones mentioned by the Guidelines, our method can
hopefully supplement the current models to show a better picture of the
substitutive relations and provide a more stable definition of the market.",Ecological Economics ,http://arxiv.org/abs/1906.10030v1,econ.EM
518,Extractive contest design,"We consider contest success functions (CSFs) that extract contestants' values
of the prize. In the case in which the values are observable to the contest
designer, in the more-than-two-contestant or common-value subcase, we present a
CSF extractive in any equilibrium; in the other subcase, we present a CSF
extractive in some equilibrium, but there exists no CSF extractive in any
equilibrium. In the case in which the values are not observable, there exists
no CSF extractive in some equilibrium. In the case in which the values are
observable and common, we present extractive a CSF extractive in any
equilibrium; we present a class of CSFs extractive in some equilibrium, and
this class can control the number of active contestants.",Ecological Economics ,http://arxiv.org/abs/2006.01808v2,econ.EM
188,Many Average Partial Effects: with An Application to Text Regression,"We study estimation, pointwise and simultaneous inference, and confidence
intervals for many average partial effects of lasso Logit. Focusing on
high-dimensional, cluster-sampling environments, we propose a new average
partial effect estimator and explore its asymptotic properties. Practical
penalty choices compatible with our asymptotic theory are also provided. The
proposed estimator allow for valid inference without requiring oracle property.
We provide easy-to-implement algorithms for cluster-robust high-dimensional
hypothesis testing and construction of simultaneously valid confidence
intervals using a multiplier cluster bootstrap. We apply the proposed
algorithms to the text regression model of Wu (2018) to examine the presence of
gendered language on the internet.",Econometric Reviews ,http://arxiv.org/abs/1812.09397v5,econ.EM
597,Equilibrium Behaviors in Repeated Games,"We examine a patient player's behavior when he can build reputations in front
of a sequence of myopic opponents. With positive probability, the patient
player is a commitment type who plays his Stackelberg action in every period.
We characterize the patient player's action frequencies in equilibrium. Our
results clarify the extent to which reputations can refine the patient player's
behavior and provide new insights to entry deterrence, business transactions,
and capital taxation. Our proof makes a methodological contribution by
establishing a new concentration inequality.",Econometric Reviews ,http://arxiv.org/abs/2007.14002v4,stat.ML
176,"Flexible shrinkage in high-dimensional Bayesian spatial autoregressive
  models","This article introduces two absolutely continuous global-local shrinkage
priors to enable stochastic variable selection in the context of
high-dimensional matrix exponential spatial specifications. Existing approaches
as a means to dealing with overparameterization problems in spatial
autoregressive specifications typically rely on computationally demanding
Bayesian model-averaging techniques. The proposed shrinkage priors can be
implemented using Markov chain Monte Carlo methods in a flexible and efficient
way. A simulation study is conducted to evaluate the performance of each of the
shrinkage priors. Results suggest that they perform particularly well in
high-dimensional environments, especially when the number of parameters to
estimate exceeds the number of observations. For an empirical illustration we
use pan-European regional economic growth data.",Econometric Theory ,http://arxiv.org/abs/1805.10822v1,econ.EM
50,Dynamic Random Subjective Expected Utility,"Dynamic Random Subjective Expected Utility (DR-SEU) allows to model choice
data observed from an agent or a population of agents whose beliefs about
objective payoff-relevant states and tastes can both evolve stochastically. Our
observable, the augmented Stochastic Choice Function (aSCF) allows, in contrast
to previous work in decision theory, for a direct test of whether the agent's
beliefs reflect the true data-generating process conditional on their private
information as well as identification of the possibly incorrect beliefs. We
give an axiomatic characterization of when an agent satisfies the model, both
in a static as well as in a dynamic setting. We look at the case when the agent
has correct beliefs about the evolution of objective states as well as at the
case when her beliefs are incorrect but unforeseen contingencies are
impossible.
  We also distinguish two subvariants of the dynamic model which coincide in
the static setting: Evolving SEU, where a sophisticated agent's utility evolves
according to a Bellman equation and Gradual Learning, where the agent is
learning about her taste. We prove easy and natural comparative statics results
on the degree of belief incorrectness as well as on the speed of learning about
taste.
  Auxiliary results contained in the online appendix extend previous decision
theory work in the menu choice and stochastic choice literature from a
technical as well as a conceptual perspective.",Econometrica,http://arxiv.org/abs/1808.00296v1,econ.GN
155,"Dark Markets with Multiple Assets: Segmentation, Asymptotic Stability,
  and Equilibrium Prices","We study a generalization of the model of a dark market due to
Duffie-G\^arleanu- Pedersen [6]. Our market is segmented and involves multiple
assets. We show that this market has a unique asymptotically stable
equilibrium. In order to establish this result, we use a novel approach
inspired by a theory due to McKenzie and Hawkins-Simon. Moreover, we obtain a
closed form solution for the price of each asset at which investors trade at
equilibrium. We conduct a comparative statics analysis which shows, among other
sensitivities, how equilibrium prices respond to the level of interactions
between investors.",Econometrica,http://arxiv.org/abs/1806.01924v1,econ.EM
167,An alternative quality of life ranking on the basis of remittances,"Remittances provide an essential connection between people working abroad and
their home countries. This paper considers these transfers as a measure of
preferences revealed by the workers, underlying a ranking of countries around
the world. In particular, we use the World Bank bilateral remittances data of
international salaries and interpersonal transfers between 2010 and 2015 to
compare European countries. The suggested least squares method has favourable
axiomatic properties. Our ranking reveals a crucial aspect of quality of life
and may become an alternative to various composite indices.",Econometrica,http://arxiv.org/abs/1809.03977v6,q-fin.EC
282,"General equilibrium in a heterogeneous-agent incomplete-market economy
  with many consumption goods and a risk-free bond","We study a pure-exchange incomplete-market economy with heterogeneous agents.
In each period, the agents choose how much to save (i.e., invest in a risk-free
bond), how much to consume, and which bundle of goods to consume while their
endowments are fluctuating. We focus on a competitive stationary equilibrium
(CSE) in which the wealth distribution is invariant, the agents maximize their
expected discounted utility, and both the prices of consumption goods and the
interest rate are market-clearing. Our main contribution is to extend some
general equilibrium results to an incomplete-market Bewley-type economy with
many consumption goods. Under mild conditions on the agents' preferences, we
show that the aggregate demand for goods depends only on their relative prices
and that the aggregate demand for savings is homogeneous of degree in prices,
and we prove the existence of a CSE. When the agents' preferences can be
represented by a CES (constant elasticity of substitution) utility function
with an elasticity of substitution that is higher than or equal to one, we
prove that the CSE is unique. Under the same preferences, we show that a higher
inequality of endowments does not change the equilibrium prices of goods, and
decreases the equilibrium interest rate. Our results shed light on the impact
of market incompleteness on the properties of general equilibrium models.",Econometrica,http://arxiv.org/abs/1906.06810v2,cs.GT
416,"On the relation between Sion's minimax theorem and existence of Nash
  equilibrium in asymmetric multi-players zero-sum game with only one alien","We consider the relation between Sion's minimax theorem for a continuous
function and a Nash equilibrium in an asymmetric multi-players zero-sum game in
which only one player is different from other players, and the game is
symmetric for the other players. Then,
  1. The existence of a Nash equilibrium, which is symmetric for players other
than one player, implies Sion's minimax theorem for pairs of this player and
one of other players with symmetry for the other players.
  2. Sion's minimax theorem for pairs of one player and one of other players
with symmetry for the other players implies the existence of a Nash equilibrium
which is symmetric for the other players.
  Thus, they are equivalent.",Econometrica,http://arxiv.org/abs/1806.07253v1,stat.ME
455,Empirical bias and efficiency of alpha-auctions: experimental evidence,"We experimentally evaluate the comparative performance of the winner-bid,
average-bid, and loser-bid auctions for the dissolution of a partnership. The
analysis of these auctions based on the empirical equilibrium refinement of
Velez and Brown (2020) arXiv:1907.12408 reveals that as long as behavior
satisfies weak payoff monotonicity, winner-bid and loser-bid auctions
necessarily exhibit a form of bias when empirical distributions of play
approximate best responses (Velez and Brown, 2020 arXiv:1905.08234). We find
support for both weak payoff monotonicity and the form of bias predicted by the
theory for these two auctions. Consistently with the theory, the average-bid
auction does not exhibit this form of bias. It has lower efficiency that the
winner-bid auction, however.",Econometrica,http://arxiv.org/abs/1905.03876v2,stat.AP
503,What are we weighting for? A mechanistic model for probability weighting,"Behavioural economics provides labels for patterns in human economic
behaviour. Probability weighting is one such label. It expresses a mismatch
between probabilities used in a formal model of a decision (i.e. model
parameters) and probabilities inferred from real people's decisions (the same
parameters estimated empirically). The inferred probabilities are called
""decision weights."" It is considered a robust experimental finding that
decision weights are higher than probabilities for rare events, and
(necessarily, through normalisation) lower than probabilities for common
events. Typically this is presented as a cognitive bias, i.e. an error of
judgement by the person. Here we point out that the same observation can be
described differently: broadly speaking, probability weighting means that a
decision maker has greater uncertainty about the world than the observer. We
offer a plausible mechanism whereby such differences in uncertainty arise
naturally: when a decision maker must estimate probabilities as frequencies in
a time series while the observer knows them a priori. This suggests an
alternative presentation of probability weighting as a principled response by a
decision maker to uncertainties unaccounted for in an observer's model.",Econometrica,http://arxiv.org/abs/2005.00056v1,econ.EM
402,"Price Competition with Geometric Brownian motion in Exchange Rate
  Uncertainty","We analyze an operational policy for a multinational manufacturer to hedge
against exchange rate uncertainties and competition. We consider a single
product and single period. Because of long-lead times, the capacity investment
must done before the selling season begins when the exchange rate between the
two countries is uncertain. we consider a duopoly competition in the foreign
country. We model the exchange rate as a random variable. We investigate the
impact of competition and exchange rate on optimal capacities and optimal
prices. We show how competition can impact the decision of the home
manufacturer to enter the foreign market.",Econometrica ,http://arxiv.org/abs/1804.08153v1,econ.EM
417,"Cluster-Robust Standard Errors for Linear Regression Models with Many
  Controls","It is common practice in empirical work to employ cluster-robust standard
errors when using the linear regression model to estimate some
structural/causal effect of interest. Researchers also often include a large
set of regressors in their model specification in order to control for observed
and unobserved confounders. In this paper we develop inference methods for
linear regression models with many controls and clustering. We show that
inference based on the usual cluster-robust standard errors by Liang and Zeger
(1986) is invalid in general when the number of controls is a non-vanishing
fraction of the sample size. We then propose a new clustered standard errors
formula that is robust to the inclusion of many controls and allows to carry
out valid inference in a variety of high-dimensional linear regression models,
including fixed effects panel data models and the semiparametric partially
linear model. Monte Carlo evidence supports our theoretical results and shows
that our proposed variance estimator performs well in finite samples. The
proposed method is also illustrated with an empirical application that
re-visits Donohue III and Levitt's (2001) study of the impact of abortion on
crime.",Econometrica ,http://arxiv.org/abs/1806.07314v3,stat.ME
495,"Bayesian shrinkage in mixture of experts models: Identifying robust
  determinants of class membership","A method for implicit variable selection in mixture of experts frameworks is
proposed. We introduce a prior structure where information is taken from a set
of independent covariates. Robust class membership predictors are identified
using a normal gamma prior. The resulting model setup is used in a finite
mixture of Bernoulli distributions to find homogenous clusters of women in
Mozambique based on their information sources on HIV. Fully Bayesian inference
is carried out via the implementation of a Gibbs sampler.",Econometrica ,http://arxiv.org/abs/1809.04853v2,econ.EM
353,Overconfidence and Prejudice,"We explore conclusions a person draws from observing society when he allows
for the possibility that individuals' outcomes are affected by group-level
discrimination. Injecting a single non-classical assumption, that the agent is
overconfident about himself, we explain key observed patterns in social
beliefs, and make a number of additional predictions. First, the agent believes
in discrimination against any group he is in more than an outsider does,
capturing widely observed self-centered views of discrimination. Second, the
more group memberships the agent shares with an individual, the more positively
he evaluates the individual. This explains one of the most basic facts about
social judgments, in-group bias, as well as ""legitimizing myths"" that justify
an arbitrary social hierarchy through the perceived superiority of the
privileged group. Third, biases are sensitive to how the agent divides society
into groups when evaluating outcomes. This provides a reason why some
ethnically charged questions should not be asked, as well as a potential
channel for why nation-building policies might be effective. Fourth, giving the
agent more accurate information about himself increases all his biases. Fifth,
the agent is prone to substitute biases, implying that the introduction of a
new outsider group to focus on creates biases against the new group but lowers
biases vis a vis other groups. Sixth, there is a tendency for the agent to
agree more with those in the same groups. As a microfoundation for our model,
we provide an explanation for why an overconfident agent might allow for
potential discrimination in evaluating outcomes, even when he initially did not
conceive of this possibility.",Econometrics and Statistics ,http://arxiv.org/abs/1909.08497v1,econ.EM
237,Constructing energy accounts for WIOD 2016 release,"Most of today's products and services are made in global supply chains. As a
result, a consumption of goods and services in one country is associated with
various environmental pressures all over the world due to international trade.
Advances in global multi-region input-output models have allowed researchers to
draw detailed, international supply-chain connections between production and
consumptions activities and associated environmental impacts. Due to a limited
data availability there is little evidence about the more recent trends in
global energy footprint. In order to expand the analytical potential of the
existing WIOD 2016 dataset to a wider range of research themes, this paper
develops energy accounts and presents the global energy footprint trends for
the period 2000-2014.",Econometrics Journal,http://arxiv.org/abs/1810.07112v1,econ.EM
80,"Population Growth and Economic Development in Bangladesh: Revisited
  Malthus","Bangladesh is the 2nd largest growing country in the world in 2016 with 7.1%
GDP growth. This study undertakes an econometric analysis to examine the
relationship between population growth and economic development. This result
indicates population growth adversely related to per capita GDP growth, which
means rapid population growth is a real problem for the development of
Bangladesh.",Econometrics Journal,http://arxiv.org/abs/1812.09393v2,econ.EM
279,"The interplay between migrants and natives as a determinant of migrants'
  assimilation: A coevolutionary approach","We study the migrants' assimilation, which we conceptualize as forming human
capital productive on the labor market of a developed host country, and we link
the observed frequent lack of assimilation with the relative deprivation that
the migrants start to feel when they move in social space towards the natives.
In turn, we presume that the native population is heterogenous and consists of
high-skill and low-skill workers. The presence of assimilated migrants might
shape the comparison group of the natives, influencing the relative deprivation
of the low-skill workers and, in consequence, the choice to form human capital
and become highly skilled. To analyse this interrelation between assimilation
choices of migrants and skill formation of natives, we construct a
coevolutionary model of the open-to-migration economy. Showing that the economy
might end up in a non-assimilation equilibrium, we discuss welfare consequences
of an assimilation policy funded from tax levied on the native population. We
identify conditions under which such costly policy can bring the migrants to
assimilation and at the same time increase the welfare of the natives, even
though the incomes of the former take a beating.",Econometrics Journal ,http://arxiv.org/abs/1906.02657v1,econ.EM
27,"Can GDP measurement be further improved? Data revision and
  reconciliation","Recent years have seen many attempts to combine expenditure-side estimates of
U.S. real output (GDE) growth with income-side estimates (GDI) to improve
estimates of real GDP growth. We show how to incorporate information from
multiple releases of noisy data to provide more precise estimates while
avoiding some of the identifying assumptions required in earlier work. This
relies on a new insight: using multiple data releases allows us to distinguish
news and noise measurement errors in situations where a single vintage does
not.
  Our new measure, GDP++, fits the data better than GDP+, the GDP growth
measure of Aruoba et al. (2016) published by the Federal Reserve Bank of
Philadephia. Historical decompositions show that GDE releases are more
informative than GDI, while the use of multiple data releases is particularly
important in the quarters leading up to the Great Recession.",Economic Alternatives Journal,http://arxiv.org/abs/1808.04970v1,econ.GN
28,"When Do Households Invest in Solar Photovoltaics? An Application of
  Prospect Theory","While investments in renewable energy sources (RES) are incentivized around
the world, the policy tools that do so are still poorly understood, leading to
costly misadjustments in many cases. As a case study, the deployment dynamics
of residential solar photovoltaics (PV) invoked by the German feed-in tariff
legislation are investigated. Here we report a model showing that the question
of when people invest in residential PV systems is found to be not only
determined by profitability, but also by profitability's change compared to the
status quo. This finding is interpreted in the light of loss aversion, a
concept developed in Kahneman and Tversky's Prospect Theory. The model is able
to reproduce most of the dynamics of the uptake with only a few financial and
behavioral assumptions",Economic Alternatives Journal,http://arxiv.org/abs/1808.05572v1,econ.GN
30,Quantifying the Computational Advantage of Forward Orthogonal Deviations,"Under suitable conditions, one-step generalized method of moments (GMM) based
on the first-difference (FD) transformation is numerically equal to one-step
GMM based on the forward orthogonal deviations (FOD) transformation. However,
when the number of time periods ($T$) is not small, the FOD transformation
requires less computational work. This paper shows that the computational
complexity of the FD and FOD transformations increases with the number of
individuals ($N$) linearly, but the computational complexity of the FOD
transformation increases with $T$ at the rate $T^{4}$ increases, while the
computational complexity of the FD transformation increases at the rate $T^{6}$
increases. Simulations illustrate that calculations exploiting the FOD
transformation are performed orders of magnitude faster than those using the FD
transformation. The results in the paper indicate that, when one-step GMM based
on the FD and FOD transformations are the same, Monte Carlo experiments can be
conducted much faster if the FOD version of the estimator is used.",Economic Alternatives Journal,http://arxiv.org/abs/1808.05995v1,econ.GN
201,Fair and Efficient Division among Families,"Is efficiency consistent with fairness? Our approach to this question
concerns the case where multiple individuals with diverse preferences are bound
to consume the same bundle. Families are our lead example: the father, mother
and children get to consume the same garden, kitchen, and vacations. We adapt
each of the three most popular principles of fairness: envy-freeness,
egalitarian-equivalence and the fair-share guarantee, in three different ways
to the world of families. For any given criterion of fairness, an allocation is
*unanimous-fair* if it is fair according to every individual member of each
family, it is *aggregate-fair* if it is fair according to a particular
aggregation of all family members' preferences, and it is *collective-fair* if
it is fair according to the family's -- typically incomplete -- preferences,
that rank a bundle above another bundle if and only if each member of the
family ranks the first bundle above the second. While efficiency is generally
incompatible with unanimous egalitarian equivalence, and incompatible with
unanimous envy-freeness in economies with three or more families, unanimously
envy-free efficient allocations always exist in economies with just two
families. The unanimous fair share guarantee is easy to achieve: Under generic
conditions the set of efficient allocations with the fair share guarantee
contains some collectively envy-free and some collectively egalitarian
equivalent allocations. We use modified versions of the traditional market
equilibrium approach and lexicographic optimization to establish our results.",Economic Analysis ,http://arxiv.org/abs/1811.06684v3,econ.GN
286,"Existence and Uniqueness of Solutions to the Stochastic Bellman Equation
  with Unbounded Shock","In this paper we develop a general framework to analyze stochastic dynamic
problems with unbounded utility functions and correlated and unbounded shocks.
We obtain new results of the existence and uniqueness of solutions to the
Bellman equation through a general fixed point theorem that generalizes known
results for Banach contractions and local contractions. We study an endogenous
growth model as well as the Lucas asset pricing model in an exchange economy,
significantly expanding their range of applicability.",Economic and Political Weekly,http://arxiv.org/abs/1907.07343v1,econ.GN
98,"The Impact of Sex Education on Sexual Activity, Pregnancy, and Abortion","The purpose of this study is to find a relation between sex education and
abortion in the United States. Accordingly, multivariate logistic regression is
employed to study the relation between abortion and frequency of sex,
pre-marriage sex, and pregnancy by rape. The finding shows the odds of abortion
among those who have had premarital sex, more frequent sex before marriage, and
been the victim of rape is higher than those who have not experienced any of
these incidents. The output identified with one unit increase in pre-marriage
sex the log-odds of abortion increases by 0.47. Similarly, it shows by one unit
increase in the frequency of sex, the log-odds of abortion increases by 0.39.
Also, for every additional pregnancy by rape, there is an expectation of a 3.17
increase in the log-odds of abortion. The findings of this study also suggests
abortion is associated with sex education. Despite previous findings, this
study shows the factors of age, having children, and social standing is not
considered a burden to parents and thereby do not have a causal relation to
abortion.",Economic Archive ,http://arxiv.org/abs/1903.08307v1,econ.GN
145,Uncertainty and Robustness of Surplus Extraction,"This paper studies a robust version of the classic surplus extraction
problem, in which the designer knows only that the beliefs of each type belong
to some set, and designs mechanisms that are suitable for all possible beliefs
in that set. We derive necessary and sufficient conditions for full extraction
in this setting, and show that these are natural set-valued analogues of the
classic convex independence condition identified by Cremer and McLean (1985,
1988). We show that full extraction is neither generically possible nor
generically impossible, in contrast to the standard setting in which full
extraction is generic. When full extraction fails, we show that natural
additional conditions can restrict both the nature of the contracts a designer
can offer and the surplus the designer can obtain.","Economic Computation and Economic Cybernetics Studies and
Research",http://arxiv.org/abs/1811.01320v2,econ.GN
172,"Influence of introducing high speed railways on intercity travel
  behavior in Vietnam","It is one of hottest topics in Vietnam whether to construct a High Speed Rail
(HSR) system or not in near future. To analyze the impacts of introducing the
HSR on the intercity travel behavior, this research develops an integrated
intercity demand forecasting model to represent trip generation and frequency,
destination choice and travel mode choice behavior. For this purpose, a
comprehensive questionnaire survey with both Revealed Preference (RP)
information (an inter-city trip diary) and Stated Preference (SP) information
was conducted in Hanoi in 2011. In the SP part, not only HSR, but also Low Cost
Carrier is included in the choice set, together with other existing inter-city
travel modes. To make full use of the advantages of each type of data and to
overcome their disadvantages, RP and SP data are combined to describe the
destination choice and mode choice behavior, while trip generation and
frequency are represented by using the RP data. The model estimation results
show the inter-relationship between trip generation and frequency, destination
choice and travel mode choice, and confirm that those components should not
dealt with separately.",Economic Theory,http://arxiv.org/abs/1810.00155v1,econ.EM
214,Herding driven by the desire to differ,"Observational learning often involves congestion: an agent gets lower payoff
from an action when more predecessors have taken that action. This preference
to act differently from previous agents may paradoxically increase all but one
agent's probability of matching the actions of the predecessors. The reason is
that when previous agents conform to their predecessors despite the preference
to differ, their actions become more informative. The desire to match
predecessors' actions may reduce herding by a similar reasoning.",Economic Theory,http://arxiv.org/abs/1904.00454v1,econ.GN
243,"The Case for Formation of ISP-Content Providers Consortiums by Nash
  Bargaining for Internet Content Delivery","The formation of consortiums of a broadband access Internet Service Provider
(ISP) and multiple Content Providers (CP) is considered for large-scale content
caching. The consortium members share costs from operations and investments in
the supporting infrastructure. Correspondingly, the model's cost function
includes marginal and fixed costs; the latter has been important in determining
industry structure. Also, if Net Neutrality regulations permit, additional
network capacity on the ISP's last mile may be contracted by the CPs. The
number of subscribers is determined by a combination of users' price elasticity
of demand and Quality of Experience. The profit generated by a coalition after
pricing and design optimization determines the game's characteristic function.
Coalition formation is by a bargaining procedure due to Okada (1996) based on
random proposers in a non-cooperative, multi-player game-theoretic framework. A
necessary and sufficient condition is obtained for the Grand Coalition to form,
which bounds subsidies from large to small contributors. Caching is generally
supported even under Net Neutrality regulations. The Grand Coalition's profit
matches upper bounds. Numerical results illustrate the analytic results.",Economic Theory,http://arxiv.org/abs/1810.10660v1,stat.ME
319,"Academic Engagement and Commercialization in an Institutional Transition
  Environment: Evidence from Shanghai Maritime University","Does academic engagement accelerate or crowd out the commercialization of
university knowledge? Research on this topic seldom considers the impact of the
institutional environment, especially when a formal institution for encouraging
the commercial activities of scholars has not yet been established. This study
investigates this question in the context of China, which is in the
institutional transition stage. Based on a survey of scholars from Shanghai
Maritime University, we demonstrate that academic engagement has a positive
impact on commercialization and that this impact is greater for risk-averse
scholars than for other risk-seeking scholars. Our results suggest that in an
institutional transition environment, the government should consider
encouraging academic engagement to stimulate the commercialization activities
of conservative scholars.",Economic Theory,http://arxiv.org/abs/1901.07725v1,econ.TH
348,"Revisiting the thermal and superthermal two-class distribution of
  incomes: A critical perspective","This paper offers a two-pronged critique of the empirical investigation of
the income distribution performed by physicists over the past decade. Their
finding rely on the graphical analysis of the observed distribution of
normalized incomes. Two central observations lead to the conclusion that the
majority of incomes are exponentially distributed, but neither each individual
piece of evidence nor their concurrent observation robustly proves that the
thermal and superthermal mixture fits the observed distribution of incomes
better than reasonable alternatives. A formal analysis using popular measures
of fit shows that while an exponential distribution with a power-law tail
provides a better fit of the IRS income data than the log-normal distribution
(often assumed by economists), the thermal and superthermal mixture's fit can
be improved upon further by adding a log-normal component. The economic
implications of the thermal and superthermal distribution of incomes, and the
expanded mixture are explored in the paper.",Economic Theory,http://arxiv.org/abs/1804.06341v1,econ.TH
309,"Digital Economy And Society. A Cross Country Comparison Of Hungary And
  Ukraine","We live in the Digital Age in which both economy and society have been
transforming significantly. The Internet and the connected digital devices are
inseparable parts of our daily life and the engine of the economic growth. In
this paper, first I analyzed the status of digital economy and society in
Hungary, then compared it with Ukraine and made conclusions regarding the
future development tendencies. Using secondary data provided by the European
Commission I investigated the five components of the Digital Economy and
Society Index of Hungary. I performed cross country analysis to find out the
significant differences between Ukraine and Hungary in terms of access to the
Internet and device use including smartphones, computers and tablets. Based on
my findings, I concluded that Hungary is more developed in terms of the
significant parameters of the digital economy and society than Ukraine, but
even Hungary is an emerging digital nation. Considering the high growth rate of
Internet, tablet and smartphone penetration in both countries, I expect faster
progress in the development of the digital economy and society in Hungary and
Ukraine.",Economic Theory ,http://arxiv.org/abs/1901.00283v1,econ.EM
429,Alternative Axioms in Group Identification Problems,"Kasher and Rubinstein (1997) introduced the problem of classifying the
members of a group in terms of the opinions of their potential members. This
involves a finite set of agents $N = \{1,2,\ldots,n\}$, each one having an
opinion about which agents should be classified as belonging to a specific
subgroup J. A Collective Identity Function (CIF) aggregates those opinions
yielding the class of members deemed $J$. Kasher and Rubinstein postulate
axioms, intended to ensure fair and socially desirable outcomes, characterizing
different CIFs. We follow their lead by replacing their liberal axiom by other
axioms, constraining the spheres of influence of the agents. We show that some
of them lead to different CIFs while in another instance we find an
impossibility result.",Economic Theory ,http://arxiv.org/abs/1912.05961v1,math.ST
47,"Ordered Kripke Model, Permissibility, and Convergence of Probabilistic
  Kripke Model","We define a modification of the standard Kripke model, called the ordered
Kripke model, by introducing a linear order on the set of accessible states of
each state. We first show this model can be used to describe the lexicographic
belief hierarchy in epistemic game theory, and perfect rationalizability can be
characterized within this model. Then we show that each ordered Kripke model is
the limit of a sequence of standard probabilistic Kripke models with a modified
(common) belief operator, in the senses of structure and the
(epsilon-)permissibilities characterized within them.",Economic Theory ,http://arxiv.org/abs/1801.08767v1,econ.EM
104,"A study of strategy to the remove and ease TBT for increasing export in
  GCC6 countries","The last technical barriers to trade(TBT) between countries are Non-Tariff
Barriers(NTBs), meaning all trade barriers are possible other than Tariff
Barriers. And the most typical examples are (TBT), which refer to measure
Technical Regulation, Standards, Procedure for Conformity Assessment, Test &
Certification etc. Therefore, in order to eliminate TBT, WTO has made all
membership countries automatically enter into an agreement on TBT",Economic Theory ,http://arxiv.org/abs/1803.03394v3,econ.TH
112,"Schooling Choice, Labour Market Matching, and Wages","We develop inference for a two-sided matching model where the characteristics
of agents on one side of the market are endogenous due to pre-matching
investments. The model can be used to measure the impact of frictions in labour
markets using a single cross-section of matched employer-employee data. The
observed matching of workers to firms is the outcome of a discrete, two-sided
matching process where firms with heterogeneous preferences over education
sequentially choose workers according to an index correlated with worker
preferences over firms. The distribution of education arises in equilibrium
from a Bayesian game: workers, knowing the distribution of worker and firm
types, invest in education prior to the matching process. Although the observed
matching exhibits strong cross-sectional dependence due to the matching
process, we propose an asymptotically valid inference procedure that combines
discrete choice methods with simulation.",Economic Theory ,http://arxiv.org/abs/1803.09020v6,econ.EM
182,Identifying the Effect of Persuasion,"This paper examines a commonly used measure of persuasion whose precise
interpretation has been lacking in the literature. Through the lens of the
potential outcome framework, we first define the persuasion rate at the
population level by a proper conditional probability of the agent taking an
action of interest with a persuasive message given that the agent does not take
the action without the conveyed message. We then formally study identification
under empirically relevant data scenarios and show that the commonly adopted
measure of persuasion does not estimate the causal rate of persuasion on any
subpopulation in general. As a whole, the persuasion rate is partially
identified with a binary instrument. The sharp lower bound is easy to estimate
and provides a robust measure of persuasion. When the population of interest is
limited to the group of compliers, a local persuasion rate can be
point-identified only under the most favorable data scenario. We further
establish that a continuous instrument opens up the possibility of point
identification for a policy-relevant subpopulation. We provide practical
methods for causal inference and revisit the literature to illustrate that the
standard measure tends to overstate the persuasive effects while masking
underlying heterogeneity.",Economic Theory ,http://arxiv.org/abs/1812.02276v4,econ.TH
323,"The Importance of Social and Government Learning in Ex Ante Policy
  Evaluation","We provide two methodological insights on \emph{ex ante} policy evaluation
for macro models of economic development. First, we show that the problems of
parameter instability and lack of behavioral constancy can be overcome by
considering learning dynamics. Hence, instead of defining social constructs as
fixed exogenous parameters, we represent them through stable functional
relationships such as social norms. Second, we demonstrate how agent computing
can be used for this purpose. By deploying a model of policy prioritization
with endogenous government behavior, we estimate the performance of different
policy regimes. We find that, while strictly adhering to policy recommendations
increases efficiency, the nature of such recipes has a bigger effect. In other
words, while it is true that lack of discipline is detrimental to prescription
outcomes (a common defense of failed recommendations), it is more important
that such prescriptions consider the systemic and adaptive nature of the
policymaking process (something neglected by traditional technocratic advice).",Economic Theory ,http://arxiv.org/abs/1902.00429v1,econ.TH
149,A Model of Competing Narratives,"We formalize the argument that political disagreements can be traced to a
""clash of narratives"". Drawing on the ""Bayesian Networks"" literature, we model
a narrative as a causal model that maps actions into consequences, weaving a
selection of other random variables into the story. An equilibrium is defined
as a probability distribution over narrative-policy pairs that maximizes a
representative agent's anticipatory utility, capturing the idea that public
opinion favors hopeful narratives. Our equilibrium analysis sheds light on the
structure of prevailing narratives, the variables they involve, the policies
they sustain and their contribution to political polarization.",Economic Theory Bulletin,http://arxiv.org/abs/1811.04232v1,econ.TH
392,"Eliciting Preferences of Ridehailing Users and Drivers: Evidence from
  the United States","Transportation Network Companies (TNCs) are changing the transportation
ecosystem, but micro-decisions of drivers and users need to be better
understood to assess the system-level impacts of TNCs. In this regard, we
contribute to the literature by estimating a) individuals' preferences of being
a rider, a driver, or a non-user of TNC services; b) preferences of ridehailing
users for ridepooling; c) TNC drivers' choice to switch to vehicles with better
fuel economy, and also d) the drivers' decision to buy, rent or lease new
vehicles with driving for TNCs being a major consideration. Elicitation of
drivers' preferences using a unique sample (N=11,902) of the U.S. population
residing in TNC-served areas is the key feature of this study. The statistical
analysis indicates that ridehailing services are mainly attracting personal
vehicle users as riders, without substantially affecting demand for transit.
Moreover, around 10% of ridehailing users reported postponing the purchase of a
new car due to the availability of TNC services. The model estimation results
indicate that the likelihood of being a TNC user increases with the increase in
age for someone younger than 44 years, but the pattern is reversed post 44
years. This change in direction of the marginal effect of age is insightful as
the previous studies have reported a negative association. We also find that
postgraduate drivers who live in metropolitan regions are more likely to switch
to fuel-efficient vehicles. These findings would inform transportation planners
and TNCs in developing policies to improve the fuel economy of the fleet.",Economic Theory Bulletin,http://arxiv.org/abs/1904.06695v1,q-fin.PM
609,New Policy Design for Food Accessibility to the People in Need,"Food insecurity is a term used to measure hunger and food deprivation of a
large population. As per the 2015 statistics provided by Feeding America - one
of the largest domestic hunger-relief organizations in the United States, 42.2
million Americans live in food insecure households, including 29.1 million
adults and 13.1 million children. This constitutes about 13.1% of households
that are food insecure. Food Banks have been developed to improve food security
for the needy. We have developed a novel food distribution policy using
suitable welfare and poverty indices and functions. In this work, we propose an
equitable and fair distribution of donated foods as per the demands and
requirements of the people, thus ensuring minimum wastage of food (perishable
and non-perishable) with focus towards nutrition. We present results and
analysis based on the application of the proposed policy using the information
of a local food bank as a case study. The results show that the new policy
performs better than the current methods in terms of population being covered
and reduction of food wastage obtaining suitable levels of nutrition.",Economic Theory Bulletin,http://arxiv.org/abs/1909.08648v1,q-fin.PM
100,Synthetic Control Methods and Big Data,"Many macroeconomic policy questions may be assessed in a case study
framework, where the time series of a treated unit is compared to a
counterfactual constructed from a large pool of control units. I provide a
general framework for this setting, tailored to predict the counterfactual by
minimizing a tradeoff between underfitting (bias) and overfitting (variance).
The framework nests recently proposed structural and reduced form machine
learning approaches as special cases. Furthermore, difference-in-differences
with matching and the original synthetic control are restrictive cases of the
framework, in general not minimizing the bias-variance objective. Using
simulation studies I find that machine learning methods outperform traditional
methods when the number of potential controls is large or the treated unit is
substantially different from the controls. Equipped with a toolbox of
approaches, I revisit a study on the effect of economic liberalisation on
economic growth. I find effects for several countries where no effect was found
in the original study. Furthermore, I inspect how a systematically important
bank respond to increasing capital requirements by using a large pool of banks
to estimate the counterfactual. Finally, I assess the effect of a changing
product price on product sales using a novel scanner dataset.",Economics & Politics,http://arxiv.org/abs/1803.00096v1,econ.GN
92,"Heterogeneous Impact of the Minimum Wage: Implications for Changes in
  Between- and Within-group Inequality","Workers who earn at or below the minimum wage in the United States are mostly
either less educated, young, or female. Little is known, however, concerning
the extent to which the minimum wage influences wage differentials among
workers with different observed characteristics and among workers with the same
observed characteristics. This paper shows that changes in the real value of
the minimum wage over recent decades have affected the relationship of hourly
wages with education, experience, and gender. The results suggest that changes
in the real value of the minimum wage account in part for the patterns of
changes in education, experience, and gender wage differentials and mostly for
the patterns of changes in within-group wage differentials among female workers
with lower levels of experience.",Economics Bulletin,http://arxiv.org/abs/1903.03925v2,econ.TH
133,The Core of an Economy with an Endogenous Social Division of Labour,"This paper considers the core of a competitive market economy with an
endogenous social division of labour. The theory is founded on the notion of a
""consumer-producer"", who consumes as well as produces commodities. First, we
show that the Core of such an economy with an endogenous social division of
labour can be founded on deviations of coalitions of arbitrary size, extending
the seminal insights of Vind and Schmeidler for pure exchange economies.
Furthermore, we establish the equivalence between the Core and the set of
competitive equilibria for continuum economies with an endogenous social
division of labour. Our analysis also concludes that self-organisation in a
social division of labour can be incorporated into the Edgeworthian barter
process directly. This is formulated as a Core equivalence result stated for a
Structured Core concept based on renegotiations among fully specialised
economic agents, i.e., coalitions that use only fully developed internal
divisions of labour. Our approach bridges the gap between standard economies
with social production and coalition production economies. Therefore, a more
straightforward and natural interpretation of coalitional improvement and the
Core can be developed than for coalition production economies.",Economics Bulletin,http://arxiv.org/abs/1809.01470v1,econ.GN
473,"Singularities and Catastrophes in Economics: Historical Perspectives and
  Future Directions","Economic theory is a mathematically rich field in which there are
opportunities for the formal analysis of singularities and catastrophes. This
article looks at the historical context of singularities through the work of
two eminent Frenchmen around the late 1960s and 1970s. Ren\'e Thom (1923-2002)
was an acclaimed mathematician having received the Fields Medal in 1958,
whereas G\'erard Debreu (1921-2004) would receive the Nobel Prize in economics
in 1983. Both were highly influential within their fields and given the
fundamental nature of their work, the potential for cross-fertilisation would
seem to be quite promising. This was not to be the case: Debreu knew of Thom's
work and cited it in the analysis of his own work, but despite this and other
applied mathematicians taking catastrophe theory to economics, the theory never
achieved a lasting following and relatively few results were published. This
article reviews Debreu's analysis of the so called ${\it regular}$ and ${\it
crtitical}$ economies in order to draw some insights into the economic
perspective of singularities before moving to how singularities arise naturally
in the Nash equilibria of game theory. Finally a modern treatment of stochastic
game theory is covered through recent work on the quantal response equilibrium.
In this view the Nash equilibrium is to the quantal response equilibrium what
deterministic catastrophe theory is to stochastic catastrophe theory, with some
caveats regarding when this analogy breaks down discussed at the end.",Economics Bulletin,http://arxiv.org/abs/1907.05582v1,math.OC
4,Brexit: The Belated Threat,"Debates on an EU-leaving referendum arose in several member states after
Brexit. We want to highlight how the exit of an additional country affects the
power distribution in the Council of the European Union. We inspect the power
indices of the member states both with and without the country which might
leave the union. Our results show a pattern connected to a change in the
threshold of the number of member states required for a decision. An exit that
modifies this threshold benefits the countries with high population, while an
exit that does not cause such a change benefits the small member states.
According to our calculations, the threat of Brexit would have worked
differently before the entry of Croatia.",Economics Letters,http://arxiv.org/abs/1808.05142v1,econ.TH
16,Information Technologies in Public Administration,"There are visible changes in the world organization, environment and health
of national conscience that create a background for discussion on possible
redefinition of global, state and regional management goals. The author applies
the sustainable development criteria to a hierarchical management scheme that
is to lead the world community to non-contradictory growth. Concrete
definitions are discussed in respect of decision-making process representing
the state mostly. With the help of systems analysis it is highlighted how to
understand who would carry the distinctive sign of world leadership in the
nearest future.",Economics Letters,http://arxiv.org/abs/1805.12107v1,econ.TH
17,Lessons from the History of European EMU,"This paper examines the history of previous examples of EMU from the
viewpoint that state actors make decisions about whether to participate in a
monetary union based on rational self-interest concerning costs and benefits to
their national economies. Illustrative examples are taken from nineteenth
century German, Italian and Japanese attempts at monetary integration with
early twentieth century ones from the Latin Monetary Union and the Scandinavian
Monetary Union and contemporary ones from the West African Monetary Union and
the European Monetary System. Lessons learned from the historical examples will
be used to identify issues that could arise with the move towards closer EMU in
Europe.",Economics Letters,http://arxiv.org/abs/1805.12112v1,econ.TH
86,Stealed-bid Auctions: Detecting Bid Leakage via Semi-Supervised Learning,"Bid leakage is a corrupt scheme in a first-price sealed-bid auction in which
the procurer leaks the opponents' bids to a favoured participant. The rational
behaviour of such participant is to bid close to the deadline in order to
receive all bids, which allows him to ensure his win at the best price
possible. While such behaviour does leave detectable traces in the data, the
absence of bid leakage labels makes supervised classification impossible.
Instead, we reduce the problem of the bid leakage detection to a
positive-unlabeled classification. The key idea is to regard the losing
participants as fair and the winners as possibly corrupted. This allows us to
estimate the prior probability of bid leakage in the sample, as well as the
posterior probability of bid leakage for each specific auction.
  We extract and analyze the data on 600,000 Russian procurement auctions
between 2014 and 2018. We find that around 9% of the auctions are exposed to
bid leakage, which results in an overall 1.5% price increase. The predicted
probability of bid leakage is higher for auctions with a higher reserve price,
with too low or too high number of participants, and if the winner has met the
auctioneer in earlier auctions.",Economics Letters,http://arxiv.org/abs/1903.00261v2,econ.TH
210,"Modelling transfer profits as externalities in a cooperative
  game-theoretic model of natural gas networks","Existing cooperative game theoretic studies of bargaining power in gas
pipeline systems are based on the so called characteristic function form (CFF).
This approach is potentially misleading if some pipelines fall under regulated
third party access (TPA). TPA, which is by now the norm in the EU, obliges the
owner of a pipeline to transport gas for others, provided they pay a regulated
transport fee. From a game theoretic perspective, this institutional setting
creates so called ""externalities,"" the description of which requires partition
function form (PFF) games. In this paper we propose a method to compute
payoffs, reflecting the power structure, for a pipeline system with regulated
TPA. The method is based on an iterative flow mechanism to determine gas flows
and transport fees for individual players and uses the recursive core and the
minimal claim function to convert the PPF game back into a CFF game, which can
be solved by standard methods. We illustrate the approach with a simple
stylized numerical example of the gas network in Central Eastern Europe with a
focus on Ukraine's power index as a major transit country.",Economics Letters,http://arxiv.org/abs/1901.11435v2,econ.EM
257,On the Effect of Imputation on the 2SLS Variance,"Endogeneity and missing data are common issues in empirical research. We
investigate how both jointly affect inference on causal parameters.
Conventional methods to estimate the variance, which treat the imputed data as
if it was observed in the first place, are not reliable. We derive the
asymptotic variance and propose a heteroskedasticity robust variance estimator
for two-stage least squares which accounts for the imputation. Monte Carlo
simulations support our theoretical findings.",Economics Letters,http://arxiv.org/abs/1903.11004v1,econ.TH
264,"Quasi-random Monte Carlo application in CGE systematic sensitivity
  analysis","The uncertainty and robustness of Computable General Equilibrium models can
be assessed by conducting a Systematic Sensitivity Analysis. Different methods
have been used in the literature for SSA of CGE models such as Gaussian
Quadrature and Monte Carlo methods. This paper explores the use of Quasi-random
Monte Carlo methods based on the Halton and Sobol' sequences as means to
improve the efficiency over regular Monte Carlo SSA, thus reducing the
computational requirements of the SSA. The findings suggest that by using
low-discrepancy sequences, the number of simulations required by the regular MC
SSA methods can be notably reduced, hence lowering the computational time
required for SSA of CGE models.",Economics Letters,http://arxiv.org/abs/1709.09755v1,econ.GN
5,Complexity of products: the effect of data regularisation,"Among several developments, the field of Economic Complexity (EC) has notably
seen the introduction of two new techniques. One is the Bootstrapped Selective
Predictability Scheme (SPSb), which can provide quantitative forecasts of the
Gross Domestic Product of countries. The other, Hidden Markov Model (HMM)
regularisation, denoises the datasets typically employed in the literature. We
contribute to EC along three different directions. First, we prove the
convergence of the SPSb algorithm to a well-known statistical learning
technique known as Nadaraya-Watson Kernel regression. The latter has
significantly lower time complexity, produces deterministic results, and it is
interchangeable with SPSb for the purpose of making predictions. Second, we
study the effects of HMM regularization on the Product Complexity and logPRODY
metrics, for which a model of time evolution has been recently proposed. We
find confirmation for the original interpretation of the logPRODY model as
describing the change in the global market structure of products with new
insights allowing a new interpretation of the Complexity measure, for which we
propose a modification. Third, we explore new effects of regularisation on the
data. We find that it reduces noise, and observe for the first time that it
increases nestedness in the export network adjacency matrix.",Economics Letters ,http://arxiv.org/abs/1808.08249v2,econ.TH
120,"The Finite Sample Performance of Treatment Effects Estimators based on
  the Lasso","This paper contributes to the literature on treatment effects estimation with
machine learning inspired methods by studying the performance of different
estimators based on the Lasso. Building on recent work in the field of
high-dimensional statistics, we use the semiparametric efficient score
estimation structure to compare different estimators. Alternative weighting
schemes are considered and their suitability for the incorporation of machine
learning estimators is assessed using theoretical arguments and various Monte
Carlo experiments. Additionally we propose an own estimator based on doubly
robust Kernel matching that is argued to be more robust to nuisance parameter
misspecification. In the simulation study we verify theory based intuition and
find good finite sample properties of alternative weighting scheme estimators
like the one we propose.",Economics Letters ,http://arxiv.org/abs/1805.05067v1,econ.EM
405,"Ill-posed Estimation in High-Dimensional Models with Instrumental
  Variables","This paper is concerned with inference about low-dimensional components of a
high-dimensional parameter vector $\beta^0$ which is identified through
instrumental variables. We allow for eigenvalues of the expected outer product
of included and excluded covariates, denoted by $M$, to shrink to zero as the
sample size increases. We propose a novel estimator based on desparsification
of an instrumental variable Lasso estimator, which is a regularized version of
2SLS with an additional correction term. This estimator converges to $\beta^0$
at a rate depending on the mapping properties of $M$ captured by a sparse link
condition. Linear combinations of our estimator of $\beta^0$ are shown to be
asymptotically normally distributed. Based on consistent covariance estimation,
our method allows for constructing confidence intervals and statistical tests
for single or low-dimensional components of $\beta^0$. In Monte-Carlo
simulations we analyze the finite sample behavior of our estimator.",Economics Letters ,http://arxiv.org/abs/1806.00666v2,q-fin.GN
411,Leave-out estimation of variance components,"We propose leave-out estimators of quadratic forms designed for the study of
linear models with unrestricted heteroscedasticity. Applications include
analysis of variance and tests of linear restrictions in models with many
regressors. An approximation algorithm is provided that enables accurate
computation of the estimator in very large datasets. We study the large sample
properties of our estimator allowing the number of regressors to grow in
proportion to the number of observations. Consistency is established in a
variety of settings where plug-in methods and estimators predicated on
homoscedasticity exhibit first-order biases. For quadratic forms of increasing
rank, the limiting distribution can be represented by a linear combination of
normal and non-central $\chi^2$ random variables, with normality ensuing under
strong identification. Standard error estimators are proposed that enable tests
of linear restrictions and the construction of uniformly valid confidence
intervals for quadratic forms of interest. We find in Italian social security
records that leave-out estimates of a variance decomposition in a two-way fixed
effects model of wage determination yield substantially different conclusions
regarding the relative contribution of workers, firms, and worker-firm sorting
to wage inequality than conventional methods. Monte Carlo exercises corroborate
the accuracy of our asymptotic approximations, with clear evidence of
non-normality emerging when worker mobility between blocks of firms is limited.",Economics Letters ,http://arxiv.org/abs/1806.01494v2,q-fin.GN
18,Implications of EMU for the European Community,"Monetary integration has both costs and benefits. Europeans have a strong
aversion to exchange rate instability. From this perspective, the EMS has shown
its limits and full monetary union involving a single currency appears to be a
necessity. This is the goal of the EMU project contained in the Maastricht
Treaty. This paper examines the pertinent choices: independence of the Central
Bank, budgetary discipline and economic policy coordination. Therefore, the
implications of EMU for the economic policy of France will be examined. If the
external force disappears, the public sector still cannot circumvent its
solvency constraint. The instrument of national monetary policy will not be
available so the absorption of asymmetric shocks will require greater wage
flexibility and fiscal policy will play a greater role. The paper includes
three parts. The first concerns the economic foundations of monetary union and
the costs it entails. The second is devoted to the institutional arrangements
under the Treaty of Maastricht. The third examines the consequences of monetary
union for the economy and the economic policy of France.",Economics of Energy and Environmental Policy,http://arxiv.org/abs/1805.12113v1,econ.GN
266,Forecasting with Dynamic Panel Data Models,"This paper considers the problem of forecasting a collection of short time
series using cross sectional information in panel data. We construct point
predictors using Tweedie's formula for the posterior mean of heterogeneous
coefficients under a correlated random effects distribution. This formula
utilizes cross-sectional information to transform the unit-specific (quasi)
maximum likelihood estimator into an approximation of the posterior mean under
a prior distribution that equals the population distribution of the random
coefficients. We show that the risk of a predictor based on a non-parametric
estimate of the Tweedie correction is asymptotically equivalent to the risk of
a predictor that treats the correlated-random-effects distribution as known
(ratio-optimality). Our empirical Bayes predictor performs well compared to
various competitors in a Monte Carlo study. In an empirical application we use
the predictor to forecast revenues for a large panel of bank holding companies
and compare forecasts that condition on actual and severely adverse
macroeconomic conditions.",Electronic Journal of Statistics,http://arxiv.org/abs/1709.10193v1,stat.ME
355,The converse envelope theorem,"I prove an envelope theorem with a converse: the envelope formula is
equivalent to a first-order condition. Like Milgrom and Segal's (2002) envelope
theorem, my result requires no structure on the choice set. I use the converse
envelope theorem to extend to abstract outcomes the canonical result in
mechanism design that any increasing allocation is implementable, and apply
this to selling information.",Electronic Proceedings in Theoretical Computer Science,http://arxiv.org/abs/1909.11219v4,cs.GT
128,"Banking Stability System: Does it Matter if the Rate of Return is Fixed
  or Stochastic?","The purpose is to compare the perfect Stochastic Return (SR) model like
Islamic banks to the Fixed Return (FR) model as in conventional banks by
measuring up their impacts at the macroeconomic level. We prove that if the
optimal choice of investor share in SR model {\alpha}* realizes the
indifference of the financial institution toward SR and FR models, there exists
{\alpha} less than {\alpha}* such that the banks strictly prefers the SR model.
Also, there exists {\alpha}, {\gamma} and {\lambda} verifying the conditions of
{\alpha}-sharing such that each party in economy can be better under the SR
model and the economic welfare could be improved in a Pareto-efficient way.",Empirical Economics ,http://arxiv.org/abs/1807.11102v1,econ.EM
224,Credit Scoring by Incorporating Dynamic Networked Information,"In this paper, the credit scoring problem is studied by incorporating
networked information, where the advantages of such incorporation are
investigated theoretically in two scenarios. Firstly, a Bayesian optimal filter
is proposed to provide risk prediction for lenders assuming that published
credit scores are estimated merely from structured financial data. Such
prediction can then be used as a monitoring indicator for the risk management
in lenders' future decisions. Secondly, a recursive Bayes estimator is further
proposed to improve the precision of credit scoring by incorporating the
dynamic interaction topology of clients. It is shown that under the proposed
evolution framework, the designed estimator has a higher precision than any
efficient estimator, and the mean square errors are strictly smaller than the
Cram\'er-Rao lower bound for clients within a certain range of scores. Finally,
simulation results for a special case illustrate the feasibility and
effectiveness of the proposed algorithms.",Empirical Economics ,http://arxiv.org/abs/1905.11795v2,econ.GN
270,Equity in Startups,"Startups have become in less than 50 years a major component of innovation
and economic growth. An important feature of the startup phenomenon has been
the wealth created through equity in startups to all stakeholders. These
include the startup founders, the investors, and also the employees through the
stock-option mechanism and universities through licenses of intellectual
property. In the employee group, the allocation to important managers like the
chief executive, vice-presidents and other officers, and independent board
members is also analyzed. This report analyzes how equity was allocated in more
than 400 startups, most of which had filed for an initial public offering. The
author has the ambition of informing a general audience about best practice in
equity split, in particular in Silicon Valley, the central place for startup
innovation.",Energies ,http://arxiv.org/abs/1711.00661v1,econ.GN
380,"Conservation or deterioration in heritage sites? Estimating willingness
  to pay for preservation","A significant part of the United Nations World Heritage Sites (WHSs) is
located in developing countries. These sites attract an increasing number of
tourist and income to these countries. Unfortunately, many of these WHSs are in
a poor condition due to climatic and environmental impacts; war and tourism
pressure, requiring the urgent need for restoration and preservation (Tuan &
Navrud, 2007). In this study, we characterise residents from Shiraz city
(visitors and non-visitors) willingness to invest in the management of the
heritage sites through models for the preservation of heritage and development
of tourism as a local resource. The research looks at different categories of
heritage sites within Shiraz city, Iran. The measurement instrument is a stated
preference referendum task administered state-wide to a sample of 489
respondents, with the payment mechanism defined as a purpose-specific
incremental levy of a fixed amount over a set period of years. A Latent Class
Binary Logit model, using parametric constraints is used innovatively to deal
with any strategic voting such as Yea-sayers and Nay-sayers, as well as
revealing the latent heterogeneity among sample members. Results indicate that
almost 14% of the sampled population is unwilling to be levied any amount
(Nay-sayers) to preserve any heritage sites. Not recognizing the presence of
nay-sayers in the data or recognizing them but eliminating them from the
estimation will result in biased Willingness to Pay (WTP) results and,
consequently, biased policy propositions by authorities. Moreover, it is found
that the type of heritage site is a driver of WTP. The results from this study
provide insights into the WTP of heritage site visitors and non-visitors with
respect to avoiding the impacts of future erosion and destruction and
contributing to heritage management and maintenance policies.",Energies ,http://arxiv.org/abs/1902.02418v1,econ.GN
396,"Shared factory: a new production node for social manufacturing in the
  context of sharing economy","Manufacturing industry is heading towards socialization, interconnection, and
platformization. Motivated by the infiltration of sharing economy usage in
manufacturing, this paper addresses a new factory model -- shared factory --
and provides a theoretical architecture and some actual cases for manufacturing
sharing. Concepts related to three kinds of shared factories which deal
respectively with sharing production-orders, manufacturing-resources and
manufacturing-capabilities, are defined accordingly. These three kinds of
shared factory modes can be used for building correspondent sharing
manufacturing ecosystems. On the basis of sharing economic analysis, we
identify feasible key enabled technologies for configuring and running a shared
factory. At the same time, opportunities and challenges of enabling the shared
factory are also analyzed in detail. In fact, shared factory, as a new
production node, enhances the sharing nature of social manufacturing paradigm,
fits the needs of light assets and gives us a new chance to use socialized
manufacturing resources. It can be drawn that implementing a shared factory
would reach a win-win way through production value-added transformation and
social innovation.",Energies ,http://arxiv.org/abs/1904.11377v1,econ.GN
161,Apologia Pro Vita Sua: The Vanishing of the White Whale in the Mists,"There are many analogies among fortune hunting in business, politics, and
science. The prime task of the gold digger was to go to the Klondikes, find the
right mine and mine the richest veins. This task requires motivation, sense of
purpose and ability. Techniques and equipment must be developed. Fortune
hunting in New England was provided at one time by hunting for whales. One went
to a great whalers' station such as New Bedford and joined the whale hunters.
The hunt in academic research is similar. A single-minded passion is called
for. These notes here are the wrap-up comments containing some terminal
observations of mine on a hunt for a theory money and financial institutions.",Energy,http://arxiv.org/abs/1807.09577v1,econ.GN
587,Reputation for Playing Mixed Actions: A Characterization Theorem,"A patient player privately observes a persistent state that directly affects
his myopic opponents' payoffs, and can be one of the several commitment types
that plays the same mixed action in every period. I characterize the set of
environments under which the patient player obtains at least his commitment
payoff in all equilibria regardless of his stage-game payoff function. Due to
interdependent values, the patient player cannot guarantee his mixed commitment
payoff by imitating the mixed-strategy commitment type, and small perturbations
to a pure commitment action can significantly reduce the patient player's
guaranteed equilibrium payoff.",Energy & Environmental Science ,http://arxiv.org/abs/2006.16206v2,math.OC
440,"Lattice structure of the random stable set in many-to-many matching
  market","For a many-to-many matching market, we study the lattice structure of the set
of random stable matchings. We define a partial order on the random stable set
and present two intuitive binary operations to compute the least upper bound
and the greatest lower bound for each side of the matching market. Then, we
prove that with these binary operations the set of random stable matchings
forms two dual lattices.",Energy Economics,http://arxiv.org/abs/2002.08156v5,q-fin.GN
254,"Bayesian MIDAS Penalized Regressions: Estimation, Selection, and
  Prediction","We propose a new approach to mixed-frequency regressions in a
high-dimensional environment that resorts to Group Lasso penalization and
Bayesian techniques for estimation and inference. In particular, to improve the
prediction properties of the model and its sparse recovery ability, we consider
a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing
the model shrinkage are automatically tuned via an adaptive MCMC algorithm. We
establish good frequentist asymptotic properties of the posterior of the
in-sample and out-of-sample prediction error, we recover the optimal posterior
contraction rate, and we show optimality of the posterior predictive density.
Simulations show that the proposed models have good selection and forecasting
performance in small samples, even when the design matrix presents
cross-correlation. When applied to forecasting U.S. GDP, our penalized
regressions can outperform many strong competitors. Results suggest that
financial variables may have some, although very limited, short-term predictive
content.",Energy Research & Social Science,http://arxiv.org/abs/1903.08025v3,econ.GN
332,"Simultaneous Confidence Intervals for High-dimensional Linear Models
  with Many Endogenous Variables","High-dimensional linear models with endogenous variables play an increasingly
important role in recent econometric literature. In this work we allow for
models with many endogenous variables and many instrument variables to achieve
identification. Because of the high-dimensionality in the second stage,
constructing honest confidence regions with asymptotically correct coverage is
non-trivial. Our main contribution is to propose estimators and confidence
regions that would achieve that. The approach relies on moment conditions that
have an additional orthogonal property with respect to nuisance parameters.
Moreover, estimation of high-dimension nuisance parameters is carried out via
new pivotal procedures. In order to achieve simultaneously valid confidence
regions we use a multiplier bootstrap procedure to compute critical values and
establish its validity.",Energy Research & Social Science ,http://arxiv.org/abs/1712.08102v4,econ.GN
333,"Structural analysis with mixed-frequency data: A MIDAS-SVAR model of US
  capital flows","We develop a new VAR model for structural analysis with mixed-frequency data.
The MIDAS-SVAR model allows to identify structural dynamic links exploiting the
information contained in variables sampled at different frequencies. It also
provides a general framework to test homogeneous frequency-based
representations versus mixed-frequency data models. A set of Monte Carlo
experiments suggests that the test performs well both in terms of size and
power. The MIDAS-SVAR is then used to study how monetary policy and financial
market volatility impact on the dynamics of gross capital inflows to the US.
While no relation is found when using standard quarterly data, exploiting the
variability present in the series within the quarter shows that the effect of
an interest rate shock is greater the longer the time lag between the month of
the shock and the end of the quarter",Energy Research & Social Science ,http://arxiv.org/abs/1802.00793v1,econ.GN
247,A Simple Combinatorial Model of World Economic History,"We use a simple combinatorial model of technological change to explain the
Industrial Revolution. The Industrial Revolution was a sudden large improvement
in technology, which resulted in significant increases in human wealth and life
spans. In our model, technological change is combining or modifying earlier
goods to produce new goods. The underlying process, which has been the same for
at least 200,000 years, was sure to produce a very long period of relatively
slow change followed with probability one by a combinatorial explosion and
sudden takeoff. Thus, in our model, after many millennia of relative quiescence
in wealth and technology, a combinatorial explosion created the sudden takeoff
of the Industrial Revolution.",Entrepreneurship Theory and Practice,http://arxiv.org/abs/1811.04502v1,econ.GN
258,Testing for Differences in Stochastic Network Structure,"How can one determine whether a community-level treatment, such as the
introduction of a social program or trade shock, alters agents' incentives to
form links in a network? This paper proposes analogues of a two-sample
Kolmogorov-Smirnov test, widely used in the literature to test the null
hypothesis of ""no treatment effects"", for network data. It first specifies a
testing problem in which the null hypothesis is that two networks are drawn
from the same random graph model. It then describes two randomization tests
based on the magnitude of the difference between the networks' adjacency
matrices as measured by the $2\to2$ and $\infty\to1$ operator norms. Power
properties of the tests are examined analytically, in simulation, and through
two real-world applications. A key finding is that the test based on the
$\infty\to1$ norm can be substantially more powerful than that based on the
$2\to2$ norm for the kinds of sparse and degree-heterogeneous networks common
in economics.",Entropy ,http://arxiv.org/abs/1903.11117v5,econ.TH
341,"Identifying the occurrence or non occurrence of cognitive bias in
  situations resembling the Monty Hall problem","People reason heuristically in situations resembling inferential puzzles such
as Bertrand's box paradox and the Monty Hall problem. The practical
significance of that fact for economic decision making is uncertain because a
departure from sound reasoning may, but does not necessarily, result in a
""cognitively biased"" outcome different from what sound reasoning would have
produced. Criteria are derived here, applicable to both experimental and
non-experimental situations, for heuristic reasoning in an inferential-puzzle
situations to result, or not to result, in cognitively bias. In some
situations, neither of these criteria is satisfied, and whether or not agents'
posterior probability assessments or choices are cognitively biased cannot be
determined.",Entropy ,http://arxiv.org/abs/1802.08935v1,econ.GN
368,Behavioral Equivalence of Extensive Game Structures,"Two extensive game structures with imperfect information are said to be
behaviorally equivalent if they share the same map (up to relabelings) from
profiles of structurally reduced strategies to induced terminal paths. We show
that this is the case if and only if one can be transformed into the other
through a composition of two elementary transformations, commonly known as
\textquotedblleft Interchanging of Simultaneous Moves\textquotedblright\ and
\textquotedblleft Coalescing Moves/Sequential Agent
Splitting.\textquotedblright",Entropy ,http://arxiv.org/abs/1911.02918v1,econ.GN
550,Granger causality on horizontal sum of Boolean algebras,"The intention of this paper is to discuss the mathematical model of causality
introduced by C.W.J. Granger in 1969. The Granger's model of causality has
become well-known and often used in various econometric models describing
causal systems, e.g., between commodity prices and exchange rates.
  Our paper presents a new mathematical model of causality between two measured
objects. We have slightly modified the well-known Kolmogorovian probability
model. In particular, we use the horizontal sum of set $\sigma$-algebras
instead of their direct product.",Entropy ,http://arxiv.org/abs/1810.01654v1,q-fin.ST
406,"Asymptotic Refinements of a Misspecification-Robust Bootstrap for
  Generalized Empirical Likelihood Estimators","I propose a nonparametric iid bootstrap procedure for the empirical
likelihood, the exponential tilting, and the exponentially tilted empirical
likelihood estimators that achieves asymptotic refinements for t tests and
confidence intervals, and Wald tests and confidence regions based on such
estimators. Furthermore, the proposed bootstrap is robust to model
misspecification, i.e., it achieves asymptotic refinements regardless of
whether the assumed moment condition model is correctly specified or not. This
result is new, because asymptotic refinements of the bootstrap based on these
estimators have not been established in the literature even under correct model
specification. Monte Carlo experiments are conducted in dynamic panel data
setting to support the theoretical finding. As an application, bootstrap
confidence intervals for the returns to schooling of Hellerstein and Imbens
(1999) are calculated. The result suggests that the returns to schooling may be
higher.",Environment and Planning A Economy and Space ,http://arxiv.org/abs/1806.00953v2,econ.GN
313,"When does privatization spur entrepreneurial performance? The moderating
  effect of institutional quality in an emerging market","We explore how institutional quality moderates the effectiveness of
privatization on entrepreneurs sales performance. To do this, we blend agency
theory and entrepreneurial cognition theory with insights from institutional
economics to develop a model of emerging market venture performance. Using data
from the World Banks Enterprise Survey of entrepreneurs in China, our results
suggest that private-owned enterprises (POEs) outperform state-owned
enterprises (SOEs) but only in environments with high-quality market
institutions. In environments with low-quality market institutions, SOEs
outperform POEs. These findings suggest that the effectiveness of privatization
on entrepreneurial performance is context-specific, which reveals more nuance
than previously has been attributed.",Environmental Modelling & Software ,http://arxiv.org/abs/1901.03356v1,physics.soc-ph
381,"Seasonality Effects on Consumers Preferences Over Quality Attributes of
  Different Beef Products","Using discrete choice modelling, the study investigates 946 American
consumers willingness-to-pay and preferences for diverse beef products. A novel
experiment was used to elicit the number of beef products that each consumer
would purchase. The range of products explored in this study included ground,
diced, roast, and six cuts of steaks (sirloin, tenderloin, flank, flap, New
York and cowboy or rib-eye). The outcome of the study suggests that US
consumers vary in their preferences for beef products by season. The presence
of a USDA certification logo is by far the most important factor affecting
consumers willingness to pay for all beef cuts, which is also heavily dependent
on season. In relation to packaging, US consumers have mixed preference for
different beef products by season. The results from a scaled adjusted ordered
logit model showed that after price, safety-related attributes such as
certification logos, types of packaging, and antibiotic free and organic
products are a stronger influence on American consumers choice. Furthermore, US
consumers on average purchase diced and roast products more often in winter
slow cooking season, than in summer, whereas New York strip and flank steak are
more popular in the summer grilling season. This study provides valuable
insights for businesses as well as policymakers to make inform decisions while
considering how consumers relatively value among different labelling and
product attributes by season and better address any ethical, safety and
aesthetic concerns that consumers might have.",Environmental Science & Technology ,http://arxiv.org/abs/1902.02419v1,cs.CY
242,"Deep Neural Networks for Choice Analysis: A Statistical Learning Theory
  Perspective","While researchers increasingly use deep neural networks (DNN) to analyze
individual choices, overfitting and interpretability issues remain as obstacles
in theory and practice. By using statistical learning theory, this study
presents a framework to examine the tradeoff between estimation and
approximation errors, and between prediction and interpretation losses. It
operationalizes the DNN interpretability in the choice analysis by formulating
the metrics of interpretation loss as the difference between true and estimated
choice probability functions. This study also uses the statistical learning
theory to upper bound the estimation error of both prediction and
interpretation losses in DNN, shedding light on why DNN does not have the
overfitting issue. Three scenarios are then simulated to compare DNN to binary
logit model (BNL). We found that DNN outperforms BNL in terms of both
prediction and interpretation for most of the scenarios, and larger sample size
unleashes the predictive power of DNN but not BNL. DNN is also used to analyze
the choice of trip purposes and travel modes based on the National Household
Travel Survey 2017 (NHTS2017) dataset. These experiments indicate that DNN can
be used for choice analysis beyond the current practice of demand forecasting
because it has the inherent utility interpretation, the flexibility of
accommodating various information formats, and the power of automatically
learning utility specification. DNN is both more predictive and interpretable
than BNL unless the modelers have complete knowledge about the choice task, and
the sample size is small. Overall, statistical learning theory can be a
foundation for future studies in the non-asymptotic data regime or using
high-dimensional statistical models in choice analysis, and the experiments
show the feasibility and effectiveness of DNN for its wide applications to
policy and behavioral analysis.",Environmental Science and Policy ,http://arxiv.org/abs/1810.10465v2,econ.GN
544,Sorting on the Used-Car Market After the Volkswagen Emission Scandal,"The disclosure of the VW emission manipulation scandal caused a
quasi-experimental market shock to the observable environmental quality of VW
diesel vehicles. To investigate the market reaction to this shock, we collect
data from a used-car online advertisement platform. We find that the supply of
used VW diesel vehicles increases after the VW emission scandal. The positive
supply side effects increase with the probability of manipulation. Furthermore,
we find negative impacts on the asking prices of used cars subject to a high
probability of manipulation. We rationalize these findings with a model for
sorting by the environmental quality of used cars.",EPJ Data Science,http://arxiv.org/abs/1908.09609v1,cs.SI
289,"The method of Eneström and Phragmén for parliamentary elections by
  means of approval voting","We study a method for proportional representation that was proposed at the
turn from the nineteenth to the twentieth century by Gustav Enestr\""om and
Edvard Phragm\'en. Like Phragm\'en's better-known iterative minimax method, it
is assumed that the voters express themselves by means of approval voting. In
contrast to the iterative minimax method, however, here one starts by fixing a
quota, i.e. the number of votes that give the right to a seat. As a matter of
fact, the method of Enestr\""om and Phragm\'en can be seen as an extension of
the method of largest remainders from closed lists to open lists, or also as an
adaptation of the single transferable vote to approval rather than preferential
voting. The properties of this method are studied and compared with those of
other methods of the same kind.",EPJ Data Science,http://arxiv.org/abs/1907.10590v1,econ.GN
60,"A theoretical framework to consider energy transfers within growth
  theory","Growth theory has rarely considered energy despite its invisible hand in all
physical systems. We develop a theoretical framework that places energy
transfers at centerstage of growth theory based on two principles: (1) goods
are material rearrangements and (2) such rearrangements are done by energy
transferred by prime movers (e.g. workers, engines). We derive the implications
of these principles for an autarkic agent that maximizes utility subject to an
energy budget constraint and maximizes energy surplus to relax such constraint.
The solution to these problems shows that growth is driven by positive marginal
energy surplus of energy goods (e.g. rice, oil), yet materializes through prime
mover accumulation. This perspective brings under one framework several results
from previous attempts to insert energy within growth theory, reconciles
economics with natural sciences, and provides a basis for a general
reinterpretation of economics and growth as the interplay between human desires
and thermodynamic processes.",European Economic Integration Review,http://arxiv.org/abs/1812.05091v1,econ.EM
72,"On the core of normal form games with a continuum of players : a
  correction","We study the core of normal form games with a continuum of players and
without side payments. We consider the weak-core concept, which is an
approximation of the core, introduced by Weber, Shapley and Shubik. For payoffs
depending on the players' strategy profile, we prove that the weak-core is
nonempty. The existence result establishes a weak-core element as a limit of
elements in weak-cores of appropriate finite games. We establish by examples
that our regularity hypotheses are relevant in the continuum case and the
weak-core can be strictly larger than the Aumann's $\alpha$-core. For games
where payoffs depend on the distribution of players' strategy profile, we prove
that analogous regularity conditions ensuring the existence of pure strategy
Nash equilibria are irrelevant for the non-vacuity of the weak-core.",European Economic Integration Review,http://arxiv.org/abs/1903.09819v1,econ.EM
20,"Shattering the glass ceiling? How the institutional context mitigates
  the gender gap in entrepreneurship","We examine how the institutional context affects the relationship between
gender and opportunity entrepreneurship. To do this, we develop a multi-level
model that connects feminist theory at the micro-level to institutional theory
at the macro-level. It is hypothesized that the gender gap in opportunity
entrepreneurship is more pronounced in low-quality institutional contexts and
less pronounced in high-quality institutional contexts. Using data from the
Global Entrepreneurship Monitor (GEM) and regulation data from the economic
freedom of the world index (EFW), we test our predictions and find evidence in
support of our model. Our findings suggest that, while there is a gender gap in
entrepreneurship, these disparities are reduced as the quality of the
institutional context improves.",European Economic Integration Review ,http://arxiv.org/abs/1812.03771v1,econ.GN
61,Causality: a decision theoretic approach,"We propose a decision-theoretic model akin to Savage (1972) that is useful
for defining causal effects. Within this framework, we define what it means for
a decision maker (DM) to act as if the relation between the two variables is
causal. Next, we provide axioms on preferences and show that these axioms are
equivalent to the existence of a (unique) Directed Acyclic Graph (DAG) that
represents the DM's preference. The notion of representation has two
components: the graph factorizes the conditional independence properties of the
DM's subjective beliefs, and arrows point from cause to effect. Finally, we
explore the connection between our representation and models used in the
statistical causality literature (for example, Pearl (1995)).",European Economic Integration Review ,http://arxiv.org/abs/1812.07414v5,econ.EM
284,Dynamically Stable Matching,"I introduce a stability notion, dynamic stability, for two-sided dynamic
matching markets where (i) matching opportunities arrive over time, (ii)
matching is one-to-one, and (iii) matching is irreversible. The definition
addresses two conceptual issues. First, since not all agents are available to
match at the same time, one must establish which agents are allowed to form
blocking pairs. Second, dynamic matching markets exhibit a form of externality
that is not present in static markets: an agent's payoff from remaining
unmatched cannot be defined independently of what other contemporaneous agents'
outcomes are. Dynamically stable matchings always exist. Dynamic stability is a
necessary condition to ensure timely participation in the economy by ensuring
that agents do not strategically delay the time at which they are available to
match.",European Economic Review,http://arxiv.org/abs/1906.11391v5,econ.GN
300,"The effects of non-tariff measures on agri-food trade: a review and
  meta-analysis of empirical evidence","The increasing policy interests and the vivid academic debate on non-tariff
measures (NTMs) has stimulated a growing literature on how NTMs affect agrifood
trade. The empirical literature provides contrasting and heterogeneous
evidence, with some studies supporting the standards as catalysts view, and
others favouring the standards as barriers explanation. To the extent that NTMs
can influence trade, understanding the prevailing effect, and the motivations
behind one effect or the other, is a pressing issue. We review a large body of
empirical evidence on the effect of NTMs on agri-food trade and conduct a
meta-analysis to disentangle potential determinants of heterogeneity in
estimates. Our findings show the role played by the publication process and by
study-specific assumptions. Some characteristics of the studies are correlated
with positive significant estimates, others covary with negative significant
estimates. Overall, we found that the effects of NTMs vary across types of
NTMs, proxy for NTMs, and levels of details of studies. Not negligible is the
influence of methodological issues and publication process.",European Journal of Business and Management Research,http://arxiv.org/abs/1811.06323v1,econ.GN
339,On the iterated estimation of dynamic discrete choice games,"We study the asymptotic properties of a class of estimators of the structural
parameters in dynamic discrete choice games. We consider K-stage policy
iteration (PI) estimators, where K denotes the number of policy iterations
employed in the estimation. This class nests several estimators proposed in the
literature such as those in Aguirregabiria and Mira (2002, 2007), Pesendorfer
and Schmidt-Dengler (2008), and Pakes et al. (2007). First, we establish that
the K-PML estimator is consistent and asymptotically normal for all K. This
complements findings in Aguirregabiria and Mira (2007), who focus on K=1 and K
large enough to induce convergence of the estimator. Furthermore, we show under
certain conditions that the asymptotic variance of the K-PML estimator can
exhibit arbitrary patterns as a function of K. Second, we establish that the
K-MD estimator is consistent and asymptotically normal for all K. For a
specific weight matrix, the K-MD estimator has the same asymptotic distribution
as the K-PML estimator. Our main result provides an optimal sequence of weight
matrices for the K-MD estimator and shows that the optimally weighted K-MD
estimator has an asymptotic distribution that is invariant to K. The invariance
result is especially unexpected given the findings in Aguirregabiria and Mira
(2007) for K-PML estimators. Our main result implies two new corollaries about
the optimal 1-MD estimator (derived by Pesendorfer and Schmidt-Dengler (2008)).
First, the optimal 1-MD estimator is optimal in the class of K-MD estimators.
In other words, additional policy iterations do not provide asymptotic
efficiency gains relative to the optimal 1-MD estimator. Second, the optimal
1-MD estimator is more or equally asymptotically efficient than any K-PML
estimator for all K. Finally, the appendix provides appropriate conditions
under which the optimal 1-MD estimator is asymptotically efficient.",European Journal of Government and Economics ,http://arxiv.org/abs/1802.06665v4,econ.GN
595,A Canon of Probabilistic Rationality,"We prove that a random choice rule satisfies Luce's Choice Axiom if and only
if its support is a choice correspondence that satisfies the Weak Axiom of
Revealed Preference, thus it consists of alternatives that are optimal
according to some preference, and random choice then occurs according to a tie
breaking among such alternatives that satisfies Renyi's Conditioning Axiom. Our
result shows that the Choice Axiom is, in a precise formal sense, a
probabilistic version of the Weak Axiom. It thus supports Luce's view of his
own axiom as a ""canon of probabilistic rationality.""",European Journal of Operational Research,http://arxiv.org/abs/2007.11386v2,stat.ME
185,"Fuzzy Difference-in-Discontinuities: Identification Theory and
  Application to the Affordable Care Act","This paper explores the use of a fuzzy regression discontinuity design where
multiple treatments are applied at the threshold. The identification results
show that, under the very strong assumption that the change in the probability
of treatment at the cutoff is equal across treatments, a
difference-in-discontinuities estimator identifies the treatment effect of
interest. The point estimates of the treatment effect using a simple fuzzy
difference-in-discontinuities design are biased if the change in the
probability of a treatment applying at the cutoff differs across treatments.
Modifications of the fuzzy difference-in-discontinuities approach that rely on
milder assumptions are also proposed. Our results suggest caution is needed
when applying before-and-after methods in the presence of fuzzy
discontinuities. Using data from the National Health Interview Survey, we apply
this new identification strategy to evaluate the causal effect of the
Affordable Care Act (ACA) on older Americans' health care access and
utilization.",European Journal of Operational Research ,http://arxiv.org/abs/1812.06537v3,econ.GN
372,"A Contribution to Theory of Factor Income Distribution, Cambridge
  Capital Controversy and Equity Premium Puzzle","Under very general conditions, we construct a micro-macro model for closed
economy with a large number of heterogeneous agents. By introducing both
financial capital (i.e. valued capital---- equities of firms) and physical
capital (i.e. capital goods), our framework gives a logically consistent,
complete factor income distribution theory with micro-foundation. The model
shows factor incomes obey different distribution rules at the micro and macro
levels, while marginal distribution theory and no-arbitrage princi-ple are
unified into a common framework. Our efforts solve the main problems of
Cambridge capital controversy, and reasonably explain the equity premium
puzzle. Strong empirical evidences support our results.",European Journal of Operational Research ,http://arxiv.org/abs/1911.12490v2,stat.AP
384,"Influencing factors that determine the usage of the crowd-shipping
  services","The objective of this study is to understand how senders choose shipping
services for different products, given the availability of both emerging
crowd-shipping (CS) and traditional carriers in a logistics market. Using data
collected from a US survey, Random Utility Maximization (RUM) and Random Regret
Minimization (RRM) models have been employed to reveal factors that influence
the diversity of decisions made by senders. Shipping costs, along with
additional real-time services such as courier reputations, tracking info,
e-notifications, and customized delivery time and location, have been found to
have remarkable impacts on senders' choices. Interestingly, potential senders
were willing to pay more to ship grocery items such as food, beverages, and
medicines by CS services. Moreover, the real-time services have low
elasticities, meaning that only a slight change in those services will lead to
a change in sender-behavior. Finally, data-science techniques were used to
assess the performance of the RUM and RRM models and found to have similar
accuracies. The findings from this research will help logistics firms address
potential market segments, prepare service configurations to fulfill senders'
expectations, and develop effective business operations strategies.",European Journal of Operational Research ,http://arxiv.org/abs/1902.08681v1,math.OC
90,"The Interdependence of Hierarchical Institutions: Federal Regulation,
  Job Creation, and the Moderating Effect of State Economic Freedom","Regulation is commonly viewed as a hindrance to entrepreneurship, but
heterogeneity in the effects of regulation is rarely explored. We focus on
regional variation in the effects of national-level regulations by developing a
theory of hierarchical institutional interdependence. Using the political
science theory of market-preserving federalism, we argue that regional economic
freedom attenuates the negative influence of national regulation on net job
creation. Using U.S. data, we find that regulation destroys jobs on net, but
regional economic freedom moderates this effect. In regions with average
economic freedom, a one percent increase in regulation results in 14 fewer jobs
created on net. However, a standard deviation increase in economic freedom
attenuates this relationship by four fewer jobs. Interestingly, this moderation
accrues strictly to older firms; regulation usually harms young firm job
creation, and economic freedom does not attenuate this relationship.",European Journal of Political Economy,http://arxiv.org/abs/1903.02924v1,econ.GN
442,A Model of Justification,"I consider decision-making constrained by considerations of morality,
rationality, or other virtues. The decision maker (DM) has a true preference
over outcomes, but feels compelled to choose among outcomes that are top-ranked
by some preference that he considers ""justifiable."" This model unites a broad
class of empirical work on distributional preferences, charitable donations,
prejudice/discrimination, and corruption/bribery. I provide a behavioral
characterization of the model. I also show that the set of justifications can
be identified from choice behavior when the true preference is known, and that
choice behavior substantially restricts both the true preference and
justifications when neither is known. I argue that the justifiability model
represents an advancement over existing models of rationalization because the
structure it places on possible ""rationales"" improves tractability,
interpretation and identification.",European Physical Journal B,http://arxiv.org/abs/2003.06844v1,physics.soc-ph
574,"Generalized Dynamic Factor Models and Volatilities: Consistency, rates,
  and prediction intervals","Volatilities, in high-dimensional panels of economic time series with a
dynamic factor structure on the levels or returns, typically also admit a
dynamic factor decomposition. We consider a two-stage dynamic factor model
method recovering the common and idiosyncratic components of both levels and
log-volatilities. Specifically, in a first estimation step, we extract the
common and idiosyncratic shocks for the levels, from which a log-volatility
proxy is computed. In a second step, we estimate a dynamic factor model, which
is equivalent to a multiplicative factor structure for volatilities, for the
log-volatility panel. By exploiting this two-stage factor approach, we build
one-step-ahead conditional prediction intervals for large $n \times T$ panels
of returns. Those intervals are based on empirical quantiles, not on
conditional variances; they can be either equal- or unequal- tailed. We provide
uniform consistency and consistency rates results for the proposed estimators
as both $n$ and $T$ tend to infinity. We study the finite-sample properties of
our estimators by means of Monte Carlo simulations. Finally, we apply our
methodology to a panel of asset returns belonging to the S&P100 index in order
to compute one-step-ahead conditional prediction intervals for the period
2006-2013. A comparison with the componentwise GARCH benchmark (which does not
take advantage of cross-sectional information) demonstrates the superiority of
our approach, which is genuinely multivariate (and high-dimensional),
nonparametric, and model-free.",European Physical Journal B,http://arxiv.org/abs/1811.10045v2,physics.soc-ph
599,Truthful Equilibria in Generalized Common Agency Models,"In this paper I discuss truthful equilibria in common agency models.
Specifically, I provide general conditions under which truthful equilibria are
plausible, easy to calculate and efficient. These conditions generalize similar
results in the literature and allow the use of truthful equilibria in novel
economic applications. Moreover, I provide two such applications. The first
application is a market game in which multiple sellers sell a uniform good to a
single buyer. The second application is a lobbying model in which there are
externalities in contributions between lobbies. This last example indicates
that externalities between principals do not necessarily prevent efficient
equilibria. In this regard, this paper provides a set of conditions, under
which, truthful equilibria in common agency models with externalities are
efficient.",Extremes ,http://arxiv.org/abs/2007.15942v1,stat.ME
62,"Duesenberry's Theory of Consumption: Habit, Learning, and Ratcheting","This paper investigates the consumption and risk taking decision of an
economic agent with partial irreversibility of consumption decision by
formalizing the theory proposed by Duesenberry (1949). The optimal policies
exhibit a type of the (s, S) policy: there are two wealth thresholds within
which consumption stays constant. Consumption increases or decreases at the
thresholds and after the adjustment new thresholds are set. The share of risky
investment in the agent's total investment is inversely U-shaped within the (s,
S) band, which generates time-varying risk aversion that can fluctuate widely
over time. This property can explain puzzles and questions on asset pricing and
households' portfolio choices, e.g., why aggregate consumption is so smooth
whereas the high equity premium is high and the equity return has high
volatility, why the risky share is so low whereas the estimated risk aversion
by the micro-level data is small, and whether and when an increase in wealth
has an impact on the risky share. Also, the partial irreversibility model can
explain both the excess sensitivity and the excess smoothness of consumption.",Filomat,http://arxiv.org/abs/1812.10038v1,econ.TH
346,Shapley Value Methods for Attribution Modeling in Online Advertising,"This paper re-examines the Shapley value methods for attribution analysis in
the area of online advertising. As a credit allocation solution in cooperative
game theory, Shapley value method directly quantifies the contribution of
online advertising inputs to the advertising key performance indicator (KPI)
across multiple channels. We simplify its calculation by developing an
alternative mathematical formulation. The new formula significantly improves
the computational efficiency and therefore extends the scope of applicability.
Based on the simplified formula, we further develop the ordered Shapley value
method. The proposed method is able to take into account the order of channels
visited by users. We claim that it provides a more comprehensive insight by
evaluating the attribution of channels at different stages of user conversion
journeys. The proposed approaches are illustrated using a real-world online
advertising campaign dataset.",Finance and Credit,http://arxiv.org/abs/1804.05327v1,econ.GN
343,"A Bayesian panel VAR model to analyze the impact of climate change on
  high-income economies","In this paper, we assess the impact of climate shocks on futures markets for
agricultural commodities and a set of macroeconomic quantities for multiple
high-income economies. To capture relations among countries, markets, and
climate shocks, this paper proposes parsimonious methods to estimate
high-dimensional panel VARs. We assume that coefficients associated with
domestic lagged endogenous variables arise from a Gaussian mixture model while
further parsimony is achieved using suitable global-local shrinkage priors on
several regions of the parameter space. Our results point towards pronounced
global reactions of key macroeconomic quantities to climate shocks. Moreover,
the empirical findings highlight substantial linkages between regionally
located climate shifts and global commodity markets.",Finance Research Letters,http://arxiv.org/abs/1804.01554v3,econ.GN
566,Nonparametric Analysis of Finite Mixtures,"Finite mixture models are useful in applied econometrics. They can be used to
model unobserved heterogeneity, which plays major roles in labor economics,
industrial organization and other fields. Mixtures are also convenient in
dealing with contaminated sampling models and models with multiple equilibria.
This paper shows that finite mixture models are nonparametrically identified
under weak assumptions that are plausible in economic applications. The key is
to utilize the identification power implied by information in covariates
variation. First, three identification approaches are presented, under distinct
and non-nested sets of sufficient conditions. Observable features of data
inform us which of the three approaches is valid. These results apply to
general nonparametric switching regressions, as well as to structural
econometric models, such as auction models with unobserved heterogeneity.
Second, some extensions of the identification results are developed. In
particular, a mixture regression where the mixing weights depend on the value
of the regressors in a fully unrestricted manner is shown to be
nonparametrically identifiable. This means a finite mixture model with
function-valued unobserved heterogeneity can be identified in a cross-section
setting, without restricting the dependence pattern between the regressor and
the unobserved heterogeneity. In this aspect it is akin to fixed effects panel
data models which permit unrestricted correlation between unobserved
heterogeneity and covariates. Third, the paper shows that fully nonparametric
estimation of the entire mixture model is possible, by forming a sample
analogue of one of the new identification strategies. The estimator is shown to
possess a desirable polynomial rate of convergence as in a standard
nonparametric estimation problem, despite nonregular features of the model.",Finance Research Letters ,http://arxiv.org/abs/1811.02727v1,q-fin.GN
567,"Nonparametric maximum likelihood methods for binary response models with
  random coefficients","Single index linear models for binary response with random coefficients have
been extensively employed in many econometric settings under various parametric
specifications of the distribution of the random coefficients. Nonparametric
maximum likelihood estimation (NPMLE) as proposed by Cosslett (1983) and
Ichimura and Thompson (1998), in contrast, has received less attention in
applied work due primarily to computational difficulties. We propose a new
approach to computation of NPMLEs for binary response models that significantly
increase their computational tractability thereby facilitating greater
flexibility in applications. Our approach, which relies on recent developments
involving the geometry of hyperplane arrangements, is contrasted with the
recently proposed deconvolution method of Gautier and Kitamura (2013). An
application to modal choice for the journey to work in the Washington DC area
illustrates the methods.",Financial History Review,http://arxiv.org/abs/1811.03329v3,econ.GN
500,Matching with Generalized Lexicographic Choice Rules,"Motivated by the need for real-world matching problems, this paper formulates
a large class of practical choice rules, Generalized Lexicographic Choice Rules
(GLCR), for institutions that consist of multiple divisions. Institutions fill
their divisions sequentially, and each division is endowed with a sub-choice
rule that satisfies classical substitutability and size monotonicity in
conjunction with a new property that we introduce, quota monotonicity. We allow
rich interactions between divisions in the form of capacity transfers. The
overall choice rule of an institution is defined as the union of the
sub-choices of its divisions. The cumulative offer mechanism (COM) with respect
to GLCR is the unique stable and strategy-proof mechanism. We define a
choice-based improvement notion and show that the COM respects improvements. We
employ the theory developed in this paper in our companion paper, Ayg\""un and
Turhan (2020), to design satisfactory matching mechanisms for India with
comprehensive affirmative action constraints.",Financial Innovation ,http://arxiv.org/abs/2004.13261v2,q-fin.GN
134,A note on contests with a constrained choice set of effort,"We consider a symmetric two-player contest, in which the choice set of effort
is constrained. We apply a fundamental property of the payoff function to show
that, under standard assumptions, there exists a unique Nash equilibrium in
pure strategies. It is shown that all equilibria are near the unconstrained
equilibrium. Perhaps surprisingly, this is not the case when players have
different prize evaluations.",Forum for Health Economics & Policy,http://arxiv.org/abs/1809.04436v11,econ.GN
528,Cities and space: Common power laws and spatial fractal structures,"City size distributions are known to be well approximated by power laws
across a wide range of countries. But such distributions are also meaningful at
other spatial scales, such as within certain regions of a country. Using data
from China, France, Germany, India, Japan, and the US, we first document that
large cities are significantly more spaced out than would be expected by chance
alone. We next construct spatial hierarchies for countries by first
partitioning geographic space using a given number of their largest cities as
cell centers, and then continuing this partitioning procedure within each cell
recursively. We find that city size distributions in different parts of these
spatial hierarchies exhibit power laws that are again far more similar than
would be expected by chance alone -- suggesting the existence of a spatial
fractal structure.",Frontiers in Big Data,http://arxiv.org/abs/1907.12289v1,physics.soc-ph
260,Bounds On Treatment Effects On Transitions,"This paper considers the identification of treatment effects on conditional
transition probabilities. We show that even under random assignment only the
instantaneous average treatment effect is point identified. Since treated and
control units drop out at different rates, randomization only ensures the
comparability of treatment and controls at the time of randomization, so that
long-run average treatment effects are not point identified. Instead we derive
informative bounds on these average treatment effects. Our bounds do not impose
(semi)parametric restrictions, for example, proportional hazards. We also
explore various assumptions such as monotone treatment response, common shocks
and positively correlated outcomes that tighten the bounds.",Frontiers in Environmental Science,http://arxiv.org/abs/1709.08981v1,econ.GN
127,"Exceeding Expectations: Stochastic Dominance as a General Decision
  Theory","The principle that rational agents should maximize expected utility or
choiceworthiness is intuitively plausible in many ordinary cases of
decision-making under uncertainty. But it is less plausible in cases of
extreme, low-probability risk (like Pascal's Mugging), and intolerably
paradoxical in cases like the St. Petersburg and Pasadena games. In this paper
I show that, under certain conditions, stochastic dominance reasoning can
capture most of the plausible implications of expectational reasoning while
avoiding most of its pitfalls. Specifically, given sufficient background
uncertainty about the choiceworthiness of one's options, many
expectation-maximizing gambles that do not stochastically dominate their
alternatives ""in a vacuum"" become stochastically dominant in virtue of that
background uncertainty. But, even under these conditions, stochastic dominance
will not require agents to accept options whose expectational superiority
depends on sufficiently small probabilities of extreme payoffs. The sort of
background uncertainty on which these results depend looks unavoidable for any
agent who measures the choiceworthiness of her options in part by the total
amount of value in the resulting world. At least for such agents, then,
stochastic dominance offers a plausible general principle of choice under
uncertainty that can explain more of the apparent rational constraints on such
choices than has previously been recognized.",Frontiers in Physics,http://arxiv.org/abs/1807.10895v5,econ.GN
532,The Time Importance for Prospect Theory,"This paper highlights that dynamic gambles can affect perceptions and reveal
behaviors that Kahneman and Tversky's Prospect Theory does not describe. The
model discussed here relates the Tsallis entropy to the Kelly's rate to
evaluate whether individuals prospect risk situations through nonadditive
dynamics or not. Then, psychology students answer a questionnaire where
additive prospects have null contrast, but other dynamics can present very high
contrast. Thus, we can note that a simple approximation between the losses and
the reference point leads individuals to risk aversion. This behavior
contradicts the risk seeking predicted by Prospect Theory. In addition, the
experiments show that two of the behaviors in the fourfold pattern of risk
attitudes also are violated. In essence, this work reveals that individuals can
prospect nonadditive dynamics in their gambles when the results are close (or
beyond) to the reference point. This dynamic aspect violates any additive
prospect models.",Frontiers in Public Health ,http://arxiv.org/abs/1908.01709v2,econ.GN
291,A Production Function with Variable Elasticity of Factor Substitution,"The main aim of this paper is to prove the existence of a new production
function with variable elasticity of factor substitution. This production
function is a more general form which includes the Cobb-Douglas production
function and the CES production function as particular cases. The econometric
estimates presented in the paper confirm some other results and reinforces the
conclusion that the sigma is well-below the Cobb-Douglas value of one.",Future Internet ,http://arxiv.org/abs/1907.12624v1,q-fin.ST
0,Can Network Theory-based Targeting Increase Technology Adoption?,"In order to induce farmers to adopt a productive new agricultural technology,
we apply simple and complex contagion diffusion models on rich social network
data from 200 villages in Malawi to identify seed farmers to target and train
on the new technology. A randomized controlled trial compares these
theory-driven network targeting approaches to simpler strategies that either
rely on a government extension worker or an easily measurable proxy for the
social network (geographic distance between households) to identify seed
farmers. Our results indicate that technology diffusion is characterized by a
complex contagion learning environment in which most farmers need to learn from
multiple people before they adopt themselves. Network theory based targeting
can out-perform traditional approaches to extension, and we identify methods to
realize these gains at low cost to policymakers.
  Keywords: Social Learning, Agricultural Technology Adoption, Complex
Contagion, Malawi
  JEL Classification Codes: O16, O13",Games,http://arxiv.org/abs/1808.01205v1,econ.GN
275,On the many-to-one strongly stable fractional matching set,"For a many-to-one matching market where firms have strict and
$\boldsymbol{q}$-responsive preferences, we give a characterization of the set
of strongly stable fractional matchings as the union of the convex hull of all
connected sets of stable matchings. Also, we prove that a strongly stable
fractional matching is represented as a convex combination of stable matchings
that are ordered in the common preferences of all firms.",Games,http://arxiv.org/abs/1905.12500v2,econ.EM
387,"Anticipated impacts of Brexit scenarios on UK food prices and
  implications for policies on poverty and health: a structured expert
  judgement approach","Food insecurity is associated with increased risk for several health
conditions and with poor chronic disease management. Key determinants for
household food insecurity are income and food costs. Whereas short-term
household incomes are likely to remain static, increased food prices would be a
significant driver of food insecurity. To investigate food price drivers for
household food security and its health consequences in the UK under scenarios
of Deal and No deal for Brexit . To estimate the 5\% and 95\% quantiles of the
projected price distributions. Structured expert judgement elicitation, a
well-established method for quantifying uncertainty, using experts. In July
2018, each expert estimated the median, 5\% and 95\% quantiles of changes in
price for ten food categories under Brexit Deal and No-deal to June 2020
assuming Brexit had taken place on 29th March 2019. These were aggregated based
on the accuracy and informativeness of the experts on calibration questions.
Ten specialists in food procurement, retail, agriculture, economics, statistics
and household food security. Results: when combined in proportions used to
calculate Consumer Prices Index food basket costs, median food price change for
Brexit with a Deal is expected to be +6.1\% [90\% credible interval:-3\%,
+17\%] and with No deal +22.5\% [+1\%, +52\%]. The number of households
experiencing food insecurity and its severity are likely to increase because of
expected sizeable increases in median food prices after Brexit. Higher
increases are more likely than lower rises and towards the upper limits, these
would entail severe impacts. Research showing a low food budget leads to
increasingly poor diet suggests that demand for health services in both the
short and longer term is likely to increase due to the effects of food
insecurity on the incidence and management of diet-sensitive conditions.",Games ,http://arxiv.org/abs/1904.03053v3,econ.GN
602,Modular structure in labour networks reveals skill basins,"There is an emerging consensus in the literature that locally embedded
capabilities and industrial know-how are key determinants of growth and
diversification processes. In order to model these dynamics as a branching
process, whereby industries grow as a function of the availability of related
or relevant skills, industry networks are typically employed. These networks,
sometimes referred to as industry spaces, describe the complex structure of the
capability or skill overlap between industry pairs, measured here via
inter-industry labour flows. Existing models typically deploy a local or
'nearest neighbour' approach to capture the size of the labour pool available
to an industry in related sectors. This approach, however, ignores higher order
interactions in the network, and the presence of industry clusters or groups of
industries which exhibit high internal skill overlap. We argue that these
clusters represent skill basins in which workers circulate and diffuse
knowledge, and delineate the size of the skilled labour force available to an
industry. By applying a multi-scale community detection algorithm to this
network of flows, we identify industry clusters on a range of scales, from many
small clusters to few large groupings. We construct a new variable, cluster
employment, which captures the workforce available to an industry within its
own cluster. Using UK data we show that this variable is predictive of
industry-city employment growth and, exploiting the multi-scale nature of the
industrial clusters detected, propose a methodology to uncover the optimal
scale at which labour pooling operates.",Games ,http://arxiv.org/abs/1909.03379v3,econ.GN
8,The role of complex analysis in modeling economic growth,"Development and growth are complex and tumultuous processes. Modern economic
growth theories identify some key determinants of economic growth. However, the
relative importance of the determinants remains unknown, and additional
variables may help clarify the directions and dimensions of the interactions.
The novel stream of literature on economic complexity goes beyond aggregate
measures of productive inputs, and considers instead a more granular and
structural view of the productive possibilities of countries, i.e. their
capabilities. Different endowments of capabilities are crucial ingredients in
explaining differences in economic performances. In this paper we employ
economic fitness, a measure of productive capabilities obtained through complex
network techniques. Focusing on the combined roles of fitness and some more
traditional drivers of growth, we build a bridge between economic growth
theories and the economic complexity literature. Our findings, in agreement
with other recent empirical studies, show that fitness plays a crucial role in
fostering economic growth and, when it is included in the analysis, can be
either complementary to traditional drivers of growth or can completely
overshadow them.",Games and Economic Behavior,http://arxiv.org/abs/1808.10428v1,econ.TH
216,Limits to green growth and the dynamics of innovation,"Central to the official ""green growth"" discourse is the conjecture that
absolute decoupling can be achieved with certain market instruments. This paper
evaluates this claim focusing on the role of technology, while changes in GDP
composition are treated elsewhere. Some fundamental difficulties for absolute
decoupling, referring specifically to thermodynamic costs, are identified
through a stylized model based on empirical knowledge on innovation and
learning. Normally, monetary costs decrease more slowly than production grows,
and this is unlikely to change should monetary costs align with thermodynamic
costs, except, potentially, in the transition after the price reform.
Furthermore, thermodynamic efficiency must eventually saturate for physical
reasons. While this model, as usual, introduces technological innovation just
as a source of efficiency, innovation also creates challenges: therefore,
attempts to sustain growth by ever-accelerating innovation collide also with
the limited reaction capacity of people and institutions. Information
technology could disrupt innovation dynamics in the future, permitting quicker
gains in eco-efficiency, but only up to saturation and exacerbating the
downsides of innovation. These observations suggest that long-term
sustainability requires much deeper transformations than the green growth
discourse presumes, exposing the need to rethink scales, tempos and
institutions, in line with ecological economics and the degrowth literature.",Games and Economic Behavior,http://arxiv.org/abs/1904.09586v2,econ.GN
311,"The Impact Of Country Of Origin In Mobile Phone Choice Of Generation Y
  And Z","Mobile phones play a very important role in our life. Mobile phone sales have
been soaring over the last decade due to the growing acceptance of
technological innovations, especially by Generations Y and Z. Understanding the
change in customers' requirement is the key to success in the smartphone
business. New, strong mobile phone models will emerge if the voice of the
customer can be heard. Although it has been widely known that country of origin
has serious impact on the attitudes and purchase decisions of mobile phone
consumers, there lack substantial studies that investigate the mobile phone
preference of young adults aged 18-25, members of late Generation Y and early
Generation Z. In order to investigate the role of country of origin in mobile
phone choice of Generations Y and Z, an online survey with 228 respondents was
conducted in Hungary in 2016. Besides the descriptive statistical methods,
crosstabs, ANOVA and Pearson correlation are used to analyze the collected data
and find out significant relationships. Factor analysis (Principal Component
Analysis) is used for data reduction to create new factor components. The
findings of this exploratory study support the idea that country of origin
plays a significant role in many respects related to young adults' mobile phone
choice. Mobile phone owners with different countries of origin attribute
crucial importance to the various product features including technical
parameters, price, design, brand name, operating system, and memory size.
Country of origin has a moderating effect on the price sensitivity of consumers
with varied net income levels. It is also found that frequent buyers of mobile
phones, especially US brand products, spend the most significant amount of
money for their consumption in this aspect.",Games and Economic Behavior,http://arxiv.org/abs/1901.00793v1,econ.TH
362,Persuasion with Coarse Communication,"We study games of Bayesian persuasion where communication is coarse. This
model captures interactions between a sender and a receiver, where the sender
is unable to fully describe the state or recommend all possible actions. The
sender always weakly benefits from more signals, as it increases their ability
to persuade. However, more signals do not always lead to more information being
sent, and the receiver might prefer outcomes with coarse communication. As a
motivating example, we study advertising where a larger signal space
corresponds to better targeting ability for the advertiser, and show that
customers may prefer less targeting. In a class of games where the sender's
utility is independent from the state, we show that an additional signal is
more valuable to the sender when the receiver is more difficult to persuade.
More generally, we characterize optimal ways to send information using limited
signals, show that the sender's optimization problem can be solved by searching
within a finite set, and prove an upper bound on the marginal value of a
signal. Finally, we show how our approach can be applied to settings with cheap
talk and heterogeneous priors.",Games and Economic Behavior,http://arxiv.org/abs/1910.13547v5,math.OC
412,A Quantitative Analysis of Possible Futures of Autonomous Transport,"Autonomous ships (AS) used for cargo transport have gained a considerable
amount of attention in recent years. They promise benefits such as reduced crew
costs, increased safety and increased flexibility. This paper explores the
effects of a faster increase in technological performance in maritime shipping
achieved by leveraging fast-improving technological domains such as computer
processors, and advanced energy storage. Based on historical improvement rates
of several modes of transport (Cargo Ships, Air, Rail, Trucking) a simplified
Markov-chain Monte-Carlo (MCMC) simulation of an intermodal transport model
(IMTM) is used to explore the effects of differing technological improvement
rates for AS. The results show that the annual improvement rates of traditional
shipping (Ocean Cargo Ships = 2.6%, Air Cargo = 5.5%, Trucking = 0.6%, Rail =
1.9%, Inland Water Transport = 0.4%) improve at lower rates than technologies
associated with automation such as Computer Processors (35.6%), Fuel Cells
(14.7%) and Automotive Autonomous Hardware (27.9%). The IMTM simulations up to
the year 2050 show that the introduction of any mode of autonomous transport
will increase competition in lower cost shipping options, but is unlikely to
significantly alter the overall distribution of transport mode costs. Secondly,
if all forms of transport end up converting to autonomous systems, then the
uncertainty surrounding the improvement rates yields a complex intermodal
transport solution involving several options, all at a much lower cost over
time. Ultimately, the research shows a need for more accurate measurement of
current autonomous transport costs and how they are changing over time.",Games and Economic Behavior,http://arxiv.org/abs/1806.01696v1,econ.TH
85,E-commerce in Hungary: A Market Analysis,"E-commerce is on the rise in Hungary, with significantly growing numbers of
customers shopping online. This paper aims to identify the direct and indirect
drivers of the double-digit growth rate, including the related macroeconomic
indicators and the Digital Economy and Society Index (DESI). Moreover, this
study provides a deep insight into industry trends and outlooks, including high
industry concentration and top industrial players. It also draws the profile of
the typical online shopper and the dominant characteristics of online
purchases. Development of e-commerce is robust, but there is still plenty of
potential for growth and progress in Hungary.","Games and Economic Behavior
",http://arxiv.org/abs/1812.11488v1,econ.TH
49,Hyper-rational choice theory,"The rational choice theory is based on this idea that people rationally
pursue goals for increasing their personal interests. In most conditions, the
behavior of an actor is not independent of the person and others' behavior.
Here, we present a new concept of rational choice as a hyper-rational choice
which in this concept, the actor thinks about profit or loss of other actors in
addition to his personal profit or loss and then will choose an action which is
desirable to him. We implement the hyper-rational choice to generalize and
expand the game theory. Results of this study will help to model the behavior
of people considering environmental conditions, the kind of behavior
interactive, valuation system of itself and others and system of beliefs and
internal values of societies. Hyper-rationality helps us understand how human
decision makers behave in interactive decisions.",Games and Economic Behavior ,http://arxiv.org/abs/1801.10520v2,econ.TH
183,A supreme test for periodic explosive GARCH,"We develop a uniform test for detecting and dating explosive behavior of a
strictly stationary GARCH$(r,s)$ (generalized autoregressive conditional
heteroskedasticity) process. Namely, we test the null hypothesis of a globally
stable GARCH process with constant parameters against an alternative where
there is an 'abnormal' period with changed parameter values. During this
period, the change may lead to an explosive behavior of the volatility process.
It is assumed that both the magnitude and the timing of the breaks are unknown.
We develop a double supreme test for the existence of a break, and then provide
an algorithm to identify the period of change. Our theoretical results hold
under mild moment assumptions on the innovations of the GARCH process.
Technically, the existing properties for the QMLE in the GARCH model need to be
reinvestigated to hold uniformly over all possible periods of change. The key
results involve a uniform weak Bahadur representation for the estimated
parameters, which leads to weak convergence of the test statistic to the
supreme of a Gaussian Process. In simulations we show that the test has good
size and power for reasonably large time series lengths. We apply the test to
Apple asset returns and Bitcoin returns.",Games and Economic Behavior ,http://arxiv.org/abs/1812.03475v1,cs.GT
590,Contracting over persistent information,"We consider a dynamic moral hazard problem between a principal and an agent,
where the sole instrument the principal has to incentivize the agent is the
disclosure of information. The principal aims at maximizing the (discounted)
number of times the agent chooses a particular action, e.g., to work hard. We
show that there exists an optimal contract, where the principal stops
disclosing information as soon as its most preferred action is a static best
reply for the agent or else continues disclosing information until the agent
perfectly learns the principal's private information. If the agent perfectly
learns the state, he learns it in finite time with probability one; the more
patient the agent, the later he learns it.",Geographical Analysis ,http://arxiv.org/abs/2007.05983v3,cs.CY
154,Interpreting Economic Complexity,"Two network measures known as the Economic Complexity Index (ECI) and Product
Complexity Index (PCI) have provided important insights into patterns of
economic development. We show that the ECI and PCI are equivalent to a spectral
clustering algorithm that partitions a similarity graph into two parts. The
measures are also related to various dimensionality reduction methods and can
be interpreted as vectors that determine distances between nodes based on their
similarity. Our results shed a new light on the ECI's empirical success in
explaining cross-country differences in GDP/capita and economic growth, which
is often linked to the diversity of country export baskets. In fact, countries
with high (low) ECI tend to specialize in high (low) PCI products. We also find
that the ECI and PCI uncover economically informative specialization patterns
across US states and UK regions.",Global Journal of Health Science,http://arxiv.org/abs/1711.08245v3,econ.GN
79,"Poverty, Income Inequality and Growth in Bangladesh: Revisited Karl-Marx","This study tries to find the relationship among poverty inequality and
growth. The major finding of this study is poverty has reduced significantly
from 2000 to 2016, which is more than 100 percent but in recent time poverty
reduction has slowed down. Slower and unequal household consumption growth
makes sloth the rate of poverty reduction. Average annual consumption fell from
1.8 percent to 1.4 percent from 2010 to 2016 and poorer households experienced
slower consumption growth compared to richer households.",History of Economic Ideas,http://arxiv.org/abs/1812.09385v1,econ.GN
588,Project selection with partially verifiable information,"We consider a principal agent project selection problem with asymmetric
information. There are $N$ projects and the principal must select exactly one
of them. Each project provides some profit to the principal and some payoff to
the agent and these profits and payoffs are the agent's private information. We
consider the principal's problem of finding an optimal mechanism for two
different objectives: maximizing expected profit and maximizing the probability
of choosing the most profitable project. Importantly, we assume partial
verifiability so that the agent cannot report a project to be more profitable
to the principal than it actually is. Under this no-overselling constraint, we
characterize the set of implementable mechanisms. Using this characterization,
we find that in the case of two projects, the optimal mechanism under both
objectives takes the form of a simple cutoff mechanism. The simple structure of
the optimal mechanism also allows us to find evidence in support of the
well-known ally-principle which says that principal delegates more authority to
an agent who shares their preferences.",Humanities and Social Sciences Communications ,http://arxiv.org/abs/2007.00907v2,cs.DL
563,"Partial Mean Processes with Generated Regressors: Continuous Treatment
  Effects and Nonseparable Models","Partial mean with generated regressors arises in several econometric
problems, such as the distribution of potential outcomes with continuous
treatments and the quantile structural function in a nonseparable triangular
model. This paper proposes a nonparametric estimator for the partial mean
process, where the second step consists of a kernel regression on regressors
that are estimated in the first step. The main contribution is a uniform
expansion that characterizes in detail how the estimation error associated with
the generated regressor affects the limiting distribution of the marginal
integration estimator. The general results are illustrated with two examples:
the generalized propensity score for a continuous treatment (Hirano and Imbens,
2004) and control variables in triangular models (Newey, Powell, and Vella,
1999; Imbens and Newey, 2009). An empirical application to the Job Corps
program evaluation demonstrates the usefulness of the method.","Humanities and Social Sciences Communications 
",http://arxiv.org/abs/1811.00157v1,econ.GN
194,"Dynamic Models with Robust Decision Makers: Identification and
  Estimation","This paper studies identification and estimation of a class of dynamic models
in which the decision maker (DM) is uncertain about the data-generating
process. The DM surrounds a benchmark model that he or she fears is
misspecified by a set of models. Decisions are evaluated under a worst-case
model delivering the lowest utility among all models in this set. The DM's
benchmark model and preference parameters are jointly underidentified. With the
benchmark model held fixed, primitive conditions are established for
identification of the DM's worst-case model and preference parameters. The key
step in the identification analysis is to establish existence and uniqueness of
the DM's continuation value function allowing for unbounded statespace and
unbounded utilities. To do so, fixed-point results are derived for monotone,
convex operators that act on a Banach space of thin-tailed functions arising
naturally from the structure of the continuation value recursion. The
fixed-point results are quite general; applications to models with learning and
Rust-type dynamic discrete choice models are also discussed. For estimation, a
perturbation result is derived which provides a necessary and sufficient
condition for consistent estimation of continuation values and the worst-case
model. The result also allows convergence rates of estimators to be
characterized. An empirical application studies an endowment economy where the
DM's benchmark model may be interpreted as an aggregate of experts' forecasting
models. The application reveals time-variation in the way the DM
pessimistically distorts benchmark probabilities. Consequences for asset
pricing are explored and connections are drawn with the literature on
macroeconomic uncertainty.",Iberoamerican Journal of Science Measurement and Communication,http://arxiv.org/abs/1812.11246v3,econ.GN
163,"Finding a promising venture capital project with todim under
  probabilistic hesitant fuzzy circumstance","Considering the risk aversion for gains and the risk seeking for losses of
venture capitalists, the TODIM has been chosen as the decision-making method.
Moreover, group decision is an available way to avoid the limited ability and
knowledge etc. of venture capitalists.Simultaneously, venture capitalists may
be hesitant among several assessed values with different probabilities to
express their real perceptionbecause of the uncertain decision-making
environment. However, the probabilistic hesitant fuzzy information can solve
such problems effectively. Therefore, the TODIM has been extended to
probabilistic hesitant fuzzy circumstance for the sake of settling the
decision-making problem of venture capitalists in this paper. Moreover, due to
the uncertain investment environment, the criteria weights are considered as
probabilistic hesitant fuzzyinformation as well. Then, a case study has been
used to verify the feasibility and validity of the proposed TODIM.Also, the
TODIM with hesitant fuzzy information has been carried out to analysis the same
case.From the comparative analysis, the superiority of the proposed TODIM in
this paper has already appeared.",Iberoamerican Journal of Science Measurement and Communication,http://arxiv.org/abs/1809.00128v1,econ.EM
209,"Theories and Practice of Agent based Modeling: Some practical
  Implications for Economic Planners","Nowadays, we are surrounded by a large number of complex phenomena ranging
from rumor spreading, social norms formation to rise of new economic trends and
disruption of traditional businesses. To deal with such phenomena,Complex
Adaptive System (CAS) framework has been found very influential among social
scientists,especially economists. As the most powerful methodology of CAS
modeling, Agent-based modeling (ABM) has gained a growing application among
academicians and practitioners. ABMs show how simple behavioral rules of agents
and local interactions among them at micro-scale can generate surprisingly
complex patterns at macro-scale. Despite a growing number of ABM publications,
those researchers unfamiliar with this methodology have to study a number of
works to understand (1) the why and what of ABMs and (2) the ways they are
rigorously developed. Therefore, the major focus of this paper is to help
social sciences researchers,especially economists get a big picture of ABMs and
know how to develop them both systematically and rigorously.",Ideas in Ecology and Evolution ,http://arxiv.org/abs/1901.08932v1,econ.GN
287,Contract Design with Costly Convex Self-Control,"In this note, we consider the pricing problem of a profit-maximizing
monopolist who faces naive consumers with convex self-control preferences.",IEEE Access,http://arxiv.org/abs/1907.07628v1,econ.EM
508,"Communication, Renegotiation and Coordination with Private Values
  (Extended Version)","An equilibrium is communication-proof if it is unaffected by new
opportunities to communicate and renegotiate. We characterize the set of
equilibria of coordination games with pre-play communication in which players
have private preferences over the feasible coordinated outcomes.
Communication-proof equilibria provide a narrow selection from the large set of
qualitatively diverse Bayesian Nash equilibria in such games. Under a
communication-proof equilibrium, players never miscoordinate, play their
jointly preferred outcome whenever there is one, and communicate only the
ordinal part of their preferences. Moreover, such equilibria are robust to
changes in players' beliefs, interim Pareto efficient, and evolutionarily
stable.",IEEE Access,http://arxiv.org/abs/2005.05713v3,stat.AP
507,Choice with Endogenous Categorization,"We propose and axiomatize the categorical thinking model (CTM) in which the
framing of the decision problem affects how agents categorize alternatives,
that in turn affects their evaluation of it. Prominent models of salience,
status quo bias, loss-aversion, inequality aversion, and present bias all fit
under the umbrella of CTM. This suggests categorization is an underlying
mechanism of key departures from the neoclassical model of choice. We
specialize CTM to provide a behavioral foundation for the salient thinking
model of Bordalo et al. (2013) that highlights its strong predictions and
distinctions from other models.",IEEE Access ,http://arxiv.org/abs/2005.05196v3,physics.soc-ph
582,Revealing Choice Bracketing,"In a decision problem comprised of multiple choices, a person may fail to
take into account the interdependencies between her choices. To understand how
people make decisions in such problems we design a novel experiment and
revealed preference tests that determine how each subject brackets her choices.
In separate portfolio allocation under risk, social allocation, and
induced-utility shopping experiments, we find that 40-43\% of our subjects are
consistent with narrow bracketing while only 0-15\% are consistent with broad
bracketing. Classifying subjects while adjusting for models' predictive
precision, 73\% of subjects are best described by narrow bracketing, 14\% by
broad bracketing, and 5\% by intermediate cases.",IEEE Access ,http://arxiv.org/abs/2006.14869v2,stat.ME
611,"Legal Architecture and Design for Gulf Cooperation Council Economic
  Integration","The Cooperation Council for the Arab States of the Gulf (GCC) is generally
regarded as a success story for economic integration in Arab countries. The
idea of regional integration gained ground by signing the GCC Charter. It
envisioned a closer economic relationship between member states.Although
economic integration among GCC member states is an ambitious step in the right
direction, there are gaps and challenges ahead. The best way to address the
gaps and challenges that exist in formulating integration processes in the GCC
is to start with a clear set of rules and put the necessary mechanisms in
place. Integration attempts must also exhibit a high level of commitment in
order to deflect dynamics of disintegration that have all too often frustrated
meaningful integration in Arab countries. If the GCC can address these issues,
it could become an economic powerhouse within Arab countries and even Asia.",IEEE Access ,http://arxiv.org/abs/1909.08798v1,cs.LG
328,"The Effect of Partisanship and Political Advertising on Close Family
  Ties","Research on growing American political polarization and antipathy primarily
studies public institutions and political processes, ignoring private effects
including strained family ties. Using anonymized smartphone-location data and
precinct-level voting, we show that Thanksgiving dinners attended by
opposing-party precinct residents were 30-50 minutes shorter than same-party
dinners. This decline from a mean of 257 minutes survives extensive spatial and
demographic controls. Dinner reductions in 2016 tripled for travelers from
media markets with heavy political advertising --- an effect not observed in
2015 --- implying a relationship to election-related behavior. Effects appear
asymmetric: while fewer Democratic-precinct residents traveled in 2016 than
2015, political differences shortened Thanksgiving dinners more among
Republican-precinct residents. Nationwide, 34 million person-hours of
cross-partisan Thanksgiving discourse were lost in 2016 to partisan effects.",IEEE Transactions on Knowledge and Data Engineering ,http://arxiv.org/abs/1711.10602v2,econ.TH
551,On LASSO for Predictive Regression,"Explanatory variables in a predictive regression typically exhibit low signal
strength and various degrees of persistence. Variable selection in such a
context is of great importance. In this paper, we explore the pitfalls and
possibilities of the LASSO methods in this predictive regression framework. In
the presence of stationary, local unit root, and cointegrated predictors, we
show that the adaptive LASSO cannot asymptotically eliminate all cointegrating
variables with zero regression coefficients. This new finding motivates a novel
post-selection adaptive LASSO, which we call the twin adaptive LASSO (TAlasso),
to restore variable selection consistency. Accommodating the system of
heterogeneous regressors, TAlasso achieves the well-known oracle property. In
contrast, conventional LASSO fails to attain coefficient estimation consistency
and variable screening in all components simultaneously. We apply these LASSO
methods to evaluate the short- and long-horizon predictability of S\&P 500
excess returns.",Information ,http://arxiv.org/abs/1810.03140v4,econ.GN
560,Factor-Driven Two-Regime Regression,"We propose a novel two-regime regression model where regime switching is
driven by a vector of possibly unobservable factors. When the factors are
latent, we estimate them by the principal component analysis of a panel data
set. We show that the optimization problem can be reformulated as mixed integer
optimization, and we present two alternative computational algorithms. We
derive the asymptotic distribution of the resulting estimator under the scheme
that the threshold effect shrinks to zero. In particular, we establish a phase
transition that describes the effect of first-stage factor estimation as the
cross-sectional dimension of panel data increases relative to the time-series
dimension. Moreover, we develop bootstrap inference and illustrate our methods
via numerical studies.",Information & Management,http://arxiv.org/abs/1810.11109v4,cs.CY
535,Ordinal Tax To Sustain a Digital Economy,"Recently, the French Senate approved a law that imposes a 3% tax on revenue
generated from digital services by companies above a certain size. While there
is a lot of political debate about economic consequences of this action, it is
actually interesting to reverse the question: We consider the long-term
implications of an economy with no such digital tax. More generally, we can
think of digital services as a special case of products with low or zero cost
of transportation. With basic economic models we show that a market with no
transportation costs is prone to monopolization as minuscule, random
differences in quality are rewarded disproportionally. We then propose a
distance-based tax to counter-balance the tendencies of random centralisation.
Unlike a tax that scales with physical (cardinal) distance, a ranked (ordinal)
distance tax leverages the benefits of digitalization while maintaining a
stable economy.",Innovation in Information Infrastructures ,http://arxiv.org/abs/1908.03287v1,econ.GN
474,"The cyclicality of loan loss provisions under three different accounting
  models: the United Kingdom, Spain, and Brazil","A controversy involving loan loss provisions in banks concerns their
relationship with the business cycle. While international accounting standards
for recognizing provisions (incurred loss model) would presumably be
pro-cyclical, accentuating the effects of the current economic cycle, an
alternative model, the expected loss model, has countercyclical
characteristics, acting as a buffer against economic imbalances caused by
expansionary or contractionary phases in the economy. In Brazil, a mixed
accounting model exists, whose behavior is not known to be pro-cyclical or
countercyclical. The aim of this research is to analyze the behavior of these
accounting models in relation to the business cycle, using an econometric model
consisting of financial and macroeconomic variables. The study allowed us to
identify the impact of credit risk behavior, earnings management, capital
management, Gross Domestic Product (GDP) behavior, and the behavior of the
unemployment rate on provisions in countries that use different accounting
models. Data from commercial banks in the United Kingdom (incurred loss), in
Spain (expected loss), and in Brazil (mixed model) were used, covering the
period from 2001 to 2012. Despite the accounting models of the three countries
being formed by very different rules regarding possible effects on the business
cycles, the results revealed a pro-cyclical behavior of provisions in each
country, indicating that when GDP grows, provisions tend to fall and vice
versa. The results also revealed other factors influencing the behavior of loan
loss provisions, such as earning management.",Inquiry,http://arxiv.org/abs/1907.07491v1,econ.GN
400,Transaction Costs in Collective Waste Recovery Systems in the EU,"The study aims to identify the institutional flaws of the current EU waste
management model by analysing the economic model of extended producer
responsibility and collective waste management systems and to create a model
for measuring the transaction costs borne by waste recovery organizations. The
model was approbated by analysing the Bulgarian collective waste management
systems that have been complying with the EU legislation for the last 10 years.
The analysis focuses on waste oils because of their economic importance and the
limited number of studies and analyses in this field as the predominant body of
research to date has mainly addressed packaging waste, mixed household waste or
discarded electrical and electronic equipment. The study aims to support the
process of establishing a circular economy in the EU, which was initiated in
2015.",International Economic Review ,http://arxiv.org/abs/1804.06792v1,econ.EM
485,Analysis of a Dynamic Voluntary Contribution Mechanism Public Good Game,"I present a dynamic, voluntary contribution mechanism, public good game and
derive its potential outcomes. In each period, players endogenously determine
contribution productivity by engaging in costly investment. The level of
contribution productivity carries from period to period, creating a dynamic
link between periods. The investment mimics investing in the stock of
technology for producing public goods such as national defense or a clean
environment. After investing, players decide how much of their remaining money
to contribute to provision of the public good, as in traditional public good
games. I analyze three kinds of outcomes of the game: the lowest payoff
outcome, the Nash Equilibria, and socially optimal behavior. In the lowest
payoff outcome, all players receive payoffs of zero. Nash Equilibrium occurs
when players invest any amount and contribute all or nothing depending on the
contribution productivity. Therefore, there are infinitely many Nash Equilibria
strategies. Finally, the socially optimal result occurs when players invest
everything in early periods, then at some point switch to contributing
everything. My goal is to discover and explain this point. I use mathematical
analysis and computer simulation to derive the results.",International Economic Review ,http://arxiv.org/abs/1807.04621v1,econ.EM
262,Sharp bounds and testability of a Roy model of STEM major choices,"We analyze the empirical content of the Roy model, stripped down to its
essential features, namely sector specific unobserved heterogeneity and
self-selection on the basis of potential outcomes. We characterize sharp bounds
on the joint distribution of potential outcomes and testable implications of
the Roy self-selection model under an instrumental constraint on the joint
distribution of potential outcomes we call stochastically monotone instrumental
variable (SMIV). We show that testing the Roy model selection is equivalent to
testing stochastic monotonicity of observed outcomes relative to the
instrument. We apply our sharp bounds to the derivation of a measure of
departure from Roy self-selection to identify values of observable
characteristics that induce the most costly misallocation of talent and sector
and are therefore prime targets for intervention. Special emphasis is put on
the case of binary outcomes, which has received little attention in the
literature to date. For richer sets of outcomes, we emphasize the distinction
between pointwise sharp bounds and functional sharp bounds, and its importance,
when constructing sharp bounds on functional features, such as inequality
measures. We analyze a Roy model of college major choice in Canada and Germany
within this framework, and we take a new look at the under-representation of
women in~STEM.",International Economic Studies,http://arxiv.org/abs/1709.09284v2,econ.EM
271,"Identifying the Effects of a Program Offer with an Application to Head
  Start","I develop tools to learn about the average effects of providing an offer to
participate in a program given data from an experiment that randomizes offers.
I allow for the complication that individuals may not comply with their
assigned status in the sense that those who are not provided an offer may
receive one from outside the experiment, and that the data may provide only
partial information on the receipt of an offer across individuals. To do so, I
propose a new nonparametric selection model with unobserved choice sets that
provides a conceptual framework to define a range of parameters evaluating the
effects of an offer, exploit the partial information available on offer
receipt, and also consider an array of identifying assumptions. I illustrate
how a computational procedure can be used to sharply learn about the parameters
under the various assumptions. Using these tools, I analyze the effects of a
policy that provides an offer to participate in the Head Start preschool
program given data from the Head Start Impact Study. I find that such a policy
affects a large number of children who take up the offer, and that they
subsequently have positive effects on test scores. These effects primarily
arise from children who do not have any preschool as an outside option.
Performing a cost-benefit analysis, I find that the earning benefits associated
with the test score gains can outweigh the net costs associated with the take
up of the offer.","International Journal of Academic Research in Business and Social
Sciences",http://arxiv.org/abs/1711.02048v5,econ.GN
576,"Trust and Betrayals: Reputational Payoffs and Behaviors without
  Commitment","I study a repeated game in which a patient player (e.g., a seller) wants to
win the trust of some myopic opponents (e.g., buyers) but can strictly benefit
from betraying them. Her benefit from betrayal is strictly positive and is her
persistent private information. I characterize every type of patient player's
highest equilibrium payoff. Her persistent private information affects this
payoff only through the lowest benefit in the support of her opponents' prior
belief. I also show that in every equilibrium which is optimal for the patient
player, her on-path behavior is nonstationary, and her long-run action
frequencies are pinned down for all except two types. Conceptually, my
payoff-type approach incorporates a realistic concern that no type of
reputation-building player is immune to reneging temptations. Compared to
commitment-type models, the incentive constraints for all types of patient
player lead to a sharp characterization of her highest attainable payoff and
novel predictions on her behaviors.",International Journal of Approximate Reasoning ,http://arxiv.org/abs/2006.08071v1,cs.LG
521,Delegation in Veto Bargaining,"A proposer requires the approval of a veto player to change a status quo.
Preferences are single peaked. Proposer is uncertain about Vetoer's ideal
point. We study Proposer's optimal mechanism without transfers. Vetoer is given
a menu, or a delegation set, to choose from. The optimal delegation set
balances the extent of Proposer's compromise with the risk of a veto. Under
reasonable conditions, ""full delegation"" is optimal: Vetoer can choose any
action between the status quo and Proposer's ideal action. This outcome largely
nullifies Proposer's bargaining power; Vetoer frequently obtains her ideal
point, and there is Pareto efficiency despite asymmetric information. More
generally, we identify when ""interval delegation"" is optimal. Optimal interval
delegation can be a Pareto improvement over cheap talk. We derive comparative
statics. Vetoer receives less discretion when preferences are more likely to be
aligned, by contrast to expertise-based delegation. Methodologically, our
analysis handles stochastic mechanisms.",International Journal of Business,http://arxiv.org/abs/2006.06773v3,econ.GN
125,Player-Compatible Learning and Player-Compatible Equilibrium,"Player-Compatible Equilibrium (PCE) imposes cross-player restrictions on the
magnitudes of the players' ""trembles"" onto different strategies. These
restrictions capture the idea that trembles correspond to deliberate
experiments by agents who are unsure of the prevailing distribution of play.
PCE selects intuitive equilibria in a number of examples where trembling-hand
perfect equilibrium (Selten, 1975) and proper equilibrium (Myerson, 1978) have
no bite. We show that rational learning and weighted fictitious play imply our
compatibility restrictions in a steady-state setting.","International Journal of Business & Applied Sciences
 ",http://arxiv.org/abs/1712.08954v8,econ.GN
290,"Closed form solutions of Lucas Uzawa model with externalities via
  partial Hamiltonian approach. Some Clarifications","The main aim of this paper is to give some clarifications to the recent paper
published in Computational and Applied Mathematics by Naz and Chaudhry.",International Journal of Communication and Media Science ,http://arxiv.org/abs/1907.12623v1,econ.GN
235,"Deriving the factor endowment--commodity output relationship for
  Thailand (1920-1927) using a three-factor two-good general equilibrium trade
  model","Feeny (1982, pp. 26-28) referred to a three-factor two-good general
equilibrium trade model, when he explained the relative importance of trade and
factor endowments in Thailand 1880-1940. For example, Feeny (1982) stated that
the growth in labor stock would be responsible for a substantial increase in
rice output relative to textile output. Is Feeny's statement plausible? The
purpose of this paper is to derive the Rybczynski sign patterns, which express
the factor endowment--commodity output relationship, for Thailand during the
period 1920 to 1927 using the EWS (economy-wide substitution)-ratio vector. A
'strong Rybczynski result' necessarily holds. I derived three Rybczynski sign
patterns. However, a more detailed estimate allowed a reduction from three
candidates to two. I restrict the analysis to the period 1920-1927 because of
data availability. The results imply that Feeny's statement might not
necessarily hold. Hence, labor stock might not affect the share of exportable
sector in national income positively. Moreover, the percentage of Chinese
immigration in the total population growth was not as large as expected. This
study will be useful when simulating real wage in Thailand.",International Journal of Coronaviruses ,http://arxiv.org/abs/1810.04819v1,econ.GN
273,"Measuring Price Discovery between Nearby and Deferred Contracts in
  Storable and Non-Storable Commodity Futures Markets","Futures market contracts with varying maturities are traded concurrently and
the speed at which they process information is of value in understanding the
pricing discovery process. Using price discovery measures, including Putnins
(2013) information leadership share and intraday data, we quantify the
proportional contribution of price discovery between nearby and deferred
contracts in the corn and live cattle futures markets. Price discovery is more
systematic in the corn than in the live cattle market. On average, nearby
contracts lead all deferred contracts in price discovery in the corn market,
but have a relatively less dominant role in the live cattle market. In both
markets, the nearby contract loses dominance when its relative volume share
dips below 50%, which occurs about 2-3 weeks before expiration in corn and 5-6
weeks before expiration in live cattle. Regression results indicate that the
share of price discovery is most closely linked to trading volume but is also
affected, to far less degree, by time to expiration, backwardation, USDA
announcements and market crashes. The effects of these other factors vary
between the markets which likely reflect the difference in storability as well
as other market-related characteristics.","International Journal of Economics and Finance
",http://arxiv.org/abs/1711.03506v1,econ.GN
240,Religion and Terrorism: Evidence from Ramadan Fasting,"We study the effect of religion and intense religious experiences on
terrorism by focusing on one of the five pillars of Islam: Ramadan fasting. For
identification, we exploit two facts: First, daily fasting from dawn to sunset
during Ramadan is considered mandatory for most Muslims. Second, the Islamic
calendar is not synchronized with the solar cycle. We find a robust negative
effect of more intense Ramadan fasting on terrorist events within districts and
country-years in predominantly Muslim countries. This effect seems to operate
partly through decreases in public support for terrorism and the operational
capabilities of terrorist groups.","International Journal of Economics and Finance Studies 
",http://arxiv.org/abs/1810.09869v3,econ.GN
151,Réintégration des refusés en Credit Scoring,"The granting process of all credit institutions rejects applicants who seem
risky regarding the repayment of their debt. A credit score is calculated and
associated with a cut-off value beneath which an applicant is rejected.
Developing a new score implies having a learning dataset in which the response
variable good/bad borrower is known, so that rejects are de facto excluded from
the learning process. We first introduce the context and some useful notations.
Then we formalize if this particular sampling has consequences on the score's
relevance. Finally, we elaborate on methods that use not-financed clients'
characteristics and conclude that none of these methods are satisfactory in
practice using data from Cr\'edit Agricole Consumer Finance.
  -----
  Un syst\`eme d'octroi de cr\'edit peut refuser des demandes de pr\^et
jug\'ees trop risqu\'ees. Au sein de ce syst\`eme, le score de cr\'edit fournit
une valeur mesurant un risque de d\'efaut, valeur qui est compar\'ee \`a un
seuil d'acceptabilit\'e. Ce score est construit exclusivement sur des donn\'ees
de clients financ\'es, contenant en particulier l'information `bon ou mauvais
payeur', alors qu'il est par la suite appliqu\'e \`a l'ensemble des demandes.
Un tel score est-il statistiquement pertinent ? Dans cette note, nous
pr\'ecisons et formalisons cette question et \'etudions l'effet de l'absence
des non-financ\'es sur les scores \'elabor\'es. Nous pr\'esentons ensuite des
m\'ethodes pour r\'eint\'egrer les non-financ\'es et concluons sur leur
inefficacit\'e en pratique, \`a partir de donn\'ees issues de Cr\'edit Agricole
Consumer Finance.",International Journal of Environmental Research and Public Health,http://arxiv.org/abs/1903.10855v1,econ.GN
600,Buy-Online-and-Pick-up-in-Store in Omnichannel Retailing,"In this paper, we extend the model of Gao and Su (2016) and consider an
omnichannel strategy in which inventory can be replenished when a retailer
sells only in physical stores. With ""buy-online-and-pick-up-in-store"" (BOPS)
having been introduced, consumers can choose to buy directly online, buy from a
retailer using BOPS, or go directly to a store to make purchases without using
BOPS. The retailer is able to select the inventory level to maximize the
probability of inventory availability at the store. Furthermore, the retailer
can incur an additional cost to reduce the BOPS ordering lead time, which
results in a lowered hassle cost for consumers who use BOPS. In conclusion, we
found that there are two types of equilibrium: that in which all consumers go
directly to the store without using BOPS and that in which all consumers use
BOPS.",International Journal of Financial Studies ,http://arxiv.org/abs/1909.00822v2,econ.EM
607,"The Optimal Deterrence of Crime: A Focus on the Time Preference of DWI
  Offenders","We develop a general model for finding the optimal penal strategy based on
the behavioral traits of the offenders. We focus on how the discount rate
(level of time discounting) affects criminal propensity on the individual
level, and how the aggregation of these effects influences criminal activities
on the population level. The effects are aggregated based on the distribution
of discount rate among the population. We study this distribution empirically
through a survey with 207 participants, and we show that it follows
zero-inflated exponential distribution. We quantify the effectiveness of the
penal strategy as its net utility for the population, and show how this
quantity can be maximized. When we apply the maximization procedure on the
offense of impaired driving (DWI), we discover that the effectiveness of DWI
deterrence depends critically on the amount of fine and prison condition.",International Journal of Financial Studies ,http://arxiv.org/abs/1909.06509v2,econ.EM
327,Identification of and correction for publication bias,"Some empirical results are more likely to be published than others. Such
selective publication leads to biased estimates and distorted inference. This
paper proposes two approaches for identifying the conditional probability of
publication as a function of a study's results, the first based on systematic
replication studies and the second based on meta-studies. For known conditional
publication probabilities, we propose median-unbiased estimators and associated
confidence sets that correct for selective publication. We apply our methods to
recent large-scale replication studies in experimental economics and
psychology, and to meta-studies of the effects of minimum wages and de-worming
programs.",International Journal of Forecasting ,http://arxiv.org/abs/1711.10527v1,stat.ME
414,"The Role of Agricultural Sector Productivity in Economic Growth: The
  Case of Iran's Economic Development Plan","This study provides the theoretical framework and empirical model for
productivity growth evaluations in agricultural sector as one of the most
important sectors in Iran's economic development plan. We use the Solow
residual model to measure the productivity growth share in the value-added
growth of the agricultural sector. Our time series data includes value-added
per worker, employment, and capital in this sector. The results show that the
average total factor productivity growth rate in the agricultural sector is
-0.72% during 1991-2010. Also, during this period, the share of total factor
productivity growth in the value-added growth is -19.6%, while it has been
forecasted to be 33.8% in the fourth development plan. Considering the
effective role of capital in the agricultural low productivity, we suggest
applying productivity management plans (especially in regards of capital
productivity) to achieve future growth goals.",International Journal of General Systems ,http://arxiv.org/abs/1806.04235v1,econ.GN
494,Bootstrap Methods in Econometrics,"The bootstrap is a method for estimating the distribution of an estimator or
test statistic by re-sampling the data or a model estimated from the data.
Under conditions that hold in a wide variety of econometric applications, the
bootstrap provides approximations to distributions of statistics, coverage
probabilities of confidence intervals, and rejection probabilities of
hypothesis tests that are more accurate than the approximations of first-order
asymptotic distribution theory. The reductions in the differences between true
and nominal coverage or rejection probabilities can be very large. In addition,
the bootstrap provides a way to carry out inference in certain settings where
obtaining analytic distributional approximations is difficult or impossible.
This article explains the usefulness and limitations of the bootstrap in
contexts of interest in econometrics. The presentation is informal and
expository. It provides an intuitive understanding of how the bootstrap works.
Mathematical details are available in references that are cited.",International Journal of Management Research and Economics,http://arxiv.org/abs/1809.04016v1,econ.GN
37,"Propensity score matching for multiple treatment levels: A CODA-based
  contribution","This study proposes a simple technique for propensity score matching for
multiple treatment levels under the strong unconfoundedness assumption with the
help of the Aitchison distance proposed in the field of compositional data
analysis (CODA).",International Journal of Management Research and Economics ,http://arxiv.org/abs/1710.08558v1,econ.TH
357,Reversals of signal-posterior monotonicity imply a bias of screening,"This note strengthens the main result of Lagziel and Lehrer (2019) (LL) ""A
bias in screening"" using Chambers Healy (2011) (CH) ""Reversals of
signal-posterior monotonicity for any bounded prior"". LL show that the
conditional expectation of an unobserved variable of interest, given that a
noisy signal of it exceeds a cutoff, may decrease in the cutoff. CH prove that
the distribution of a variable conditional on a lower signal may first order
stochastically dominate the distribution conditional on a higher signal.
  The nonmonotonicity result is also extended to the empirically relevant
exponential and Pareto distributions, and to a wide range of signals.",International Journal of Operational Research,http://arxiv.org/abs/1910.03117v6,econ.GN
552,"Simple Inference on Functionals of Set-Identified Parameters Defined by
  Linear Moments","This paper considers uniformly valid inference for linear functionals and
scalar subvectors of partially identified parameters defined by linear moment
inequalities. Our proposed procedure amounts to bootstrapping the value
functions of four carefully constructed ""perturbed"" linear programming
problems, and does not require the researcher to grid over the parameter space.
Our low-level conditions for uniform validity rely on an application of Sard's
Theorem from differential topology. Our procedure is asymptotically
conservative for the true partially identified parameter, but is valid under
weak assumptions, performs well in finite samples, and is computationally
simple to implement.",International Journal of Operational Research,http://arxiv.org/abs/1810.03180v9,econ.GN
484,"Heterogeneous Effects of Unconventional Monetary Policy on Loan Demand
  and Supply. Insights from the Bank Lending Survey","This paper analyzes the bank lending channel and the heterogeneous effects on
the euro area, providing evidence that the channel is indeed working. The
analysis of the transmission mechanism is based on structural impulse responses
to an unconventional monetary policy shock on bank loans. The Bank Lending
Survey (BLS) is exploited in order to get insights on developments of loan
demand and supply. The contribution of this paper is to use country-specific
data to analyze the consequences of unconventional monetary policy, instead of
taking an aggregate stance by using euro area data. This approach provides a
deeper understanding of the bank lending channel and its effects. That is, an
expansionary monetary policy shock leads to an increase in loan demand, supply
and output growth. A small north-south disparity between the countries can be
observed.",International Journal of Social Science Studies,http://arxiv.org/abs/1807.04161v1,econ.GN
342,"On the solution of the variational optimisation in the rational
  inattention framework","I analyse the solution method for the variational optimisation problem in the
rational inattention framework proposed by Christopher A. Sims. The solution,
in general, does not exist, although it may exist in exceptional cases. I show
that the solution does not exist for the quadratic and the logarithmic
objective functions analysed by Sims (2003, 2006). For a linear-quadratic
objective function a solution can be constructed under restrictions on all but
one of its parameters. This approach is, therefore, unlikely to be applicable
to a wider set of economic models.",International Journal of Standardization Research,http://arxiv.org/abs/1802.09869v2,econ.GN
292,On the Solutions of the Lucas-Uzawa Model,"In a recent paper, Naz and Chaudry provided two solutions for the model of
Lucas-Uzawa, via the Partial Hamiltonian Approach. The first one of these
solutions coincides exactly with that determined by Chilarescu. For the second
one, they claim that this is a new solution, fundamentally different than that
obtained by Chilarescu. We will prove in this paper, using the existence and
uniqueness theorem of nonlinear differential equations, that this is not at all
true.",International Journal of Statistics and Economics,http://arxiv.org/abs/1907.12658v1,stat.AP
337,"Knowledge and Unanimous Acceptance of Core Payoffs: An Epistemic
  Foundation for Cooperative Game Theory","We provide an epistemic foundation for cooperative games by proof theory via
studying the knowledge for players unanimously accepting only core payoffs. We
first transform each cooperative game into a decision problem where a player
can accept or reject any payoff vector offered to her based on her knowledge
about available cooperation. Then we use a modified KD-system in epistemic
logic, which can be regarded as a counterpart of the model for non-cooperative
games in Bonanno (2008), (2015), to describe a player's knowledge,
decision-making criterion, and reasoning process; especially, a formula called
C-acceptability is defined to capture the criterion for accepting a core payoff
vector. Within this syntactical framework, we characterize the core of a
cooperative game in terms of players' knowledge. Based on that result, we
discuss an epistemic inconsistency behind Debreu-Scarf Theorem, that is, the
increase of the number of replicas has invariant requirement on each
participant's knowledge from the aspect of competitive market, while requires
unbounded epistemic ability players from the aspect of cooperative game.",International Journal of Sustainable Transportation ,http://arxiv.org/abs/1802.04595v4,cs.CY
213,The preference lattice,"Most comparisons of preferences are instances of single-crossing dominance.
We examine the lattice structure of single-crossing dominance, proving
characterisation, existence and uniqueness results for minimum upper bounds of
arbitrary sets of preferences. We apply these theorems to derive comparative
statics for collective choice, to characterise a 'generalised maxmin' class of
ambiguity-averse preferences over Savage acts, and to revisit the tension
between liberalism and Pareto-efficiency in social choice (Sen, 1970).","International Journal of Theoretical and Applied Finance 
",http://arxiv.org/abs/1902.07260v3,econ.GN
472,"A global economic policy uncertainty index from principal component
  analysis","This paper constructs a global economic policy uncertainty index through the
principal component analysis of the economic policy uncertainty indices for
twenty primary economies around the world. We find that the PCA-based global
economic policy uncertainty index is a good proxy for the economic policy
uncertainty on a global scale, which is quite consistent with the GDP-weighted
global economic policy uncertainty index. The PCA-based economic policy
uncertainty index is found to be positively related with the volatility and
correlation of the global financial market, which indicates that the stocks are
more volatile and correlated when the global economic policy uncertainty is
higher. The PCA-based global economic policy uncertainty index performs
slightly better because the relationship between the PCA-based uncertainty and
market volatility and correlation is more significant.",International Journal of Transportation Science & Technology,http://arxiv.org/abs/1907.05049v2,econ.GN
139,Completeness and Transitivity of Preferences on Mixture Sets,"In this paper, we show that the presence of the Archimedean and the
mixture-continuity properties of a binary relation, both empirically
non-falsifiable in principle, foreclose the possibility of consistency
(transitivity) without decisiveness (completeness), or decisiveness without
consistency, or in the presence of a weak consistency condition, neither. The
basic result can be sharpened when specialized from the context of a
generalized mixture set to that of a mixture set in the sense of
Herstein-Milnor (1953). We relate the results to the antecedent literature, and
view them as part of an investigation into the interplay of the structure of
the choice space and the behavioral assumptions on the binary relation defined
on it; the ES research program due to Eilenberg (1941) and Sonnenschein (1965),
and one to which Schmeidler (1971) is an especially influential contribution.",International Journal of Workplace Health Management ,http://arxiv.org/abs/1810.02454v1,econ.GN
70,J. S. Mill's Liberal Principle and Unanimity,"The broad concept of an individual's welfare is actually a cluster of related
specific concepts that bear a ""family resemblance"" to one another. One might
care about how a policy will affect people both in terms of their subjective
preferences and also in terms of some notion of their objective interests. This
paper provides a framework for evaluation of policies in terms of welfare
criteria that combine these two considerations. Sufficient conditions are
provided for such a criterion to imply the same ranking of social states as
does Pareto's unanimity criterion. Sufficiency is proved via study of a
community of agents with interdependent ordinal preferences.",Issues in Political Economy,http://arxiv.org/abs/1903.07769v1,econ.EM
336,A General Method for Demand Inversion,"This paper describes a numerical method to solve for mean product qualities
which equates the real market share to the market share predicted by a discrete
choice model. The method covers a general class of discrete choice model,
including the pure characteristics model in Berry and Pakes(2007) and the
random coefficient logit model in Berry et al.(1995) (hereafter BLP). The
method transforms the original market share inversion problem to an
unconstrained convex minimization problem, so that any convex programming
algorithm can be used to solve the inversion. Moreover, such results also imply
that the computational complexity of inverting a demand model should be no more
than that of a convex programming problem. In simulation examples, I show the
method outperforms the contraction mapping algorithm in BLP. I also find the
method remains robust in pure characteristics models with near-zero market
shares.",Italian Economic Journal ,http://arxiv.org/abs/1802.04444v3,stat.AP
40,Implications of macroeconomic volatility in the Euro area,"In this paper we estimate a Bayesian vector autoregressive model with factor
stochastic volatility in the error term to assess the effects of an uncertainty
shock in the Euro area. This allows us to treat macroeconomic uncertainty as a
latent quantity during estimation. Only a limited number of contributions to
the literature estimate uncertainty and its macroeconomic consequences jointly,
and most are based on single country models. We analyze the special case of a
shock restricted to the Euro area, where member states are highly related by
construction. We find significant results of a decrease in real activity for
all countries over a period of roughly a year following an uncertainty shock.
Moreover, equity prices, short-term interest rates and exports tend to decline,
while unemployment levels increase. Dynamic responses across countries differ
slightly in magnitude and duration, with Ireland, Slovakia and Greece
exhibiting different reactions for some macroeconomic fundamentals.",Journal of Agricultural Economics ,http://arxiv.org/abs/1801.02925v2,econ.GN
274,"Economic Complexity Unfolded: Interpretable Model for the Productive
  Structure of Economies","Economic complexity reflects the amount of knowledge that is embedded in the
productive structure of an economy. It resides on the premise of hidden
capabilities - fundamental endowments underlying the productive structure. In
general, measuring the capabilities behind economic complexity directly is
difficult, and indirect measures have been suggested which exploit the fact
that the presence of the capabilities is expressed in a country's mix of
products. We complement these studies by introducing a probabilistic framework
which leverages Bayesian non-parametric techniques to extract the dominant
features behind the comparative advantage in exported products. Based on
economic evidence and trade data, we place a restricted Indian Buffet Process
on the distribution of countries' capability endowment, appealing to a culinary
metaphor to model the process of capability acquisition. The approach comes
with a unique level of interpretability, as it produces a concise and
economically plausible description of the instantiated capabilities.",Journal of Applied Econometrics ,http://arxiv.org/abs/1711.07327v2,q-fin.TR
428,A Bilateral River Bargaining Problem with Negative Externality,"This article is addressing the problem of river sharing between two agents
along a river in the presence of negative externalities. Where, each agent
claims river water based on the hydrological characteristics of the
territories. The claims can be characterized by some international framework
(principles) of entitlement. These international principles are appears to be
inequitable by the other agents in the presence of negative externalities. The
negotiated treaties address sharing water along with the issue of negative
externalities imposed by the upstream agent on the downstream agents. The
market based bargaining mechanism is used for modeling and for characterization
of agreement points.",Journal of Applied Econometrics ,http://arxiv.org/abs/1912.05844v1,stat.ME
96,"The effects of institutional quality on formal and informal borrowing
  across high-, middle-, and low-income countries","This paper examines the effects of institutional quality on financing choice
of individual using a large dataset of 137,160 people from 131 countries. We
classify borrowing activities into three categories, including formal,
constructive informal, and underground borrowing. Although the result shows
that better institutions aids the uses of formal borrowing, the impact of
institutions on constructive informal and underground borrowing among three
country sub-groups differs. Higher institutional quality improves constructive
informal borrowing in middle-income countries but reduces the use of
underground borrowing in high- and low-income countries.","Journal of Applied Economics and Business
",http://arxiv.org/abs/1903.07866v1,econ.GN
68,"Exact Solution for the Portfolio Diversification Problem Based on
  Maximizing the Risk Adjusted Return","The potential benefits of portfolio diversification have been known to
investors for a long time. Markowitz (1952) suggested the seminal approach for
optimizing the portfolio problem based on finding the weights as budget shares
that minimize the variance of the underlying portfolio. Hatemi-J and El-Khatib
(2015) suggested finding the weights that will result in maximizing the risk
adjusted return of the portfolio. This approach seems to be preferred by the
rational investors since it combines risk and return when the optimal budget
shares are sought for. The current paper provides a general solution for this
risk adjusted return problem that can be utilized for any potential number of
assets that are included in the portfolio.","Journal of Artificial Societies and Social Simulation 
",http://arxiv.org/abs/1903.01082v1,econ.EM
446,Final Topology for Preference Spaces,"We say a model is continuous in utilities (resp., preferences) if small
perturbations of utility functions (resp., preferences) generate small changes
in the model's outputs. While similar, these two concepts are equivalent only
when the topology satisfies the following universal property: for each
continuous mapping from preferences to model's outputs there is a unique
mapping from utilities to model's outputs that is faithful to the preference
map and is continuous. The topologies that satisfy such a universal property
are called final topologies. In this paper we analyze the properties of the
final topology for preference sets. This is of practical importance since most
of the analysis on continuity is done via utility functions and not the
primitive preference space. Our results allow the researcher to extrapolate
continuity in utility to continuity in the underlying preferences.",Journal of Big Data ,http://arxiv.org/abs/2004.02357v2,stat.ML
56,Strategically Simple Mechanisms,"We define and investigate a property of mechanisms that we call ""strategic
simplicity,"" and that is meant to capture the idea that, in strategically
simple mechanisms, strategic choices require limited strategic sophistication.
We define a mechanism to be strategically simple if choices can be based on
first-order beliefs about the other agents' preferences and first-order
certainty about the other agents' rationality alone, and there is no need for
agents to form higher-order beliefs, because such beliefs are irrelevant to the
optimal strategies. All dominant strategy mechanisms are strategically simple.
But many more mechanisms are strategically simple. In particular, strategically
simple mechanisms may be more flexible than dominant strategy mechanisms in the
bilateral trade problem and the voting problem.",Journal of Business and Economic Statistics,http://arxiv.org/abs/1812.00849v1,econ.EM
413,A Growth Model with Unemployment,"A standard growth model is modified in a straightforward way to incorporate
what Keynes (1936) suggests in the ""essence"" of his general theory. The
theoretical essence is the idea that exogenous changes in investment cause
changes in employment and unemployment. We implement this idea by assuming the
path for capital growth rate is exogenous in the growth model. The result is a
growth model that can explain both long term trends and fluctuations around the
trend. The modified growth model was tested using the U.S. economic data from
1947 to 2014. The hypothesized inverse relationship between the capital growth
and changes in unemployment was confirmed, and the structurally estimated model
fits fluctuations in unemployment reasonably well.",Journal of Business and Economic Statistics,http://arxiv.org/abs/1806.04228v1,econ.EM
504,On the Equivalence of Neural and Production Networks,"This paper identifies the mathematical equivalence between economic networks
of Cobb-Douglas agents and Artificial Neural Networks. It explores two
implications of this equivalence under general conditions. First, a burgeoning
literature has established that network propagation can transform microeconomic
perturbations into large aggregate shocks. Neural network equivalence amplifies
the magnitude and complexity of this phenomenon. Second, if economic agents
adjust their production and utility functions in optimal response to local
conditions, market pricing is a sufficient and robust channel for information
feedback leading to macro learning.",Journal of Business and Economic Statistics,http://arxiv.org/abs/2005.00510v2,econ.EM
281,"On the Equilibrium Uniqueness in Cournot Competition with Demand
  Uncertainty","We revisit the linear Cournot model with uncertain demand that is studied in
Lagerl\""of (2006)* and provide sufficient conditions for equilibrium uniqueness
that complement the existing results. We show that if the distribution of the
demand intercept has the decreasing mean residual demand (DMRD) or the
increasing generalized failure rate (IGFR) property, then uniqueness of
equilibrium is guaranteed. The DMRD condition implies log-concavity of the
expected profits per unit of output without additional assumptions on the
existence or the shape of the density of the demand intercept and, hence,
answers in the affirmative the conjecture of Lagerl\""of (2006)* that such
conditions may not be necessary.
  *Johan Lagerl\""of, Equilibrium uniqueness in a Cournot model with demand
uncertainty. The B.E. Journal in Theoretical Economics, Vol. 6: Iss 1.
(Topics), Article 19:1--6, 2006.",Journal of Business and Economic Statistics ,http://arxiv.org/abs/1906.03558v3,econ.EM
591,Equilibrium Refinement in Finite Evidence Games,"Evidence games study situations where a sender persuades a receiver by
selectively disclosing hard evidence about an unknown state of the world.
Evidence games often have multiple equilibria. Hart et al. (2017) propose to
focus on truth-leaning equilibria, i.e., perfect Bayesian equilibria where the
sender prefers disclosing truthfully when indifferent, and the receiver takes
off-path disclosure at face value. They show that a truth-leaning equilibrium
is an equilibrium of a perturbed game where the sender has an infinitesimal
reward for truth-telling. We show that, when the receiver's action space is
finite, truth-leaning equilibrium may fail to exist, and it is not equivalent
to equilibrium of the perturbed game. To restore existence, we introduce a
disturbed game with a small uncertainty about the receiver's payoff. A
purifiable equilibrium is a truth-leaning equilibrium in an infinitesimally
disturbed game. It exists and features a simple characterization. A
truth-leaning equilibrium that is also purifiable is an equilibrium of the
perturbed game.",Journal of Business Research ,http://arxiv.org/abs/2007.06403v1,econ.GN
470,"Emergent inequality and endogenous dynamics in a simple behavioral
  macroeconomic model","Standard macroeconomic models assume that households are rational in the
sense that they are perfect utility maximizers, and explain economic dynamics
in terms of shocks that drive the economy away from the stead-state. Here we
build on a standard macroeconomic model in which a single rational
representative household makes a savings decision of how much to consume or
invest. In our model households are myopic boundedly rational heterogeneous
agents embedded in a social network. From time to time each household updates
its savings rate by copying the savings rate of its neighbor with the highest
consumption. If the updating time is short, the economy is stuck in a poverty
trap, but for longer updating times economic output approaches its optimal
value, and we observe a critical transition to an economy with irregular
endogenous oscillations in economic output, resembling a business cycle. In
this regime households divide into two groups: Poor households with low savings
rates and rich households with high savings rates. Thus inequality and economic
dynamics both occur spontaneously as a consequence of imperfect household
decision making. Our work here supports an alternative program of research that
substitutes utility maximization for behaviorally grounded decision making.",Journal of Causal Inference ,http://arxiv.org/abs/1907.02155v1,stat.ME
94,"A micro-simulation model of irrigation farms in the southern
  Murray-Darling Basin","This paper presents a farm level irrigation microsimulation model of the
southern Murray-Darling Basin. The model leverages detailed ABARES survey data
to estimate a series of input demand and output supply equations, derived from
a normalised quadratic profit function. The parameters from this estimation are
then used to simulate the impact on total cost, revenue and profit of a
hypothetical 30 per cent increase in the price of water. The model is still
under development, with several potential improvements suggested in the
conclusion. This is a working paper, provided for the purpose of receiving
feedback on the analytical approach to improve future iterations of the
microsimulation model.",Journal of Cleaner Production,http://arxiv.org/abs/1903.05781v1,econ.GN
229,"Dominating Attributes Of Professed Firm Culture Of Holding Companies -
  Members Of The Bulgarian Industrial Capital Association","This article aims to outline the diversity of cultural phenomena that occur
at organizational level, emphasizing the place and role of the key attributes
of professed firm culture for the survival and successful development of big
business organizations. The holding companies, members of the Bulgarian
Industrial Capital Association, are chosen as a survey object as the mightiest
driving engines of the local economy. That is why their emergence and
development in the transition period is monitored and analyzed. Based on an
empirical study of relevant website content, important implications about
dominating attributes of professed firm culture on them are found and several
useful recommendations to their senior management are made.",Journal of Cleaner Production,http://arxiv.org/abs/1810.02617v1,econ.GN
448,"A geometric characterization of VES and Kadiyala-type production
  functions","The basic concepts of the differential geometry are shortly reviewed and
applied to the study of VES production function in the spirit of the works of
V\^ilcu and collaborators. A similar characterization is given for a more
general production function, namely the Kadiyala production function, in the
case of developable surfaces.",Journal of Cleaner Production ,http://arxiv.org/abs/2004.09617v1,physics.soc-ph
480,Cancer Risk Messages: Public Health and Economic Welfare,"Statements for public health purposes such as ""1 in 2 will get cancer by age
85"" have appeared in public spaces. The meaning drawn from such statements
affects economic welfare, not just public health. Both markets and government
use risk information on all kinds of risks, useful information can, in turn,
improve economic welfare, however inaccuracy can lower it. We adapt the
contingency table approach so that a quoted risk is cross-classified with the
states of nature. We show that bureaucratic objective functions regarding the
accuracy of a reported cancer risk can then be stated.",Journal of Complex Networks,http://arxiv.org/abs/1807.03045v2,physics.soc-ph
496,On the Choice of Instruments in Mixed Frequency Specification Tests,"Time averaging has been the traditional approach to handle mixed sampling
frequencies. However, it ignores information possibly embedded in high
frequency. Mixed data sampling (MIDAS) regression models provide a concise way
to utilize the additional information in high-frequency variables. In this
paper, we propose a specification test to choose between time averaging and
MIDAS models, based on a Durbin-Wu-Hausman test. In particular, a set of
instrumental variables is proposed and theoretically validated when the
frequency ratio is large. As a result, our method tends to be more powerful
than existing methods, as reconfirmed through the simulations.",Journal of Computational and Applied Mathematics,http://arxiv.org/abs/1809.05503v1,math.ST
596,"Generating Empirical Core Size Distributions of Hedonic Games using a
  Monte Carlo Method","Data analytics allows an analyst to gain insight into underlying populations
through the use of various computational approaches, including Monte Carlo
methods. This paper discusses an approach to apply Monte Carlo methods to
hedonic games. Hedonic games have gain popularity over the last two decades
leading to several research articles that are concerned with the necessary,
sufficient, or both conditions of the existence of a core partition.
Researchers have used analytical methods for this work. We propose that using a
numerical approach will give insights that might not be available through
current analytical methods. In this paper, we describe an approach to
representing hedonic games, with strict preferences, in a matrix form that can
easily be generated; that is, a hedonic game with randomly generated
preferences for each player. Using this generative approach, we were able to
create and solve, i.e., find any core partitions, of millions of hedonic games.
Our Monte Carlo experiment generated games with up to thirteen players. The
results discuss the distribution form of the core size of the games of a given
number of players. We also discuss computational considerations. Our numerical
study of hedonic games gives insight into the underlying properties of hedonic
games.",Journal of Criminal Justice,http://arxiv.org/abs/2007.12127v1,cs.LG
222,The paradox of monotone structural QRE,"McKelvey and Palfrey (1995)'s monotone structural Quantal Response
Equilibrium theory may be misspecified for the study of monotone behavior.",Journal of Data Science and Its Applications,http://arxiv.org/abs/1905.05814v2,econ.GN
471,Relationships between different Macroeconomic Variables using VECM,"Through this paper, an attempt has been made to quantify the underlying
relationships between the leading macroeconomic indicators. More clearly, an
effort has been made in this paper to assess the cointegrating relationships
and examine the error correction behavior revealed by macroeconomic variables
using econometric techniques that were initially developed by Engle and Granger
(1987), and further explored by various succeeding papers, with the latest
being Tu and Yi (2017). Gross Domestic Product, Discount Rate, Consumer Price
Index and population of U.S are representatives of the economy that have been
used in this study to analyze the relationships between economic indicators and
understand how an adverse change in one of these variables might have
ramifications on the others. This is performed to corroborate and guide the
belief that a policy maker with specified intentions cannot ignore the
spillover effects caused by implementation of a certain policy.",Journal of Development Economics ,http://arxiv.org/abs/1907.04447v1,econ.EM
519,"The importance of being discrete: on the inaccuracy of continuous
  approximations in auction theory","While auction theory views bids and valuations as continuous variables,
real-world auctions are necessarily discrete. In this paper, we use a
combination of analytical and computational methods to investigate whether
incorporating discreteness substantially changes the predictions of auction
theory, focusing on the case of uniformly distributed valuations so that our
results bear on the majority of auction experiments. In some cases, we find
that introducing discreteness changes little. For example, the first-price
auction with two bidders and an even number of values has a symmetric
equilibrium that closely resembles its continuous counterpart and converges to
its continuous counterpart as the discretisation goes to zero. In others,
however, we uncover discontinuity results. For instance, introducing an
arbitrarily small amount of discreteness into the all-pay auction makes its
symmetric, pure-strategy equilibrium disappear; and appears (based on
computational experiments) to rob the game of pure-strategy equilibria
altogether. These results raise questions about the continuity approximations
on which auction theory is based and prompt a re-evaluation of the experimental
literature.",Journal of Development Economics ,http://arxiv.org/abs/2006.03016v2,econ.EM
144,"Corrigendum to ""Managerial Incentive Problems: A Dynamic Perspective""","This paper corrects some mathematical errors in Holmstr\""om (1999) and
clarifies the assumptions that are sufficient for the results of Holmstr\""om
(1999). The results remain qualitatively the same.",Journal of Eastern Europe Research in Business and Economics,http://arxiv.org/abs/1811.00455v2,econ.GN
147,Mechanism Design with Limited Commitment,"We develop a tool akin to the revelation principle for dynamic
mechanism-selection games in which the designer can only commit to short-term
mechanisms. We identify a canonical class of mechanisms rich enough to
replicate the outcomes of any equilibrium in a mechanism-selection game between
an uninformed designer and a privately informed agent. A cornerstone of our
methodology is the idea that a mechanism should encode not only the rules that
determine the allocation, but also the information the designer obtains from
the interaction with the agent. Therefore, how much the designer learns, which
is the key tension in design with limited commitment, becomes an explicit part
of the design. Our result simplifies the search for the designer-optimal
outcome by reducing the agent's behavior to a series of participation,
truthtelling, and Bayes' plausibility constraints the mechanisms must satisfy.",Journal of Eastern Europe Research in Business and Economics,http://arxiv.org/abs/1811.03579v6,econ.GN
306,Lee-Carter method for forecasting mortality for Peruvian Population,"In this article, we have modeled mortality rates of Peruvian female and male
populations during the period of 1950-2017 using the Lee-Carter (LC) model. The
stochastic mortality model was introduced by Lee and Carter (1992) and has been
used by many authors for fitting and forecasting the human mortality rates. The
Singular Value Decomposition (SVD) approach is used for estimation of the
parameters of the LC model. Utilizing the best fitted auto regressive
integrated moving average (ARIMA) model we forecast the values of the time
dependent parameter of the LC model for the next thirty years. The forecasted
values of life expectancy at different age group with $95\%$ confidence
intervals are also reported for the next thirty years. In this research we use
the data, obtained from the Peruvian National Institute of Statistics (INEI).",Journal of Ecology and The Natural Environment,http://arxiv.org/abs/1811.09622v1,econ.GN
53,Mechanism Design with News Utility,"News utility is the idea that the utility of an agent depends on changes in
her beliefs over consumption and money. We introduce news utility into
otherwise classical static Bayesian mechanism design models. We show that a key
role is played by the timeline of the mechanism, i.e. whether there are delays
between the announcement stage, the participation stage, the play stage and the
realization stage of a mechanism. Depending on the timing, agents with news
utility can experience two additional news utility effects: a surprise effect
derived from comparing to pre-mechanism beliefs, as well as a realization
effect derived from comparing post-play beliefs with the actual outcome of the
mechanism.
  We look at two distinct mechanism design settings reflecting the two main
strands of the classical literature. In the first model, a monopolist screens
an agent according to the magnitude of her loss aversion. In the second model,
we consider a general multi-agent Bayesian mechanism design setting where the
uncertainty of each player stems from not knowing the intrinsic types of the
other agents. We give applications to auctions and public good provision which
illustrate how news utility changes classical results.
  For both models we characterize the optimal design of the timeline. A
timeline featuring no delay between participation and play but a delay in
realization is never optimal in either model. In the screening model the
optimal timeline is one without delays. In auction settings, under fairly
natural assumptions the optimal timeline has delays between all three stages of
the mechanism.",Journal of Econometrics,http://arxiv.org/abs/1808.04020v1,econ.EM
113,Panel Data Analysis with Heterogeneous Dynamics,"This paper proposes a model-free approach to analyze panel data with
heterogeneous dynamic structures across observational units. We first compute
the sample mean, autocovariances, and autocorrelations for each unit, and then
estimate the parameters of interest based on their empirical distributions. We
then investigate the asymptotic properties of our estimators using double
asymptotics and propose split-panel jackknife bias correction and inference
based on the cross-sectional bootstrap. We illustrate the usefulness of our
procedures by studying the deviation dynamics of the law of one price. Monte
Carlo simulations confirm that the proposed bias correction is effective and
yields valid inference in small samples.",Journal of Econometrics,http://arxiv.org/abs/1803.09452v2,econ.EM
171,"Extended opportunity cost model to find near equilibrium electricity
  prices under non-convexities","This paper finds near equilibrium prices for electricity markets with
nonconvexities due to binary variables, in order to reduce the market
participants' opportunity costs, such as generators' unrecovered costs. The
opportunity cost is defined as the difference between the profit when the
instructions of the market operator are followed and when the market
participants can freely make their own decisions based on the market prices. We
use the minimum complementarity approximation to the minimum total opportunity
cost (MTOC) model, from previous research, with tests on a much more realistic
unit commitment (UC) model than in previous research, including features such
as reserve requirements, ramping constraints, and minimum up and down times.
The developed model incorporates flexible price responsive demand, as in
previous research, but since not all demand is price responsive, we consider
the more realistic case that total demand is a mixture of fixed and flexible.
Another improvement over previous MTOC research is computational: whereas the
previous research had nonconvex terms among the objective function's continuous
variables, we convert the objective to an equivalent form that contains only
linear and convex quadratic terms in the continuous variables. We compare the
unit commitment model with the standard social welfare optimization version of
UC, in a series of sensitivity analyses, varying flexible demand to represent
varying degrees of future penetration of electric vehicles and smart
appliances, different ratios of generation availability, and different values
of transmission line capacities to consider possible congestion. The minimum
total opportunity cost and social welfare solutions are mostly very close in
different scenarios, except in some extreme cases.",Journal of Econometrics,http://arxiv.org/abs/1809.09734v1,econ.EM
173,"A New Form of Banking -- Concept and Mathematical Model of Venture
  Banking","This theoretical model contains concept, equations, and graphical results for
venture banking. A system of 27 equations describes the behavior of the
venture-bank and underwriter system allowing phase-space type graphs that show
where profits and losses occur. These results confirm and expand those obtained
from the original spreadsheet based model. An example investment in a castle at
a loss is provided to clarify concept. This model requires that all investments
are in enterprises that create new utility value. The assessed utility value
created is the new money out of which the venture bank and underwriter are
paid. The model presented chooses parameters that ensure that the venture-bank
experiences losses before the underwriter does. Parameters are: DIN Premium,
0.05; Clawback lien fraction, 0.77; Clawback bonds and equity futures discount,
1.5 x (USA 12 month LIBOR); Range of clawback bonds sold, 0 to 100%; Range of
equity futures sold 0 to 70%.",Journal of Econometrics,http://arxiv.org/abs/1810.00516v8,econ.EM
265,"Estimation of Peer Effects in Endogenous Social Networks: Control
  Function Approach","We propose a method of estimating the linear-in-means model of peer effects
in which the peer group, defined by a social network, is endogenous in the
outcome equation for peer effects. Endogeneity is due to unobservable
individual characteristics that influence both link formation in the network
and the outcome of interest. We propose two estimators of the peer effect
equation that control for the endogeneity of the social connections using a
control function approach. We leave the functional form of the control function
unspecified and treat it as unknown. To estimate the model, we use a sieve
semiparametric approach, and we establish asymptotics of the semiparametric
estimator.",Journal of Econometrics,http://arxiv.org/abs/1709.10024v3,stat.ME
419,"The transmission of uncertainty shocks on income inequality: State-level
  evidence from the United States","In this paper, we explore the relationship between state-level household
income inequality and macroeconomic uncertainty in the United States. Using a
novel large-scale macroeconometric model, we shed light on regional disparities
of inequality responses to a national uncertainty shock. The results suggest
that income inequality decreases in most states, with a pronounced degree of
heterogeneity in terms of shapes and magnitudes of the dynamic responses. By
contrast, some few states, mostly located in the West and South census region,
display increasing levels of income inequality over time. We find that this
directional pattern in responses is mainly driven by the income composition and
labor market fundamentals. In addition, forecast error variance decompositions
allow for a quantitative assessment of the importance of uncertainty shocks in
explaining income inequality. The findings highlight that volatility shocks
account for a considerable fraction of forecast error variance for most states
considered. Finally, a regression-based analysis sheds light on the driving
forces behind differences in state-specific inequality responses.",Journal of Econometrics,http://arxiv.org/abs/1806.08278v1,math.ST
162,"Hospitality Students' Perceptions towards Working in Hotels: a case
  study of the faculty of tourism and hotels in Alexandria University","The tourism and hospitality industry worldwide has been confronted with the
problem of attracting and retaining quality employees. If today's students are
to become the effective practitioners of tomorrow, it is fundamental to
understand their perceptions of tourism employment. Therefore, this research
aims at investigating the perceptions of hospitality students at the Faculty of
Tourism in Alexandria University towards the industry as a career choice. A
self-administrated questionnaire was developed to rate the importance of 20
factors in influencing career choice, and the extent to which hospitality as a
career offers these factors. From the results, it is clear that students
generally do not believe that the hospitality career will offer them the
factors they found important. However, most of respondents (70.6%) indicated
that they would work in the industry after graduation. Finally, a set of
specific remedial actions that hospitality stakeholders could initiate to
improve the perceptions of hospitality career are discussed.","Journal of Econometrics
",http://arxiv.org/abs/1807.09660v1,econ.EM
34,A Note on the Multi-Agent Contracts in Continuous Time,"Dynamic contracts with multiple agents is a classical decentralized
decision-making problem with asymmetric information. In this paper, we extend
the single-agent dynamic incentive contract model in continuous-time to a
multi-agent scheme in finite horizon and allow the terminal reward to be
dependent on the history of actions and incentives. We first derive a set of
sufficient conditions for the existence of optimal contracts in the most
general setting and conditions under which they form a Nash equilibrium. Then
we show that the principal's problem can be converted to solving
Hamilton-Jacobi-Bellman (HJB) equation requiring a static Nash equilibrium.
Finally, we provide a framework to solve this problem by solving partial
differential equations (PDE) derived from backward stochastic differential
equations (BSDE).",Journal of Econometrics ,http://arxiv.org/abs/1710.00377v2,econ.EM
55,The Structure of Equilibria in Trading Networks with Frictions,"Several structural results for the set of competitive equilibria in trading
networks with frictions are established: The lattice theorem, the rural
hospitals theorem, the existence of side-optimal equilibria, and a
group-incentive-compatibility result hold with imperfectly transferable utility
and in the presence of frictions. While our results are developed in a trading
network model, they also imply analogous (and new) results for exchange
economies with combinatorial demand and for two-sided matching markets with
transfers.",Journal of Econometrics ,http://arxiv.org/abs/1808.07924v6,econ.EM
81,"Does Random Consideration Explain Behavior when Choice is Hard? Evidence
  from a Large-scale Experiment","We study population behavior when choice is hard because considering
alternatives is costly. To simplify their choice problem, individuals may pay
attention to only a subset of available alternatives. We design and implement a
novel online experiment that exogenously varies choice sets and consideration
costs for a large sample of individuals. We provide a theoretical and
statistical framework that allows us to test random consideration at the
population level. Within this framework, we compare competing models of random
consideration. We find that the standard random utility model fails to explain
the population behavior. However, our results suggest that a model of random
consideration with logit attention and heterogeneous preferences provides a
good explanation for the population behavior. Finally, we find that the random
consideration rule that subjects use is different for different consideration
costs while preferences are not. We observe that the higher the consideration
cost the further behavior is from the full-consideration benchmark, which
supports the hypothesis that hard choices have a substantial negative impact on
welfare via limited consideration.",Journal of Econometrics ,http://arxiv.org/abs/1812.09619v2,econ.EM
250,The Africa-Dummy: Gone with the Millennium?,"A fixed effects regression estimator is introduced that can directly identify
and estimate the Africa-Dummy in one regression step so that its correct
standard errors as well as correlations to other coefficients can easily be
estimated. We can estimate the Nickel bias and found it to be negligibly tiny.
Semiparametric extensions check whether the Africa-Dummy is simply a result of
misspecification of the functional form. In particular, we show that the
returns to growth factors are different for Sub-Saharan African countries
compared to the rest of the world. For example, returns to population growth
are positive and beta-convergence is faster. When extending the model to
identify the development of the Africa-Dummy over time we see that it has been
changing dramatically over time and that the punishment for Sub-Saharan African
countries has been decreasing incrementally to reach insignificance around the
turn of the millennium.",Journal of Econometrics ,http://arxiv.org/abs/1903.02357v1,stat.ME
278,Conventions and Coalitions in Repeated Games,"We develop a theory of repeated interaction for coalitional behavior. We
consider stage games where both individuals and coalitions may deviate.
However, coalition members cannot commit to long-run behavior, and anticipate
that today's actions influence tomorrow's behavior. We evaluate the degree to
which history-dependence can deter coalitional deviations. If monitoring is
perfect, every feasible and strictly individually rational payoff can be
supported by history-dependent conventions. By contrast, if players can make
secret side-payments to each other, every coalition achieves a coalitional
minmax value, potentially reducing the set of supportable payoffs to the core
of the stage game.",Journal of Econometrics ,http://arxiv.org/abs/1906.00280v2,stat.ME
335,Prediction of Shared Bicycle Demand with Wavelet Thresholding,"Consumers are creatures of habit, often periodic, tied to work, shopping and
other schedules. We analyzed one month of data from the world's largest
bike-sharing company to elicit demand behavioral cycles, initially using models
from animal tracking that showed large customers fit an Ornstein-Uhlenbeck
model with demand peaks at periodicities of 7, 12, 24 hour and 7-days. Lorenz
curves of bicycle demand showed that the majority of customer usage was
infrequent, and demand cycles from time-series models would strongly overfit
the data yielding unreliable models. Analysis of thresholded wavelets for the
space-time tensor of bike-sharing contracts was able to compress the data into
a 56-coefficient model with little loss of information, suggesting that
bike-sharing demand behavior is exceptionally strong and regular. Improvements
to predicted demand could be made by adjusting for 'noise' filtered by our
model from air quality and weather information and demand from infrequent
riders.",Journal of Econometrics ,http://arxiv.org/abs/1802.02683v1,econ.EM
427,Fuzzy Group Identification Problems,"We present a fuzzy version of the Group Identification Problem (""Who is a
J?"") introduced by Kasher and Rubinstein (1997). We consider a class $N =
\{1,2,\ldots,n\}$ of agents, each one with an opinion about the membership to a
group J of the members of the society, consisting in a function $\pi : N \to
[0; 1]$, indicating for each agent, including herself, the degree of membership
to J. We consider the problem of aggregating those functions, satisfying
different sets of axioms and characterizing different aggregators. While some
results are analogous to those of the originally crisp model, the fuzzy version
is able to overcome some of the main impossibility results of Kasher and
Rubinstein.",Journal of Econometrics ,http://arxiv.org/abs/1912.05540v2,math.ST
432,Targeting in social networks with anonymized information,"This paper studies whether a planner who only has information about the
network topology can discriminate among agents according to their network
position. The planner proposes a simple menu of contracts, one for each
location, in order to maximize total welfare, and agents choose among the menu.
This mechanism is immune to deviations by single agents, and to deviations by
groups of agents of sizes 2, 3 and 4 if side-payments are ruled out. However,
if compensations are allowed, groups of agents may have an incentive to jointly
deviate from the optimal contract in order to exploit other agents. We identify
network topologies for which the optimal contract is group incentive compatible
with transfers: undirected networks and regular oriented trees, and network
topologies for which the planner must assign uniform quantities: single root
and nested neighborhoods directed networks.",Journal of Econometrics ,http://arxiv.org/abs/2001.03122v1,math.ST
453,Economic Performance Through Time: A Dynamical Theory,"The central problems of Development Economics are the explanation of the
gross disparities in the global distribution, $\cal{D}$, of economic
performance, $\cal{E}$, and the persistence, $\cal{P}$, of said distribution.
Douglass North argued, epigrammatically, that institutions, $\cal{I}$, are the
rules of the game, meaning that $\cal{I}$ determines or at least constrains
$\cal{E}$. This promised to explain $\cal{D}$. 65,000 citations later, the
central problems remain unsolved. North's institutions are informal, slowly
changing cultural norms as well as roads, guilds, and formal legislation that
may change overnight. This definition, mixing the static and the dynamic, is
unsuited for use in a necessarily time dependent theory of developing
economies. We offer here a suitably precise definition of $\cal{I}$, a
dynamical theory of economic development, a new measure of the economy, an
explanation of $\cal{P}$, a bivariate model that explains half of $\cal{D}$,
and a critical reconsideration of North's epigram.",Journal of Econometrics ,http://arxiv.org/abs/1905.02956v1,math.ST
198,"Econometric analysis of potential outcomes time series: instruments,
  shocks, linearity and the causal response function","Bojinov & Shephard (2019) defined potential outcome time series to
nonparametrically measure dynamic causal effects in time series experiments.
Four innovations are developed in this paper: ""instrumental paths,"" treatments
which are ""shocks,"" ""linear potential outcomes"" and the ""causal response
function."" Potential outcome time series are then used to provide a
nonparametric causal interpretation of impulse response functions, generalized
impulse response functions, local projections and LP-IV.",Journal of Economic and Social Thought ,http://arxiv.org/abs/1903.01637v3,econ.GN
93,A fractional-order difference Cournot duopoly game with long memory,"We reconsider the Cournot duopoly problem in light of the theory for long
memory. We introduce the Caputo fractional-order difference calculus to
classical duopoly theory to propose a fractional-order discrete Cournot duopoly
game model, which allows participants to make decisions while making full use
of their historical information. Then we discuss Nash equilibria and local
stability by using linear approximation. Finally, we detect the chaos of the
model by employing a 0-1 test algorithm.",Journal of Economic Behavior and Organization,http://arxiv.org/abs/1903.04305v1,econ.GN
354,"Time-consistent decisions and rational expectation equilibrium existence
  in DSGE models","Under some initial conditions, it is shown that time consistency requirements
prevent rational expectation equilibrium (REE) existence for dynamic stochastic
general equilibrium models induced by consumer heterogeneity, in contrast to
static models. However, one can consider REE-prohibiting initial conditions as
limits of other initial conditions. The REE existence issue then is overcome by
using a limit of economies. This shows that significant care must be taken of
when dealing with rational expectation equilibria.",Journal of Economic Behavior and Organization,http://arxiv.org/abs/1909.10915v4,econ.TH
32,A Residual Bootstrap for Conditional Value-at-Risk,"This paper proposes a fixed-design residual bootstrap method for the two-step
estimator of Francq and Zako\""ian (2015) associated with the conditional
Value-at-Risk. The bootstrap's consistency is proven for a general class of
volatility models and intervals are constructed for the conditional
Value-at-Risk. A simulation study reveals that the equal-tailed percentile
bootstrap interval tends to fall short of its nominal value. In contrast, the
reversed-tails bootstrap interval yields accurate coverage. We also compare the
theoretically analyzed fixed-design bootstrap with the recursive-design
bootstrap. It turns out that the fixed-design bootstrap performs equally well
in terms of average coverage, yet leads on average to shorter intervals in
smaller samples. An empirical application illustrates the interval estimation.","Journal of Economic Behavior and Organization
",http://arxiv.org/abs/1808.09125v3,econ.GN
108,"Testing Continuity of a Density via g-order statistics in the Regression
  Discontinuity Design","In the regression discontinuity design (RDD), it is common practice to assess
the credibility of the design by testing the continuity of the density of the
running variable at the cut-off, e.g., McCrary (2008). In this paper we propose
an approximate sign test for continuity of a density at a point based on the
so-called g-order statistics, and study its properties under two complementary
asymptotic frameworks. In the first asymptotic framework, the number q of
observations local to the cut-off is fixed as the sample size n diverges to
infinity, while in the second framework q diverges to infinity slowly as n
diverges to infinity. Under both of these frameworks, we show that the test we
propose is asymptotically valid in the sense that it has limiting rejection
probability under the null hypothesis not exceeding the nominal level. More
importantly, the test is easy to implement, asymptotically valid under weaker
conditions than those used by competing methods, and exhibits finite sample
validity under stronger conditions than those needed for its asymptotic
validity. In a simulation study, we find that the approximate sign test
provides good control of the rejection probability under the null hypothesis
while remaining competitive under the alternative hypothesis. We finally apply
our test to the design in Lee (2008), a well-known application of the RDD to
study incumbency advantage.",Journal of Economic Interaction and Coordination,http://arxiv.org/abs/1803.07951v6,econ.GN
295,Third person enforcement in a prisoner's dilemma game,"We theoretically study the effect of a third person enforcement on a one-shot
prisoner's dilemma game played by two persons, with whom the third person plays
repeated prisoner's dilemma games. We find that the possibility of the third
person's future punishment causes them to cooperate in the one-shot game.",Journal of Economic Interaction and Coordination,http://arxiv.org/abs/1908.04971v1,econ.GN
378,How on Earth: Flourishing in a Not-for-Profit World by 2050,"In this book, we outline a model of a non-capitalist market economy based on
not-for-profit forms of business. This work presents both a critique of the
current economic system and a vision of a more socially, economically, and
ecologically sustainable economy. The point of departure is the purpose and
profit-orientation embedded in the legal forms used by businesses (e.g.,
for-profit or not-for-profit) and the ramifications of this for global
sustainability challenges such as environmental pollution, resource use,
climate change, and economic inequality. We document the rapid rise of
not-for-profit forms of business in the global economy and offer a conceptual
framework and an analytical lens through which to view these relatively new
economic actors and their potential for transforming the economy. The book
explores how a market consisting of only or mostly not-for-profit forms of
business might lead to better financial circulation, economic equality, social
well-being, and environmental regeneration as compared to for-profit markets.",Journal of Economic Interaction and Coordination,http://arxiv.org/abs/1902.01398v1,econ.GN
575,Repeated Communication with Private Lying Cost,"I study repeated communication games between a patient sender and a sequence
of receivers. The sender has persistent private information about his
psychological cost of lying, and in every period, can privately observe the
realization of an i.i.d. state before communication takes place. I characterize
every type of sender's highest equilibrium payoff. When the highest lying cost
in the support of the receivers' prior belief approaches the sender's benefit
from lying, every type's highest equilibrium payoff in the repeated
communication game converges to his equilibrium payoff in a one-shot Bayesian
persuasion game. I also show that in every sender-optimal equilibrium, no type
of sender mixes between telling the truth and lying at every history. When
there exist ethical types whose lying costs outweigh their benefits, I provide
necessary and sufficient conditions for all non-ethical type senders to attain
their optimal commitment payoffs. I identify an outside option effect through
which the possibility of being ethical decreases every non-ethical type's
payoff.",Journal of Economic Interaction and Coordination,http://arxiv.org/abs/2006.08069v1,q-fin.GN
594,Only Time Will Tell: Credible Dynamic Signaling,"This paper characterizes informational outcomes in a model of dynamic
signaling with vanishing commitment power. It shows that contrary to popular
belief, informative equilibria with payoff-relevant signaling can exist without
requiring unreasonable off-path beliefs. The paper provides a sharp
characterization of possible separating equilibria: all signaling must take
place through attrition, when the weakest type mixes between revealing own type
and pooling with the stronger types. The framework explored in the paper is
general, imposing only minimal assumptions on payoff monotonicity and
single-crossing. Applications to bargaining, monopoly price signaling, and
labor market signaling are developed to demonstrate the results in specific
contexts.",Journal of Economic Interaction and Coordination,http://arxiv.org/abs/2007.09568v3,q-fin.GN
570,"Identification and estimation of multinomial choice models with latent
  special covariates","Identification of multinomial choice models is often established by using
special covariates that have full support. This paper shows how these
identification results can be extended to a large class of multinomial choice
models when all covariates are bounded. I also provide a new
$\sqrt{n}$-consistent asymptotically normal estimator of the finite-dimensional
parameters of the model.",Journal of Economic Interaction and Coordination ,http://arxiv.org/abs/1811.05555v3,q-fin.GN
12,Forecasting the sustainable status of the labor market in agriculture,"In this article, a game-theoretic model is constructed that is related to the
problem of optimal assignments. Examples are considered. A compromise point is
found, the Nash equilibriums and the decision of the Nash arbitration scheme
are constructed.",Journal of Economic Theory,http://arxiv.org/abs/1805.09686v1,econ.TH
65,Interdistrict School Choice: A Theory of Student Assignment,"Interdistrict school choice programs-where a student can be assigned to a
school outside of her district-are widespread in the US, yet the market-design
literature has not considered such programs. We introduce a model of
interdistrict school choice and present two mechanisms that produce stable or
efficient assignments. We consider three categories of policy goals on
assignments and identify when the mechanisms can achieve them. By introducing a
novel framework of interdistrict school choice, we provide a new avenue of
research in market design.",Journal of Economic Theory,http://arxiv.org/abs/1812.11297v2,econ.GN
102,Pricing Mechanism in Information Goods,"We study three pricing mechanisms' performance and their effects on the
participants in the data industry from the data supply chain perspective. A
win-win pricing strategy for the players in the data supply chain is proposed.
We obtain analytical solutions in each pricing mechanism, including the
decentralized and centralized pricing, Nash Bargaining pricing, and revenue
sharing mechanism.",Journal of Economic Theory,http://arxiv.org/abs/1803.01530v1,econ.TH
283,Informed Principal Problems in Bilateral Trading,"We study bilateral trade with interdependent values as an informed-principal
problem. The mechanism-selection game has multiple equilibria that differ with
respect to principal's payoff and trading surplus. We characterize the
equilibrium that is worst for every type of principal, and characterize the
conditions under which there are no equilibria with different payoffs for the
principal. We also show that this is the unique equilibrium that survives the
intuitive criterion.",Journal of Economic Theory,http://arxiv.org/abs/1906.10311v6,math.PR
179,Estimation and Inference for Policy Relevant Treatment Effects,"The policy relevant treatment effect (PRTE) measures the average effect of
switching from a status-quo policy to a counterfactual policy. Estimation of
the PRTE involves estimation of multiple preliminary parameters, including
propensity scores, conditional expectation functions of the outcome and
covariates given the propensity score, and marginal treatment effects. These
preliminary estimators can affect the asymptotic distribution of the PRTE
estimator in complicated and intractable manners. In this light, we propose an
orthogonal score for double debiased estimation of the PRTE, whereby the
asymptotic distribution of the PRTE estimator is obtained without any influence
of preliminary parameter estimators as far as they satisfy mild requirements of
convergence rates. To our knowledge, this paper is the first to develop limit
distribution theories for inference about the PRTE.","Journal of Economic Theory
",http://arxiv.org/abs/1805.11503v4,math.PR
239,"Causal Tree Estimation of Heterogeneous Household Response to
  Time-Of-Use Electricity Pricing Schemes","We examine the household-specific effects of the introduction of Time-of-Use
(TOU) electricity pricing schemes. Using a causal forest (Athey and Imbens,
2016; Wager and Athey, 2018; Athey et al., 2019), we consider the association
between past consumption and survey variables, and the effect of TOU pricing on
household electricity demand. We describe the heterogeneity in household
variables across quartiles of estimated demand response and utilise variable
importance measures.
  Household-specific estimates produced by a causal forest exhibit reasonable
associations with covariates. For example, households that are younger, more
educated, and that consume more electricity, are predicted to respond more to a
new pricing scheme. In addition, variable importance measures suggest that some
aspects of past consumption information may be more useful than survey
information in producing these estimates.",Journal of Economic Theory ,http://arxiv.org/abs/1810.09179v3,econ.TH
245,"Decision-making in Livestock Biosecurity Practices amidst Environmental
  and Social Uncertainty: Evidence from an Experimental Game","Livestock industries are vulnerable to disease threats, which can cost
billions of dollars and have substantial negative social ramifications. Losses
are mitigated through increased use of disease-related biosecurity practices,
making increased biosecurity an industry goal. Currently, there is no
industry-wide standard for sharing information about disease incidence or
on-site biosecurity strategies, resulting in uncertainty regarding disease
prevalence and biosecurity strategies employed by industry stakeholders. Using
an experimental simulation game, we examined human participant's willingness to
invest in biosecurity when confronted with disease outbreak scenarios. We
varied the scenarios by changing the information provided about 1) disease
incidence and 2) biosecurity strategy or response by production facilities to
the threat of disease. Here we show that willingness to invest in biosecurity
increases with increased information about disease incidence, but decreases
with increased information about biosecurity practices used by nearby
facilities. Thus, the type or context of the uncertainty confronting the
decision maker may be a major factor influencing behavior. Our findings suggest
that policies and practices that encourage greater sharing of disease incidence
information should have the greatest benefit for protecting herd health.",Journal of Economic Theory ,http://arxiv.org/abs/1811.01081v2,econ.TH
129,Preference Identification,"An experimenter seeks to learn a subject's preference relation. The
experimenter produces pairs of alternatives. For each pair, the subject is
asked to choose. We argue that, in general, large but finite data do not give
close approximations of the subject's preference, even when the limiting
(countably infinite) data are enough to infer the preference perfectly. We
provide sufficient conditions on the set of alternatives, preferences, and
sequences of pairs so that the observation of finitely many choices allows the
experimenter to learn the subject's preference with arbitrary precision. While
preferences can be identified under our sufficient conditions, we show that it
is harder to identify utility functions. We illustrate our results with several
examples, including consumer choice, expected utility, and preferences in the
Anscombe-Aumann model.",Journal of Economic Theory and Econometrics,http://arxiv.org/abs/1807.11585v1,econ.TH
109,Causal Inference for Survival Analysis,"In this paper, we propose the use of causal inference techniques for survival
function estimation and prediction for subgroups of the data, upto individual
units. Tree ensemble methods, specifically random forests were modified for
this purpose. A real world healthcare dataset was used with about 1800 patients
with breast cancer, which has multiple patient covariates as well as disease
free survival days (DFS) and a death event binary indicator (y). We use the
type of cancer curative intervention as the treatment variable (T=0 or 1,
binary treatment case in our example). The algorithm is a 2 step approach. In
step 1, we estimate heterogeneous treatment effects using a causalTree with the
DFS as the dependent variable. Next, in step 2, for each selected leaf of the
causalTree with distinctly different average treatment effect (with respect to
survival), we fit a survival forest to all the patients in that leaf, one
forest each for treatment T=0 as well as T=1 to get estimated patient level
survival curves for each treatment (more generally, any model can be used at
this step). Then, we subtract the patient level survival curves to get the
differential survival curve for a given patient, to compare the survival
function as a result of the 2 treatments. The path to a selected leaf also
gives us the combination of patient features and their values which are
causally important for the treatment effect difference at the leaf.","Journal of Economics and Political Economy
",http://arxiv.org/abs/1803.08218v1,econ.GN
203,Fair Odds for Noisy Probabilities,"We suggest that one individual holds multiple degrees of belief about an
outcome, given the evidence. We then investigate the implications of such noisy
probabilities for a buyer and a seller of binary options and find the odds
agreed upon to ensure zero-expectation betting, differ from those consistent
with the relative frequency of outcomes. More precisely, the buyer and the
seller agree to odds that are higher (lower) than the reciprocal of their
averaged unbiased probabilities when this average indicates the outcome is more
(less) likely to occur than chance. The favorite-longshot bias thereby emerges
to establish the foundation of an equitable market. As corollaries, our work
suggests the old-established way of revealing someone's degree of belief
through wagers may be more problematic than previously thought, and implies
that betting markets cannot generally promise to support rational decisions.",Journal of Economics and Political Economy ,http://arxiv.org/abs/1811.12516v1,econ.GN
23,"Dynamic Programming with Recursive Preferences: Optimality and
  Applications","This paper provides new conditions for dynamic optimality in discrete time
and uses them to establish fundamental dynamic programming results for several
commonly used recursive preference specifications. These include Epstein-Zin
preferences, risk-sensitive preferences, narrow framing models and recursive
preferences with sensitivity to ambiguity. The results obtained for these
applications include (i) existence of optimal policies, (ii) uniqueness of
solutions to the Bellman equation, (iii) a complete characterization of optimal
policies via Bellman's principle of optimality, and (iv) a globally convergent
method of computation via value function iteration.",Journal of Energy Efficiency,http://arxiv.org/abs/1812.05748v4,econ.EM
241,"Expropriations, Property Confiscations and New Offshore Entities:
  Evidence from the Panama Papers","Using the Panama Papers, we show that the beginning of media reporting on
expropriations and property confiscations in a country increases the
probability that offshore entities are incorporated by agents from the same
country in the same month. This result is robust to the use of country-year
fixed effects and the exclusion of tax havens. Further analysis shows that the
effect is driven by countries with non-corrupt and effective governments, which
supports the notion that offshore entities are incorporated when reasonably
well-intended and well-functioning governments become more serious about
fighting organized crime by confiscating proceeds of crime.",Journal of Energy Storage,http://arxiv.org/abs/1810.09876v1,econ.GN
78,"Growth, Industrial Externality, Prospect Dynamics, and Well-being on
  Markets","Functions or 'functionings' enable to give a structure to any activity and
their combinations constitute the capabilities which characterize economic
assets such as work utility. The basic law of supply and demand naturally
emerges from that structure while integrating this utility within frames of
reference in which conditions of growth and associated inflation are identified
in the exchange mechanisms. Growth sustainability is built step by step taking
into account functional and organizational requirements which are followed
through a project up to a product delivery with different levels of
externalities. Entering the market through that structure leads to designing
basic equations of its dynamics and to finding canonical solutions, or
particular equilibria, after specifying the notion of maturity introduced in
order to refine the basic model. This approach allows to tackle behavioral
foundations of Prospect Theory through a generalization of its probability
weighting function for rationality analyses which apply to Western, Educated,
Industrialized, Rich, and Democratic societies as well as to the poorest ones.
The nature of reality and well-being appears then as closely related to the
relative satisfaction reached on the market, as it can be conceived by an
agent, according to business cycles; this reality being the result of the
complementary systems that govern human mind as structured by rational
psychologists. The final concepts of growth integrate and extend the maturity
part of the behavioral model into virtuous or erroneous sustainability.",Journal of Environmental Economics and Management,http://arxiv.org/abs/1812.09302v4,econ.GN
195,Model Selection in Utility-Maximizing Binary Prediction,"The maximum utility estimation proposed by Elliott and Lieli (2013) can be
viewed as cost-sensitive binary classification; thus, its in-sample overfitting
issue is similar to that of perceptron learning. A utility-maximizing
prediction rule (UMPR) is constructed to alleviate the in-sample overfitting of
the maximum utility estimation. We establish non-asymptotic upper bounds on the
difference between the maximal expected utility and the generalized expected
utility of the UMPR. Simulation results show that the UMPR with an appropriate
data-dependent penalty achieves larger generalized expected utility than common
estimators in the binary classification if the conditional probability of the
binary outcome is misspecified.",Journal of Environments ,http://arxiv.org/abs/1903.00716v3,econ.GN
511,Efficient and fair trading algorithms in market design environments,"We propose a new method to define trading algorithms in market design
environments. Dropping the traditional idea of clearing cycles in generated
graphs, we use parameterized linear equations to define trading algorithms. Our
method has two advantages. First, our method avoids discussing the details of
who trades with whom and how, which can be a difficult question in complex
environments. Second, by controlling parameter values in our equations, our
method is flexible and transparent to satisfy various fairness criteria. We
apply our method to several models and obtain new trading algorithms that are
efficient and fair.",Journal of Forecasting,http://arxiv.org/abs/2005.06878v3,q-fin.GN
1,Information Content of DSGE Forecasts,"This paper examines the question whether information is contained in
forecasts from DSGE models beyond that contained in lagged values, which are
extensively used in the models. Four sets of forecasts are examined. The
results are encouraging for DSGE forecasts of real GDP. The results suggest
that there is information in the DSGE forecasts not contained in forecasts
based only on lagged values and that there is no information in the
lagged-value forecasts not contained in the DSGE forecasts. The opposite is
true for forecasts of the GDP deflator.
  Keywords: DSGE forecasts, Lagged values
  JEL Classification Codes: E10, E17, C53",Journal of Forecasting,http://arxiv.org/abs/1808.02910v1,econ.GN
456,"Particulate Air Pollution, Birth Outcomes, and Infant Mortality:
  Evidence from Japan's Automobile Emission Control Law of 1992","This study investigates the impacts of the Automobile NOx Law of 1992 on
ambient air pollutants and fetal and infant health outcomes in Japan. Using
panel data taken from more than 1,500 monitoring stations between 1987 and
1997, we find that NOx and SO2 levels reduced by 87% and 52%, respectively in
regulated areas following the 1992 regulation. In addition, using a
municipal-level Vital Statistics panel dataset and adopting the regression
differences-in-differences method, we find that the enactment of the regulation
explained most of the improvements in the fetal death rate between 1991 and
1993. This study is the first to provide evidence on the positive impacts of
this large-scale automobile regulation policy on fetal health.",Journal of Forecasting,http://arxiv.org/abs/1905.04417v2,econ.EM
436,All-Pay Auctions as Models for Trade Wars and Military Annexation,"We explore an application of all-pay auctions to model trade wars and
territorial annexation. Specifically, in the model we consider the expected
resource, production, and aggressive (military/tariff) power are public
information, but actual resource levels are private knowledge. We consider the
resource transfer at the end of such a competition which deprives the weaker
country of some fraction of its original resources. In particular, we derive
the quasi-equilibria strategies for two country conflicts under different
scenarios. This work is relevant for the ongoing US-China trade war, and the
recent Russian capture of Crimea, as well as historical and future conflicts.",Journal of Forecasting ,http://arxiv.org/abs/2002.03492v1,stat.CO
324,Quantifying the Coherence of Development Policy Priorities,"Over the last 30 years, the concept of policy coherence for development has
received especial attention among academics, practitioners and international
organizations. However, its quantification and measurement remain elusive. To
address this challenge, we develop a theoretical and empirical framework to
measure the coherence of policy priorities for development. Our procedure takes
into account the country-specific constraints that governments face when trying
to reach specific development goals. Hence, we put forward a new definition of
policy coherence where context-specific efficient resource allocations are
employed as the baseline to construct an index. To demonstrate the usefulness
and validity of our index, we analyze the cases of Mexico, Korea and Estonia,
three developing countries that, arguably, joined the OECD with the aim of
coherently establishing policies that could enable a catch-up process. We find
that Korea shows significant signs of policy coherence, Estonia seems to be in
the process of achieving it, and Mexico has unequivocally failed. Furthermore,
our results highlight the limitations of assessing coherence in terms of naive
benchmark comparisons using development-indicator data. Altogether, our
framework sheds new light in a promising direction to develop bespoke analytic
tools to meet the 2030 agenda.",Journal of Fuzzy Extension & Applications ,http://arxiv.org/abs/1902.00430v1,econ.GN
479,Cancer Risk Messages: A Light Bulb Model,"The meaning of public messages such as ""One in x people gets cancer"" or ""One
in y people gets cancer by age z"" can be improved. One assumption commonly
invoked is that there is no other cause of death, a confusing assumption. We
develop a light bulb model to clarify cumulative risk and we use Markov chain
modeling, incorporating the assumption widely in place, to evaluate transition
probabilities. Age-progression in the cancer risk is then reported on
Australian data. Future modelling can elicit realistic assumptions.","Journal of Guidance, Control and Dynamics",http://arxiv.org/abs/1807.03040v2,math.OC
196,Finite Sample Inference for the Maximum Score Estimand,"We provide a finite sample inference method for the structural parameters of
a semiparametric binary response model under a conditional median restriction
originally studied by Manski (1975, 1985). Our inference method is valid for
any sample size and irrespective of whether the structural parameters are point
identified or partially identified, for example due to the lack of a
continuously distributed covariate with large support. Our inference approach
exploits distributional properties of observable outcomes conditional on the
observed sequence of exogenous variables. Moment inequalities conditional on
this size n sequence of exogenous covariates are constructed, and the test
statistic is a monotone function of violations of sample moment inequalities.
The critical value used for inference is provided by the appropriate quantile
of a known function of n independent Rademacher random variables. We
investigate power properties of the underlying test and provide simulation
studies to support the theoretical findings.",Journal of Health Economics,http://arxiv.org/abs/1903.01511v2,econ.GN
527,Market and Long Term Accounting Operational Performance,"Following the value relevance literature, this study verifies whether the
marketplace differentiates companies of high, medium, and low long-term
operational performance, measured by accounting information on profitability,
sales variation and indebtedness. The data comprises the Corporate Financial
Statements disclosed during the period from 1996 to 2009 and stock prices of
companies listed on the Sao Paulo Stock Exchange and Commodities and Futures
Exchange - BM&FBOVESPA. The final sample is composed of 142 non-financial
companies. Five year mobile windows were used, which resulted in ten five-year
periods. After checking each company indices, the accounting variables were
unified in an Index Performance Summary to synthesize the final performance for
each five-year period, which allowed segregation in operational performance
levels. Multiple regressions were performed using panel data techniques, fixed
effects model and dummies variables, and then hypothesis tests were made.
Regarding the explanatory power of each individual variable, the results show
that not all behaviors are according to the research hypothesis and that the
Brazilian stock market differentiates companies of high and low long-term
operational performance. This distinction is not fully perceived between
companies of high and medium operational performance.",Journal of Historical Political Economy ,http://arxiv.org/abs/1907.11719v1,econ.GN
217,Tax Mechanisms and Gradient Flows,"We demonstrate how a static optimal income taxation problem can be analyzed
using dynamical methods. Specifically, we show that the taxation problem is
intimately connected to the heat equation. Our first result is a new property
of the optimal tax which we call the fairness principle. The optimal tax at any
income is invariant under a family of properly adjusted Gaussian averages (the
heat kernel) of the optimal taxes at other incomes. That is, the optimal tax at
a given income is equal to the weighted by the heat kernels average of optimal
taxes at other incomes and income densities. Moreover, this averaging happens
at every scale tightly linked to each other providing a unified weighting
scheme at all income ranges. The fairness principle arises not due to equality
considerations but rather it represents an efficient way to smooth the burden
of taxes and generated revenues across incomes. Just as nature wants to
distribute heat evenly, the optimal way for a government to raise revenues is
to distribute the tax burden and raised revenues evenly among individuals. We
then construct a gradient flow of taxes -- a dynamic process changing the
existing tax system in the direction of the increase in tax revenues -- and
show that it takes the form of a heat equation. The fairness principle holds
also for the short-term asymptotics of the gradient flow, where the averaging
is done over the current taxes. The gradient flow we consider can be viewed as
a continuous process of a reform of the nonlinear income tax schedule and thus
unifies the variational approach to taxation and optimal taxation. We present
several other characteristics of the gradient flow focusing on its smoothing
properties.",Journal of Human Capital,http://arxiv.org/abs/1904.13276v1,econ.GN
36,"A Note on Gale, Kuhn, and Tucker's Reductions of Zero-Sum Games","Gale, Kuhn and Tucker (1950) introduced two ways to reduce a zero-sum game by
packaging some strategies with respect to a probability distribution on them.
In terms of value, they gave conditions for a desirable reduction. We show that
a probability distribution for a desirable reduction relies on optimal
strategies in the original game. Also, we correct an improper example given by
them to show that the reverse of a theorem does not hold.",Journal of Human Resources,http://arxiv.org/abs/1710.02326v1,econ.EM
538,Automation Impacts on China's Polarized Job Market,"When facing threats from automation, a worker residing in a large Chinese
city might not be as lucky as a worker in a large U.S. city, depending on the
type of large city in which one resides. Empirical studies found that large
U.S. cities exhibit resilience to automation impacts because of the increased
occupational and skill specialization. However, in this study, we observe
polarized responses in large Chinese cities to automation impacts. The
polarization might be attributed to the elaborate master planning of the
central government, through which cities are assigned with different industrial
goals to achieve globally optimal economic success and, thus, a fast-growing
economy. By dividing Chinese cities into two groups based on their
administrative levels and premium resources allocated by the central
government, we find that Chinese cities follow two distinct industrial
development trajectories, one trajectory owning government support leads to a
diversified industrial structure and, thus, a diversified job market, and the
other leads to specialty cities and, thus, a specialized job market. By
revisiting the automation impacts on a polarized job market, we observe a
Simpson's paradox through which a larger city of a diversified job market
results in greater resilience, whereas larger cities of specialized job markets
are more susceptible. These findings inform policy makers to deploy appropriate
policies to mitigate the polarized automation impacts.",Journal of Information Science ,http://arxiv.org/abs/1908.05518v1,econ.GN
579,The uniqueness of dynamic Groves mechanisms on restricted domains,"This paper examines necessary and sufficient conditions for the uniqueness of
dynamic Groves mechanisms when the domain of valuations is restricted. Our
approach is to appropriately define the total valuation function, which is the
expected discounted sum of each period's valuation function from the allocation
and thus a dynamic counterpart of the static valuation function, and then to
port the results for static Groves mechanisms to the dynamic setting.",Journal of Informetrics,http://arxiv.org/abs/2006.14190v1,cs.DL
298,A Cardinal Comparison of Experts,"In various situations, decision makers face experts that may provide
conflicting advice. This advice may be in the form of probabilistic forecasts
over critical future events. We consider a setting where the two forecasters
provide their advice repeatedly and ask whether the decision maker can learn to
compare and rank the two forecasters based on past performance. We take an
axiomatic approach and propose three natural axioms that a comparison test
should comply with. We propose a test that complies with our axioms. Perhaps,
not surprisingly, this test is closely related to the likelihood ratio of the
two forecasts over the realized sequence of events. More surprisingly, this
test is essentially unique. Furthermore, using results on the rate of
convergence of supermartingales, we show that whenever the two
experts\textquoteright{} advice are sufficiently distinct, the proposed test
will detect the informed expert in any desired degree of precision in some
fixed finite time.",Journal of Informetrics ,http://arxiv.org/abs/1908.10649v3,cs.DL
205,Conditions for the uniqueness of the Gately point for cooperative games,"We are studying the Gately point, an established solution concept for
cooperative games. We point out that there are superadditive games for which
the Gately point is not unique, i.e. in general the concept is rather
set-valued than an actual point. We derive conditions under which the Gately
point is guaranteed to be a unique imputation and provide a geometric
interpretation. The Gately point can be understood as the intersection of a
line defined by two points with the set of imputations. Our uniqueness
conditions guarantee that these two points do not coincide. We provide
demonstrative interpretations for negative propensities to disrupt. We briefly
show that our uniqueness conditions for the Gately point include quasibalanced
games and discuss the relation of the Gately point to the $\tau$-value in this
context. Finally, we point out relations to cost games and the ACA method and
end upon a few remarks on the implementation of the Gately point and an
upcoming software package for cooperative game theory.",Journal of Interdisciplinary Mathematics,http://arxiv.org/abs/1901.01485v1,econ.TH
42,"Does it Pay to Buy the Pot in the Canadian 6/49 Lotto? Implications for
  Lottery Design","Despite its unusual payout structure, the Canadian 6/49 Lotto is one of the
few government sponsored lotteries that has the potential for a favorable
strategy we call ""buying the pot."" By buying the pot we mean that a syndicate
buys each ticket in the lottery, ensuring that it holds a jackpot winner. We
assume that the other bettors independently buy small numbers of tickets. This
paper presents (1) a formula for the syndicate's expected return, (2)
conditions under which buying the pot produces a significant positive expected
return, and (3) the implications of these findings for lottery design.","Journal of International Financial Markets, Institutions & Money",http://arxiv.org/abs/1801.02959v1,econ.GN
13,"Justifying the Adoption and Relevance of Inflation Targeting Framework:
  A Time-Varying Evidence from Ghana","This paper scrutinizes the rationale for the adoption of inflation targeting
(IT) by Bank of Ghana in 2002. In this case, we determine the stability or
otherwise of the relationship between money supply and inflation in Ghana over
the period 1970M1-2016M3 using battery of econometric methods. The empirical
results show an unstable link between inflation and monetary growth in Ghana,
while the final state coefficient of inflation elasticity to money growth is
positive but statistically insignificant. We find that inflation elasticity to
monetary growth has continued to decline since the 1970s, showing a waning
impact of money growth on inflation in Ghana. Notably, there is also evidence
of negative inflation elasticity to monetary growth between 2001 and 2004,
lending support to the adoption of IT framework in Ghana in 2002. We emphasized
that the unprecedented 31-months of single-digit inflation (June 2010-December
2012), despite the observed inflationary shocks in 2010 and 2012, reinforces
the immense contribution of the IT framework in anchoring inflation
expectations, with better inflation outcomes and inflation variability in
Ghana. The paper therefore recommends the continuous pursuance and
strengthening of the IT framework in Ghana, as it embodies a more eclectic
approach to policy formulation and implementation.",Journal of Islamic Economics Banking and Finance,http://arxiv.org/abs/1805.11562v1,econ.TH
45,Characterizing Assumption of Rationality by Incomplete Information,"We characterize common assumption of rationality of 2-person games within an
incomplete information framework. We use the lexicographic model with
incomplete information and show that a belief hierarchy expresses common
assumption of rationality within a complete information framework if and only
if there is a belief hierarchy within the corresponding incomplete information
framework that expresses common full belief in caution, rationality, every good
choice is supported, and prior belief in the original utility functions.","Journal of Management and Training for Industries 
 ",http://arxiv.org/abs/1801.04714v1,econ.GN
364,Costly Verification in Collective Decisions,"We study how a principal should optimally choose between implementing a new
policy and maintaining the status quo when information relevant for the
decision is privately held by agents. Agents are strategic in revealing their
information; the principal cannot use monetary transfers to elicit this
information, but can verify an agent's claim at a cost. We characterize the
mechanism that maximizes the expected utility of the principal. This mechanism
can be implemented as a cardinal voting rule, in which agents can either cast a
baseline vote, indicating only whether they are in favor of the new policy, or
they make specific claims about their type. The principal gives more weight to
specific claims and verifies a claim whenever it is decisive.",Journal of Mathematical Economics,http://arxiv.org/abs/1910.13979v2,math.OC
7,Enforcing Regulation Under Illicit Adaptation,"Attempts to curb illegal activity by enforcing regulations gets complicated
when agents react to the new regulatory regime in unanticipated ways to
circumvent enforcement. We present a research strategy that uncovers such
reactions, and permits program evaluation net of such adaptive behaviors. Our
interventions were designed to reduce over-fishing of the critically endangered
Pacific hake by either (a) monitoring and penalizing vendors that sell illegal
fish or (b) discouraging consumers from purchasing using an information
campaign. Vendors attempt to circumvent the ban through hidden sales and other
means, which we track using mystery shoppers. Instituting random monitoring
visits are much more effective in reducing true hake availability by limiting
such cheating, compared to visits that occur on a predictable schedule.
Monitoring at higher frequency (designed to limit temporal displacement of
illegal sales) backfires, because targeted agents learn faster, and cheat more
effectively. Sophisticated policy design is therefore crucial for determining
the sustained, longer-term effects of enforcement. Data collected from
fishermen, vendors, and consumers allow us to document the upstream,
downstream, spillover, and equilibrium effects of enforcement on the entire
supply chain. The consumer information campaign generates two-thirds of the
gains compared to random monitoring, but is simpler for the government to
implement and almost as cost-effective.",Journal of Mathematical Economics,http://arxiv.org/abs/1808.09887v1,econ.TH
226,"District heating systems under high CO2 emission prices: the role of the
  pass-through from emission cost to electricity prices","Low CO2 prices have prompted discussion about political measures aimed at
increasing the cost of carbon dioxide emissions. These costs affect, inter
alia, integrated district heating system operators (DHSO), often owned by
municipalities with some political influence, that use a variety of (CO2 emis-
sion intense) heat generation technologies. We examine whether DHSOs have an
incentive to support measures that increase CO2 emission prices in the short
term. Therefore, we (i) develop a simplified analytical framework to analyse
optimal decisions of a district heating operator, and (ii) investigate the
market-wide effects of increasing emission prices, in particular the pass-
through from emission costs to electricity prices. Using a numerical model of
the common Austrian and German power system, we estimate a pass-through from
CO2 emission prices to power prices between 0.69 and 0.53 as of 2017, depending
on the absolute emission price level. We find the CO2 emission cost
pass-through to be sufficiently high so that low-emission district heating
systems operating at least moderately efficient generation units benefit from
rising CO2 emission prices in the short term.",Journal of Mathematical Economics,http://arxiv.org/abs/1810.02109v1,cs.GT
39,Resource Abundance and Life Expectancy,"This paper investigates the impacts of major natural resource discoveries
since 1960 on life expectancy in the nations that they were resource poor prior
to the discoveries. Previous literature explains the relation between nations
wealth and life expectancy, but it has been silent about the impacts of
resource discoveries on life expectancy. We attempt to fill this gap in this
study. An important advantage of this study is that as the previous researchers
argued resource discovery could be an exogenous variable. We use longitudinal
data from 1960 to 2014 and we apply three modern empirical methods including
Difference-in-Differences, Event studies, and Synthetic Control approach, to
investigate the main question of the research which is 'how resource
discoveries affect life expectancy?'. The findings show that resource
discoveries in Ecuador, Yemen, Oman, and Equatorial Guinea have positive and
significant impacts on life expectancy, but the effects for the European
countries are mostly negative.",Journal of Mathematical Economics ,http://arxiv.org/abs/1801.00369v1,econ.TH
255,"Identification and Estimation of a Partially Linear Regression Model
  using Network Data","I study a regression model in which one covariate is an unknown function of a
latent driver of link formation in a network. Rather than specify and fit a
parametric network formation model, I introduce a new method based on matching
pairs of agents with similar columns of the squared adjacency matrix, the ijth
entry of which contains the number of other agents linked to both agents i and
j. The intuition behind this approach is that for a large class of network
formation models the columns of the squared adjacency matrix characterize all
of the identifiable information about individual linking behavior. In this
paper, I describe the model, formalize this intuition, and provide consistent
estimators for the parameters of the regression model. Auerbach (2021)
considers inference and an application to network peer effects.","Journal of Open Innovation Technology Market and Complexity
",http://arxiv.org/abs/1903.09679v3,econ.GN
395,Observing Actions in Bayesian Games,"We study Bayesian coordination games where agents receive noisy private
information over the game's payoff structure, and over each others' actions. If
private information over actions is precise, we find that agents can coordinate
on multiple equilibria. If private information over actions is of low quality,
equilibrium uniqueness obtains like in a standard global games setting. The
current model, with its flexible information structure, can thus be used to
study phenomena such as bank-runs, currency crises, recessions, riots, and
revolutions, where agents rely on information over each others' actions.",Journal of Optimization Theory and Applications,http://arxiv.org/abs/1904.10744v1,econ.GN
439,VAT Compliance Incentives,"In this work I clarify VAT evasion incentives through a game theoretical
approach. Traditionally, evasion has been linked to the decreasing risk
aversion in higher revenues (Allingham and Sandmo (1972), Cowell (1985)
(1990)). I claim tax evasion to be a rational choice when compliance is
stochastically more expensive than evading, even in absence of controls and
sanctions. I create a framework able to measure the incentives for taxpayers to
comply. The incentives here are deductions of specific VAT documented expenses
from the income tax. The issue is very well known and deduction policies at
work in many countries. The aim is to compute the right parameters for each
precise class of taxpayers. VAT evasion is a collusive conduct between the two
counterparts of the transaction. I therefore first explore the convenience for
the two private counterparts to agree on the joint evasion and to form a
coalition. Crucial is that compliance incentives break the agreement among the
transaction participants' coalition about evading. The game solution leads to
boundaries for marginal tax rates or deduction percentages, depending on
parameters, able to create incentives to comply The stylized example presented
here for VAT policies, already in use in many countries, is an attempt to
establish a more general method for tax design, able to make compliance the
""dominant strategy"", satisfying the ""outside option"" constraint represented by
evasion, even in absence of audit and sanctions. The theoretical results
derived here can be easily applied to real data for precise tax design
engineering.",Journal of Planning Education and Research ,http://arxiv.org/abs/2002.07862v3,econ.GN
218,"Compactification of Extensive Game Structures and Backward Dominance
  Procedure","We study the relationship between invariant transformations on extensive game
structures and backward dominance procedure (BD), a generalization of the
classical backward induction introduced in Perea (2014). We show that
behavioral equivalence with unambiguous orderings of information sets, a
critical property that guarantees BD's applicability, can be characterized by
the classical Coalescing and a modified Interchange/Simultanizing in Battigalli
et al. (2020). We also give conditions on transformations that improve BD's
efficiency. In addition, we discuss the relationship between transformations
and Bonanno (2014)'s generalized backward induction.",Journal of Political Economy,http://arxiv.org/abs/1905.00355v3,econ.GN
219,Netflix Games: Local Public Goods with Capacity Constraints,"This paper considers incentives to provide goods that are partially
excludable along social links. Individuals face a capacity constraint in that,
conditional upon providing, they may nominate only a subset of neighbours as
co-beneficiaries. Our model has two typically incompatible ingredients: (i) a
graphical game (individuals decide how much of the good to provide), and (ii)
graph formation (individuals decide which subset of neighbours to nominate as
co-beneficiaries). For any capacity constraints and any graph, we show the
existence of specialised pure strategy Nash equilibria - those in which some
individuals (the Drivers, D) contribute while the remaining individuals (the
Passengers, P) free ride. The proof is constructive and corresponds to showing,
for a given capacity, the existence of a new kind of spanning bipartite
subgraph, a DP-subgraph, with partite sets D and P. We consider how the number
of Drivers in equilibrium changes as the capacity constraints are relaxed and
show a weak monotonicity result. Finally, we introduce dynamics and show that
only specialised equilibria are stable against individuals unilaterally
changing their provision level.",Journal of Political Economy,http://arxiv.org/abs/1905.01693v2,econ.GN
252,"Inference for First-Price Auctions with Guerre, Perrigne, and Vuong's
  Estimator","We consider inference on the probability density of valuations in the
first-price sealed-bid auctions model within the independent private value
paradigm. We show the asymptotic normality of the two-step nonparametric
estimator of Guerre, Perrigne, and Vuong (2000) (GPV), and propose an easily
implementable and consistent estimator of the asymptotic variance. We prove the
validity of the pointwise percentile bootstrap confidence intervals based on
the GPV estimator. Lastly, we use the intermediate Gaussian approximation
approach to construct bootstrap-based asymptotically valid uniform confidence
bands for the density of the valuations.","Journal of Political Economy
 ",http://arxiv.org/abs/1903.06401v1,econ.GN
35,"Rate-Optimal Estimation of the Intercept in a Semiparametric
  Sample-Selection Model","This paper presents a new estimator of the intercept of a linear regression
model in cases where the outcome varaible is observed subject to a selection
rule. The intercept is often in this context of inherent interest; for example,
in a program evaluation context, the difference between the intercepts in
outcome equations for participants and non-participants can be interpreted as
the difference in average outcomes of participants and their counterfactual
average outcomes if they had chosen not to participate. The new estimator can
under mild conditions exhibit a rate of convergence in probability equal to
$n^{-p/(2p+1)}$, where $p\ge 2$ is an integer that indexes the strength of
certain smoothness assumptions. This rate of convergence is shown in this
context to be the optimal rate of convergence for estimation of the intercept
parameter in terms of a minimax criterion. The new estimator, unlike other
proposals in the literature, is under mild conditions consistent and
asymptotically normal with a rate of convergence that is the same regardless of
the degree to which selection depends on unobservables in the outcome equation.
Simulation evidence and an empirical example are included.",Journal of Political Economy ,http://arxiv.org/abs/1710.01423v3,econ.EM
135,Time preference and information acquisition,"I consider the sequential implementation of a target information structure. I
characterize the set of decision time distributions induced by all signal
processes that satisfy a per-period learning capacity constraint. I find that
all decision time distributions have the same expectation, and the maximal and
minimal elements by mean-preserving spread order are deterministic distribution
and exponential distribution. The result implies that when time preference is
risk loving (e.g. standard or hyperbolic discounting), Poisson signal is
optimal since it induces the most risky exponential decision time distribution.
When time preference is risk neutral (e.g. constant delay cost), all signal
processes are equally optimal.",Journal of Productivity Analysis,http://arxiv.org/abs/1809.05120v2,econ.GN
95,The Prosumer Economy -- Being Like a Forest,"Planetary life support systems are collapsing due to climate change and the
biodiversity crisis. The root cause is the existing consumer economy, coupled
with profit maximisation based on ecological and social externalities. Trends
can be reversed, civilisation may be saved by transforming the profit
maximising consumer economy into an ecologically and socially just economy,
which we call the prosumer economy. Prosumer economy is a macro scale circular
economy with minimum negative or positive ecological and social impact, an
ecosystem of producers and prosumers, who have synergistic and circular
relationships with deepened circular supply chains, networks, where leakage of
wealth out of the system is minimised. In a prosumer economy there is no waste,
no lasting negative impacts on the ecology and no social exploitation. The
prosumer economy is like a lake or a forest, an economic ecosystem that is
productive and supportive of the planet. We are already planting this forest
through Good4Trust.org, started in Turkey. Good4Trust is a community platform
bringing together ecologically and socially just producers and prosumers.
Prosumers come together around a basic ethical tenet the golden rule and share
on the platform their good deeds. The relationship are already deepening and
circularity is forming to create a prosumer economy. The platforms software to
structure the economy is open source, and is available to be licenced to start
Good4Trust anywhere on the planet. Complexity theory tells us that if enough
agents in a given system adopt simple rules which they all follow, the system
may shift. The shift from a consumer economy to a prosumer economy has already
started, the future is either ecologically and socially just or bust.",Journal of Public Economics Plus ,http://arxiv.org/abs/1903.07615v1,econ.GN
308,"Methodological provisions for conducting empirical research of the
  availability and implementation of the consumers socially responsible
  intentions","Social responsibility of consumers is one of the main conditions for the
recoupment of enterprises expenses associated with the implementation of social
and ethical marketing tasks. Therefore, the enterprises, which plan to act on
terms of social and ethical marketing, should monitor the social responsibility
of consumers in the relevant markets. At the same time, special attention
should be paid to the analysis of factors that prevent consumers from
implementing their socially responsible intentions in the regions with a low
level of social activity of consumers. The purpose of the article is to develop
methodological guidelines that determine the tasks and directions of conducting
empirical studies aimed at assessing the gap between the socially responsible
intentions of consumers and the actual implementation of these intentions, as
well as to identify the causes of this gap. An empirical survey of the sampled
consumers in Kharkiv was carried out in terms of the proposed methodological
provisions. It revealed a rather high level of respondents' willingness to
support socially responsible enterprises and a rather low level of
implementation of these intentions due to the lack of consumers awareness. To
test the proposed methodological guidelines, an empirical study of the
consumers social responsibility was conducted in 2017 on a sample of students
and professors of the Semen Kuznets Kharkiv National University of Economics
(120 people). Questioning of the respondents was carried out using the Google
Forms. The finding allowed to make conclusion for existence of a high level of
respondents' willingness to support socially responsible and socially active
enterprises. However, the study also revealed the existence of a significant
gap between the intentions and actions of consumers, caused by the lack of
awareness.",Journal of Quantitative Analysis in Sports ,http://arxiv.org/abs/1901.00191v1,stat.AP
253,An Integrated Panel Data Approach to Modelling Economic Growth,"Empirical growth analysis has three major problems --- variable selection,
parameter heterogeneity and cross-sectional dependence --- which are addressed
independently from each other in most studies. The purpose of this study is to
propose an integrated framework that extends the conventional linear growth
regression model to allow for parameter heterogeneity and cross-sectional error
dependence, while simultaneously performing variable selection. We also derive
the asymptotic properties of the estimator under both low and high dimensions,
and further investigate the finite sample performance of the estimator through
Monte Carlo simulations. We apply the framework to a dataset of 89 countries
over the period from 1960 to 2014. Our results reveal some cross-country
patterns not found in previous studies (e.g., ""middle income trap hypothesis"",
""natural resources curse hypothesis"", ""religion works via belief, not
practice"", etc.).",Journal of Retailing and Consumer Services ,http://arxiv.org/abs/1903.07948v1,econ.GN
340,Kernel Estimation for Panel Data with Heterogeneous Dynamics,"This paper proposes nonparametric kernel-smoothing estimation for panel data
to examine the degree of heterogeneity across cross-sectional units. We first
estimate the sample mean, autocovariances, and autocorrelations for each unit
and then apply kernel smoothing to compute their density functions. The
dependence of the kernel estimator on bandwidth makes asymptotic bias of very
high order affect the required condition on the relative magnitudes of the
cross-sectional sample size (N) and the time-series length (T). In particular,
it makes the condition on N and T stronger and more complicated than those
typically observed in the long-panel literature without kernel smoothing. We
also consider a split-panel jackknife method to correct bias and construction
of confidence intervals. An empirical application and Monte Carlo simulations
illustrate our procedure in finite samples.",Journal of Risk and Financial Management ,http://arxiv.org/abs/1802.08825v4,econ.GN
542,Government Expenditure on Research Plans and their Diversity,"In this study, we consider research and development investment by the
government. Our study is motivated by the bias in the budget allocation owing
to the competitive funding system. In our model, each researcher presents
research plans and expenses, and the government selects a research plan in two
periods---before and after the government knows its favorite plan---and spends
funds on the adopted program in each period. We demonstrate that, in a subgame
perfect equilibrium, the government adopts equally as many active plans as
possible. In an equilibrium, the selected plans are distributed proportionally.
Thus, the investment in research projects is symmetric and unbiased. Our
results imply that equally widespread expenditure across all research fields is
better than the selection of and concentration in some specific fields.",Journal of Risk and Insurance,http://arxiv.org/abs/1908.08786v1,q-fin.RM
192,"Decentralization Estimators for Instrumental Variable Quantile
  Regression Models","The instrumental variable quantile regression (IVQR) model (Chernozhukov and
Hansen, 2005) is a popular tool for estimating causal quantile effects with
endogenous covariates. However, estimation is complicated by the non-smoothness
and non-convexity of the IVQR GMM objective function. This paper shows that the
IVQR estimation problem can be decomposed into a set of conventional quantile
regression sub-problems which are convex and can be solved efficiently. This
reformulation leads to new identification results and to fast, easy to
implement, and tuning-free estimators that do not require the availability of
high-level ""black box"" optimization routines.",Journal of Smart Economic Growth ,http://arxiv.org/abs/1812.10925v4,econ.GN
459,"Perceived Advantage in Perspective Application of Integrated Choice and
  Latent Variable Model to Capture Electric Vehicles Perceived Advantage from
  Consumers Perspective","Relative advantage, or the degree to which a new technology is perceived to
be better over the existing technology it supersedes, has a significant impact
on individuals decision of adopting to the new technology. This paper
investigates the impact of electric vehicles perceived advantage over the
conventional internal combustion engine vehicles, from consumers perspective,
on their decision to select electric vehicles. Data is obtained from a stated
preference survey from 1176 residents in New South Wales, Australia. The
collected data is used to estimate an integrated choice and latent variable
model of electric vehicle choice, which incorporates the perceived advantage of
electric vehicles in the form of latent variables in the utility function. The
design of the electric vehicle, impact on the environment, and safety are three
identified advantages from consumers point of view. The model is used to
simulate the effectiveness of various policies to promote electric vehicles on
different cohorts. Rebate on the purchase price is found to be the most
effective strategy to promote electric vehicles adoption.",Journal of Statistical Mechanics,http://arxiv.org/abs/1905.11606v2,econ.GN
541,"Implementing result-based agri-environmental payments by means of
  modelling","From a theoretical point of view, result-based agri-environmental payments
are clearly preferable to action-based payments. However, they suffer from two
major practical disadvantages: costs of measuring the results and payment
uncertainty for the participating farmers. In this paper, we propose an
alternative design to overcome these two disadvantages by means of modelling
(instead of measuring) the results. We describe the concept of model-informed
result-based agri-environmental payments (MIRBAP), including a hypothetical
example of payments for the protection and enhancement of soil functions. We
offer a comprehensive discussion of the relative advantages and disadvantages
of MIRBAP, showing that it not only unites most of the advantages of
result-based and action-based schemes, but also adds two new advantages: the
potential to address trade-offs among multiple policy objectives and management
for long-term environmental effects. We argue that MIRBAP would be a valuable
addition to the agri-environmental policy toolbox and a reflection of recent
advancements in agri-environmental modelling.",Journal of Statistical Mechanics,http://arxiv.org/abs/1908.08219v2,q-fin.ST
180,"Column Generation Algorithms for Nonparametric Analysis of Random
  Utility Models","Kitamura and Stoye (2014) develop a nonparametric test for linear inequality
constraints, when these are are represented as vertices of a polyhedron instead
of its faces. They implement this test for an application to nonparametric
tests of Random Utility Models. As they note in their paper, testing such
models is computationally challenging. In this paper, we develop and implement
more efficient algorithms, based on column generation, to carry out the test.
These improved algorithms allow us to tackle larger datasets.",Journal of Statistical Mechanics,http://arxiv.org/abs/1812.01400v1,physics.soc-ph
488,Quantile-Regression Inference With Adaptive Control of Size,"Regression quantiles have asymptotic variances that depend on the conditional
densities of the response variable given regressors. This paper develops a new
estimate of the asymptotic variance of regression quantiles that leads any
resulting Wald-type test or confidence region to behave as well in large
samples as its infeasible counterpart in which the true conditional response
densities are embedded. We give explicit guidance on implementing the new
variance estimator to control adaptively the size of any resulting Wald-type
test. Monte Carlo evidence indicates the potential of our approach to deliver
powerful tests of heterogeneity of quantile treatment effects in covariates
with good size performance over different quantile levels, data-generating
processes and sample sizes. We also include an empirical example. Supplementary
material is available online.",Journal of Statistical Software ,http://arxiv.org/abs/1807.06977v2,stat.CO
497,"Estimating grouped data models with a binary dependent variable and
  fixed effects: What are the issues","This article deals with asimple issue: if we have grouped data with a binary
dependent variable and want to include fixed effects (group specific
intercepts) in the specification, is Ordinary Least Squares (OLS) in any way
superior to a (conditional) logit form? In particular, what are the
consequences of using OLS instead of a fixed effects logit model with respect
to the latter dropping all units which show no variability in the dependent
variable while the former allows for estimation using all units. First, we show
that the discussion of fthe incidental parameters problem is based on an
assumption about the kinds of data being studied; for what appears to be the
common use of fixed effect models in political science the incidental
parameters issue is illusory. Turning to linear models, we see that OLS yields
a linear combination of the estimates for the units with and without variation
in the dependent variable, and so the coefficient estimates must be carefully
interpreted. The article then compares two methods of estimating logit models
with fixed effects, and shows that the Chamberlain conditional logit is as good
as or better than a logit analysis which simply includes group specific
intercepts (even though the conditional logit technique was designed to deal
with the incidental parameters problem!). Related to this, the article
discusses the estimation of marginal effects using both OLS and logit. While it
appears that a form of logit with fixed effects can be used to estimate
marginal effects, this method can be improved by starting with conditional
logit and then using the those parameter estimates to constrain the logit with
fixed effects model. This method produces estimates of sample average marginal
effects that are at least as good as OLS, and much better when group size is
small or the number of groups is large. .",Journal of Statistical Software ,http://arxiv.org/abs/1809.06505v1,stat.CO
498,Proxy Controls and Panel Data,"We present a flexible approach to estimation, and inference in nonparametric,
non-separable models using `proxy controls': covariates that do not satisfy a
standard `unconfoundedness' assumption but are informative proxies for
variables that do. Our analysis applies to cross-sectional settings but is
particularly well-suited to panel models. Our identification results motivate a
simple and `well-posed' nonparametric estimator. We derive convergence rates
for the estimator and construct uniform confidence bands with asymptotically
correct size. In panel settings, our methods provide a novel approach to the
difficult problem of identification with non-separable, general heterogeneity
and fixed T. In panels, observations from different periods serve as proxies
for unobserved heterogeneity and our key identifying assumptions follow from
restrictions on the serial dependence structure. We apply our methodology to
two empirical settings. We estimate causal effects of grade retention on
cognitive performance using cross-sectional variation and we estimate consumer
demand counterfactuals using panel data.",Journal of Statistical Software ,http://arxiv.org/abs/1810.00283v7,stat.CO
251,"A Varying Coefficient Model for Assessing the Returns to Growth to
  Account for Poverty and Inequality","Various papers demonstrate the importance of inequality, poverty and the size
of the middle class for economic growth. When explaining why these measures of
the income distribution are added to the growth regression, it is often
mentioned that poor people behave different which may translate to the economy
as a whole. However, simply adding explanatory variables does not reflect this
behavior. By a varying coefficient model we show that the returns to growth
differ a lot depending on poverty and inequality. Furthermore, we investigate
how these returns differ for the poorer and for the richer part of the
societies. We argue that the differences in the coefficients impede, on the one
hand, that the means coefficients are informative, and, on the other hand,
challenge the credibility of the economic interpretation. In short, we show
that, when estimating mean coefficients without accounting for poverty and
inequality, the estimation is likely to suffer from a serious endogeneity bias.",Journal of the American Statistical Association,http://arxiv.org/abs/1903.02390v1,stat.ME
268,"Heterogeneous Employment Effects of Job Search Programmes: A Machine
  Learning Approach","We systematically investigate the effect heterogeneity of job search
programmes for unemployed workers. To investigate possibly heterogeneous
employment effects, we combine non-experimental causal empirical models with
Lasso-type estimators. The empirical analyses are based on rich administrative
data from Swiss social security records. We find considerable heterogeneities
only during the first six months after the start of training. Consistent with
previous results of the literature, unemployed persons with fewer employment
opportunities profit more from participating in these programmes. Furthermore,
we also document heterogeneous employment effects by residence status. Finally,
we show the potential of easy-to-implement programme participation rules for
improving average employment effects of these active labour market programmes.","Journal of the American Statistical Association
",http://arxiv.org/abs/1709.10279v2,econ.EM
71,Slow persuasion,"What are the value and form of optimal persuasion when information can be
generated only slowly? We study this question in a dynamic model in which a
'sender' provides public information over time subject to a graduality
constraint, and a decision-maker takes an action in each period. Using a novel
'viscosity' dynamic programming principle, we characterise the sender's
equilibrium value function and information provision. We show that the
graduality constraint inhibits information provision relative to unconstrained
persuasion. The gap can be substantial, but closes as the constraint slackens.
Contrary to unconstrained persuasion, less-than-full information may be
provided even if players have aligned preferences but different prior beliefs.","Journal of the American Statistical Association 
 ",http://arxiv.org/abs/1903.09055v5,econ.EM
297,Improving Information from Manipulable Data,"Data-based decisionmaking must account for the manipulation of data by agents
who are aware of how decisions are being made and want to affect their
allocations. We study a framework in which, due to such manipulation, data
becomes less informative when decisions depend more strongly on data. We
formalize why and how a decisionmaker should commit to underutilizing data.
Doing so attenuates information loss and thereby improves allocation accuracy.",Journal of the European Economic Association ,http://arxiv.org/abs/1908.10330v5,econ.TH
385,"Climate Change and Agriculture: Subsistence Farmers' Response to Extreme
  Heat","This paper examines how subsistence farmers respond to extreme heat. Using
micro-data from Peruvian households, we find that high temperatures reduce
agricultural productivity, increase area planted, and change crop mix. These
findings are consistent with farmers using input adjustments as a short-term
mechanism to attenuate the effect of extreme heat on output. This response
seems to complement other coping strategies, such as selling livestock, but
exacerbates the drop in yields, a standard measure of agricultural
productivity. Using our estimates, we show that accounting for land adjustments
is important to quantify damages associated with climate change.",Journal of the European Economic Association ,http://arxiv.org/abs/1902.09204v2,econ.TH
375,"How do governments determine policy priorities? Studying development
  strategies through spillover networks","Determining policy priorities is a challenging task for any government
because there may be, for example, a multiplicity of objectives to be
simultaneously attained, a multidimensional policy space to be explored,
inefficiencies in the implementation of public policies, interdependencies
between policy issues, etc. Altogether, these factor s generate a complex
landscape that governments need to navigate in order to reach their goals. To
address this problem, we develop a framework to model the evolution of
development indicators as a political economy game on a network. Our approach
accounts for the --recently documented-- network of spillovers between policy
issues, as well as the well-known political economy problem arising from budget
assignment. This allows us to infer not only policy priorities, but also the
effective use of resources in each policy issue. Using development indicators
data from more than 100 countries over 11 years, we show that the
country-specific context is a central determinant of the effectiveness of
policy priorities. In addition, our model explains well-known aggregate facts
about the relationship between corruption and development. Finally, this
framework provides a new analytic tool to generate bespoke advice on
development strategies.",Journal of the Physical Society of Japan,http://arxiv.org/abs/1902.00432v1,q-bio.PE
454,"The mitigating role of regulation on the concentric patterns of
  broadband diffusion. The case of Finland","This article analyzes the role of Finnish regulation in achieving the
broadband penetration goals defined by the National Regulatory Authority. It is
well known that in the absence of regulatory mitigation the population density
has a positive effect on broadband diffusion. Hence, we measure the effect of
the population density on the determinants of broadband diffusion throughout
the postal codes of Finland via Geographically Weighted Regression. We suggest
that the main determinants of broadband diffusion and the population density
follow a spatial pattern that is either concentric with a weak/medium/strong
strength or non-concentric convex/concave. Based on 10 patterns, we argue that
the Finnish spectrum policy encouraged Mobile Network Operators to satisfy
ambitious Universal Service Obligations without the need for a Universal
Service Fund. Spectrum auctions facilitated infrastructure-based competition
via equitable spectrum allocation and coverage obligation delivery via low-fee
licenses. However, state subsidies for fiber deployment did not attract
investment from nationwide operators due to mobile preference. These subsidies
encouraged demand-driven investment, leading to the emergence of fiber consumer
cooperatives. To explain this emergence, we show that when population density
decreases, the level of mobile service quality decreases and community
commitment increases. Hence, we recommend regulators implementing market-driven
strategies for 5G to stimulate local investment. For example, by allocating the
3.5 GHz and higher bands partly through local light licensing.",Journal of the Royal Statistical Society,http://arxiv.org/abs/1905.03002v3,stat.ML
524,Reputation Building under Observational Learning,"I study a social learning model in which the object to learn is a strategic
player's endogenous actions rather than an exogenous state. A patient seller
faces a sequence of buyers and decides whether to build a reputation for
supplying high quality products. Each buyer does not have access to the
seller's complete records, but can observe all previous buyers' actions, and
some informative private signal about the seller's actions. I examine how the
buyers' private signals affect the speed of social learning and the seller's
incentives to establish reputations. When each buyer privately observes a
bounded subset of the seller's past actions, the speed of learning is strictly
positive but can vanish to zero as the seller becomes patient. As a result,
reputation building can lead to low payoff for the patient seller and low
social welfare. When each buyer observes an unboundedly informative private
signal about the seller's current-period action, the speed of learning is
uniformly bounded from below and a patient seller can secure high returns from
building reputations. My results shed light on the effectiveness of various
policies in accelerating social learning and encouraging sellers to establish
good reputations.",Journal of Theoretical and Applied Information Technology,http://arxiv.org/abs/2006.08068v6,econ.GN
464,"Growing green: the role of path dependency and structural jumps in the
  green economy expansion","Existing research argues that countries increase their production basket by
adding products which require similar capabilities to those they already
produce, a process referred to as path dependency. Green economic growth is a
global movement that seeks to achieve economic expansion while at the same time
mitigating environmental risks. We postulate that countries engaging in green
economic growth are motivated to invest strategically to develop new
capabilities that will help them transition to a green economy. As a result,
they could potentially increase their production baskets not only by a path
dependent process but also by the non path dependent process we term, high
investment structural jumps. The main objective of this research is to
determine whether countries increase their green production basket mainly by a
process of path dependency, or alternatively, by a process of structural jumps.
We analyze data from 65 countries and over a period from years 2007 to 2017. We
focus on China as our main case study. The results of this research show that
countries not only increase their green production baskets based on their
available capabilities, following path dependency, but also expand to products
that path dependency does not predict by investing in innovating and developing
new environmental related technologies.",Journal of Transport Geography ,http://arxiv.org/abs/1906.05269v2,cs.CY
424,"Stochastic model specification in Markov switching vector error
  correction models","This paper proposes a hierarchical modeling approach to perform stochastic
model specification in Markov switching vector error correction models. We
assume that a common distribution gives rise to the regime-specific regression
coefficients. The mean as well as the variances of this distribution are
treated as fully stochastic and suitable shrinkage priors are used. These
shrinkage priors enable to assess which coefficients differ across regimes in a
flexible manner. In the case of similar coefficients, our model pushes the
respective regions of the parameter space towards the common distribution. This
allows for selecting a parsimonious model while still maintaining sufficient
flexibility to control for sudden shifts in the parameters, if necessary. We
apply our modeling approach to real-time Euro area data and assume transition
probabilities between expansionary and recessionary regimes to be driven by the
cointegration errors. The results suggest that the regime allocation is
governed by a subset of short-run adjustment coefficients and regime-specific
variance-covariance matrices. These findings are complemented by an
out-of-sample forecast exercise, illustrating the advantages of the model for
predicting Euro area inflation in real time.","Journal of Vibration Testing and System Dynamics
 ",http://arxiv.org/abs/1807.00529v2,q-fin.ST
88,"Elusive Longer-Run Impacts of Head Start: Replications Within and Across
  Cohorts","Using an additional decade of CNLSY data, this study replicated and extended
Deming's (2009) evaluation of Head Start's life-cycle skill formation impacts
in three ways. Extending the measurement interval for Deming's adulthood
outcomes, we found no statistically significant impacts on earnings and mixed
evidence of impacts on other adult outcomes. Applying Deming's sibling
comparison framework to more recent birth cohorts born to CNLSY mothers
revealed mostly negative Head Start impacts. Combining all cohorts shows
generally null impacts on school-age and early adulthood outcomes.",Journal of World Trade ,http://arxiv.org/abs/1903.01954v4,econ.GN
153,"Economic Complexity: ""Buttarla in caciara"" vs a constructive approach","This note is a contribution to the debate about the optimal algorithm for
Economic Complexity that recently appeared on ArXiv [1, 2] . The authors of [2]
eventually agree that the ECI+ algorithm [1] consists just in a renaming of the
Fitness algorithm we introduced in 2012, as we explicitly showed in [3].
However, they omit any comment on the fact that their extensive numerical tests
claimed to demonstrate that the same algorithm works well if they name it ECI+,
but not if its name is Fitness. They should realize that this eliminates any
credibility to their numerical methods and therefore also to their new
analysis, in which they consider many algorithms [2]. Since by their own
admission the best algorithm is the Fitness one, their new claim became that
the search for the best algorithm is pointless and all algorithms are alike.
This is exactly the opposite of what they claimed a few days ago and it does
not deserve much comments. After these clarifications we also present a
constructive analysis of the status of Economic Complexity, its algorithms, its
successes and its perspectives. For us the discussion closes here, we will not
reply to further comments.",Journalism,http://arxiv.org/abs/1709.05272v1,econ.GN
83,Thought Viruses and Asset Prices,"We use insights from epidemiology, namely the SIR model, to study how agents
infect each other with ""investment ideas."" Once an investment idea ""goes
viral,"" equilibrium prices exhibit the typical ""fever peak,"" which is
characteristic for speculative excesses. Using our model, we identify a time
line of symptoms that indicate whether a boom is in its early or later stages.
Regarding the market's top, we find that prices start to decline while the
number of infected agents, who buy the asset, is still rising. Moreover, the
presence of fully rational agents (i) accelerates booms (ii) lowers peak prices
and (iii) produces broad, drawn-out, market tops.",Korean Economic Review,http://arxiv.org/abs/1812.11417v1,econ.TH
131,Repeated Coordination with Private Learning,"We study a repeated game with payoff externalities and observable actions
where two players receive information over time about an underlying
payoff-relevant state, and strategically coordinate their actions. Players
learn about the true state from private signals, as well as the actions of
others. They commonly learn the true state (Cripps et al., 2008), but do not
coordinate in every equilibrium. We show that there exist stable equilibria in
which players can overcome unfavorable signal realizations and eventually
coordinate on the correct action, for any discount factor. For high discount
factors, we show that in addition players can also achieve efficient payoffs.",Labour Economics,http://arxiv.org/abs/1809.00051v1,econ.GN
409,"Asymptotic Refinements of a Misspecification-Robust Bootstrap for
  Generalized Method of Moments Estimators","I propose a nonparametric iid bootstrap that achieves asymptotic refinements
for t tests and confidence intervals based on GMM estimators even when the
model is misspecified. In addition, my bootstrap does not require recentering
the moment function, which has been considered as critical for GMM. Regardless
of model misspecification, the proposed bootstrap achieves the same sharp
magnitude of refinements as the conventional bootstrap methods which establish
asymptotic refinements by recentering in the absence of misspecification. The
key idea is to link the misspecified bootstrap moment condition to the large
sample theory of GMM under misspecification of Hall and Inoue (2003). Two
examples are provided: Combining data sets and invalid instrumental variables.",Labour Economics,http://arxiv.org/abs/1806.01450v1,econ.EM
502,Slot-specific Priorities with Capacity Transfers,"In many real-world matching applications, there are restrictions for
institutions either on priorities of their slots or on the transferability of
unfilled slots over others (or both). Motivated by the need in such real-life
matching problems, this paper formulates a family of practical choice rules,
slot-specific priorities with capacity transfers (SSPwCT). These practical
rules invoke both slot-specific priorities structure and transferability of
vacant slots. We show that the cumulative offer mechanism (COM) is stable,
strategy-proof and respects improvements with regards to SSPwCT choice rules.
Transferring the capacity of one more unfilled slot, while all else is
constant, leads to strategy-proof Pareto improvement of the COM. Following
Kominer's (2020) formulation, we also provide comparative static results for
expansion of branch capacity and addition of new contracts in the SSPwCT
framework. Our results have implications for resource allocation problems with
diversity considerations.",Labour Economics,http://arxiv.org/abs/2004.13265v2,econ.EM
559,"Probabilistic Forecasting in Day-Ahead Electricity Markets: Simulating
  Peak and Off-Peak Prices","In this paper we include dependency structures for electricity price
forecasting and forecasting evaluation. We work with off-peak and peak time
series from the German-Austrian day-ahead price, hence we analyze bivariate
data. We first estimate the mean of the two time series, and then in a second
step we estimate the residuals. The mean equation is estimated by OLS and
elastic net and the residuals are estimated by maximum likelihood. Our
contribution is to include a bivariate jump component on a mean reverting jump
diffusion model in the residuals. The models' forecasts are evaluated using
four different criteria, including the energy score to measure whether the
correlation structure between the time series is properly included or not. In
the results it is observed that the models with bivariate jumps provide better
results with the energy score, which means that it is important to consider
this structure in order to properly forecast correlated time series.",Machine Learning with Applications,http://arxiv.org/abs/1810.08418v2,cs.LG
352,Constrained Pseudo-market Equilibrium,"We propose a pseudo-market solution to resource allocation problems subject
to constraints. Our treatment of constraints is general: including
bihierarchical constraints due to considerations of diversity in school choice,
or scheduling in course allocation; and other forms of constraints needed to
model, for example, the market for roommates, and combinatorial assignment
problems. Constraints give rise to pecuniary externalities, which are
internalized via prices. Agents pay to the extent that their purchases affect
the value of relevant constraints at equilibrium prices. The result is a
constrained efficient market equilibrium outcome. The outcome is fair whenever
the constraints do not single out individual agents. Our result can be extended
to economies with endowments, and address participation constraints.",Management Digitaler Plattformen,http://arxiv.org/abs/1909.05986v4,econ.GN
21,"Influence of High-Speed Railway System on Inter-city Travel Behavior in
  Vietnam","To analyze the influence of introducing the High-Speed Railway (HSR) system
on business and non-business travel behavior, this study develops an integrated
inter-city travel demand model to represent trip generations, destination
choice, and travel mode choice behavior. The accessibility calculated from the
RP/SP (Revealed Preference/Stated Preference) combined nested logit model of
destination and mode choices is used as an explanatory variable in the trip
frequency models. One of the important findings is that additional travel would
be induced by introducing HSR. Our simulation analyses also reveal that HSR and
conventional airlines will be the main modes for middle distances and long
distances, respectively. The development of zones may highly influence the
destination choices for business purposes, while prices of HSR and Low-Cost
Carriers affect choices for non-business purposes. Finally, the research
reveals that people on non-business trips are more sensitive to changes in
travel time, travel cost and regional attributes than people on business trips.",Management Science,http://arxiv.org/abs/1812.04184v1,econ.GN
168,A Flexible Design for Funding Public Goods,"We propose a design for philanthropic or publicly-funded seeding to allow
(near) optimal provision of a decentralized, self-organizing ecosystem of
public goods. The concept extends ideas from Quadratic Voting to a funding
mechanism for endogenous community formation. Individuals make public goods
contributions to projects of value to them. The amount received by the project
is (proportional to) the square of the sum of the square roots of contributions
received. Under the ""standard model"" this yields first best public goods
provision. Variations can limit the cost, help protect against collusion and
aid coordination. We discuss applications to campaign finance, open source
software ecosystems, news media finance and urban public projects. More
broadly, we offer a resolution to the classic liberal-communitarian debate in
political philosophy by providing neutral and non-authoritarian rules that
nonetheless support collective organization.",Management Science,http://arxiv.org/abs/1809.06421v2,econ.GN
157,"Explicit Solutions for Optimal Resource Extraction Problems under Regime
  Switching Lévy Models","This paper studies the problem of optimally extracting nonrenewable natural
resources. Taking into account the fact that the market values of the main
natural resources i.e. oil, natural gas, copper,..., etc, fluctuate randomly
following global and seasonal macroeconomic parameters, the prices of natural
resources are modeled using Markov switching L\'evy processes. We formulate
this optimal extraction problem as an infinite-time horizon optimal control
problem. We derive closed-form solutions for the value function as well as the
optimal extraction policy. Numerical examples are presented to illustrate these
results.",Management Science ,http://arxiv.org/abs/1806.06105v1,econ.TH
233,"Critical review of models, containing cultural levels beyond the
  organizational one","The current article traces back the scientific interest to cultural levels
across the organization at the University of National and World Economy, and
especially in the series of Economic Alternatives - an official scientific
magazine, issued by this Institution. Further, a wider and critical review of
international achievements in this field is performed, revealing diverse
analysis perspectives with respect to cultural levels. Also, a useful model of
exploring and teaching the cultural levels beyond the organization is proposed.
  Keywords: globalization, national culture, organization culture, cultural
levels, cultural economics. JEL: M14, Z10.",Management Science ,http://arxiv.org/abs/1810.03605v1,econ.TH
43,"Solving Dynamic Discrete Choice Models: Integrated or Expected Value
  Function?","Dynamic Discrete Choice Models (DDCMs) are important in the structural
estimation literature. Since the structural errors are practically always
continuous and unbounded in nature, researchers often use the expected value
function. The idea to solve for the expected value function made solution more
practical and estimation feasible. However, as we show in this paper, the
expected value function is impractical compared to an alternative: the
integrated (ex ante) value function. We provide brief descriptions of the
inefficacy of the former, and benchmarks on actual problems with varying
cardinality of the state space and number of decisions. Though the two
approaches solve the same problem in theory, the benchmarks support the claim
that the integrated value function is preferred in practice.",Marketing and Management of Innovations,http://arxiv.org/abs/1801.03978v1,econ.GN
103,"A Nonparametric Approach to Measure the Heterogeneous Spatial
  Association: Under Spatial Temporal Data","Spatial association and heterogeneity are two critical areas in the research
about spatial analysis, geography, statistics and so on. Though large amounts
of outstanding methods has been proposed and studied, there are few of them
tend to study spatial association under heterogeneous environment.
Additionally, most of the traditional methods are based on distance statistic
and spatial weighted matrix. However, in some abstract spatial situations,
distance statistic can not be applied since we can not even observe the
geographical locations directly. Meanwhile, under these circumstances, due to
invisibility of spatial positions, designing of weight matrix can not
absolutely avoid subjectivity. In this paper, a new entropy-based method, which
is data-driven and distribution-free, has been proposed to help us investigate
spatial association while fully taking the fact that heterogeneity widely
exist. Specifically, this method is not bounded with distance statistic or
weight matrix. Asymmetrical dependence is adopted to reflect the heterogeneity
in spatial association for each individual and the whole discussion in this
paper is performed on spatio-temporal data with only assuming stationary
m-dependent over time.",Mathematical Finance ,http://arxiv.org/abs/1803.02334v2,econ.TH
6,"Economics of carbon-dioxide abatement under an exogenous constraint on
  cumulative emissions","The fossil-fuel induced contribution to further warming over the 21st century
will be determined largely by integrated CO2 emissions over time rather than
the precise timing of the emissions, with a relation of near-proportionality
between global warming and cumulative CO2 emissions. This paper examines
optimal abatement pathways under an exogenous constraint on cumulative
emissions. Least cost abatement pathways have carbon tax rising at the
risk-free interest rate, but if endogenous learning or climate damage costs are
included in the analysis, the carbon tax grows more slowly. The inclusion of
damage costs in the optimization leads to a higher initial carbon tax, whereas
the effect of learning depends on whether it appears as an additive or
multiplicative contribution to the marginal cost curve. Multiplicative models
are common in the literature and lead to delayed abatement and a smaller
initial tax. The required initial carbon tax increases with the cumulative
abatement goal and is higher for lower interest rates. Delaying the start of
abatement is costly owing to the increasing marginal abatement cost. Lower
interest rates lead to higher relative costs of delaying abatement because
these induce higher abatement rates early on. The fraction of business-as-usual
emissions (BAU) avoided in optimal pathways increases for low interest rates
and rapid growth of the abatement cost curve, which allows a lower threshold
global warming goal to become attainable without overshoot in temperature. Each
year of delay in starting abatement raises this threshold by an increasing
amount, because the abatement rate increases exponentially with time.",Mathematical Social Sciences,http://arxiv.org/abs/1808.08717v2,econ.TH
15,A Physical Review on Currency,"A theoretical self-sustainable economic model is established based on the
fundamental factors of production, consumption, reservation and reinvestment,
where currency is set as a unconditional credit symbol serving as transaction
equivalent and stock means. Principle properties of currency are explored in
this ideal economic system. Physical analysis reveals some facts that were not
addressed by traditional monetary theory, and several basic principles of ideal
currency are concluded: 1. The saving-replacement is a more primary function of
currency than the transaction equivalents; 2. The ideal efficiency of currency
corresponds to the least practical value; 3. The contradiction between constant
face value of currency and depreciable goods leads to intrinsic inflation.",Mathematical Social Sciences,http://arxiv.org/abs/1805.12102v1,econ.TH
349,Estimating Treatment Effects in Mover Designs,"Researchers increasingly leverage movement across multiple treatments to
estimate causal effects. While these ""mover regressions"" are often motivated by
a linear constant-effects model, it is not clear what they capture under weaker
quasi-experimental assumptions. I show that binary treatment mover regressions
recover a convex average of four difference-in-difference comparisons and are
thus causally interpretable under a standard parallel trends assumption.
Estimates from multiple-treatment models, however, need not be causal without
stronger restrictions on the heterogeneity of treatment effects and
time-varying shocks. I propose a class of two-step estimators to isolate and
combine the large set of difference-in-difference quasi-experiments generated
by a mover design, identifying mover average treatment effects under
conditional-on-covariate parallel trends and effect homogeneity restrictions. I
characterize the efficient estimators in this class and derive specification
tests based on the model's overidentifying restrictions. Future drafts will
apply the theory to the Finkelstein et al. (2016) movers design, analyzing the
causal effects of geography on healthcare utilization.","Mathematical Social Sciences
",http://arxiv.org/abs/1804.06721v1,physics.soc-ph
14,How do public research labs use funding for research? A case study,"This paper discusses how public research organizations consume funding for
research, applying a new approach based on economic metabolism of research
labs, in a broad analogy with biology. This approach is applied to a case study
in Europe represented by one of the biggest European public research
organizations, the National Research council of Italy. Results suggest that
funding for research (state subsidy and public contracts) of this public
research organization is mainly consumed for the cost of personnel. In
addition, the analysis shows a disproportionate growth of the cost of personnel
in public research labs in comparison with total revenue from government. In
the presence of shrinking public research lab budgets, this organizational
behavior generates inefficiencies and stress. R&D management and public policy
implications are suggested for improving economic performance of public
research organizations in turbulent markets.",Mathematical Social Sciences ,http://arxiv.org/abs/1805.11932v1,econ.TH
164,"Nash equilibrium of partially asymmetric three-players zero-sum game
  with two strategic variables","We consider a partially asymmetric three-players zero-sum game with two
strategic variables. Two players (A and B) have the same payoff functions, and
Player C does not. Two strategic variables are $t_i$'s and $s_i$'s for $i=A, B,
C$. Mainly we will show the following results.
  1. The equilibrium when all players choose $t_i$'s is equivalent to the
equilibrium when Players A and B choose $t_i$'s and Player C chooses $s_C$ as
their strategic variables. 2. The equilibrium when all players choose $s_i$'s
is equivalent to the equilibrium when Players A and B choose $s_i$'s and Player
C chooses $t_C$ as their strategic variables.
  The equilibrium when all players choose $t_i$'s and the equilibrium when all
players choose $s_i$'s are not equivalent although they are equivalent in a
symmetric game in which all players have the same payoff functions.",Mathematics,http://arxiv.org/abs/1809.02465v1,econ.GN
177,"Equilibrium Restrictions and Approximate Models -- With an application
  to Pricing Macroeconomic Risk","We propose a method that reconciles two popular approaches to structural
estimation and inference: Using a complete - yet approximate model versus
imposing a set of credible behavioral conditions. This is done by distorting
the approximate model to satisfy these conditions. We provide the asymptotic
theory and Monte Carlo evidence, and illustrate that counterfactual experiments
are possible. We apply the methodology to the model of long run risks in
aggregate consumption (Bansal and Yaron, 2004), where the complete model is
generated using the Campbell and Shiller (1988) approximation. Using US data,
we investigate the empirical importance of the neglected non-linearity. We find
that distorting the model to satisfy the non-linear equilibrium condition is
strongly preferred by the data while the quality of the approximation is yet
another reason for the downward bias to estimates of the intertemporal
elasticity of substitution and the upward bias in risk aversion.",Mathematics and Financial Economics,http://arxiv.org/abs/1805.10869v3,econ.EM
310,"The Institutional Economics of Collective Waste Recovery Systems: an
  empirical investigation","The main purpose of the study is to develop the model for transaction costs
measurement in the Collective Waste Recovery Systems. The methodology of New
Institutional Economics is used in the research. The impact of the study is
related both to the enlargement of the limits of the theory about the
interaction between transaction costs and social costs and to the
identification of institutional failures of the European concept for circular
economy. A new model for social costs measurement is developed. Keywords:
circular economy, transaction costs, extended producer responsibility JEL: A13,
C51, D23, L22, Q53",Mathematics and Financial Economics ,http://arxiv.org/abs/1901.00495v2,q-fin.MF
232,"Social capital at venture capital firms and their financial performance:
  Evidence from China","This paper studies the extent to which social capital drives performance in
the Chinese venture capital market and explores the trend toward VC syndication
in China. First, we propose a hybrid model based on syndicated social networks
and the latent-variable model, which describes the social capital at venture
capital firms and builds relationships between social capital and performance
at VC firms. Then, we build three hypotheses about the relationships and test
the hypotheses using our proposed model. Some numerical simulations are given
to support the test results. Finally, we show that the correlations between
social capital and financial performance at venture capital firms are weak in
China and find that China's venture capital firms lack mature social capital
links.",Mathematics and Sports,http://arxiv.org/abs/1810.02952v1,econ.EM
114,"Aide et Croissance dans les pays de l'Union Economique et Mon{é}taire
  Ouest Africaine (UEMOA) : retour sur une relation controvers{é}e","The main purpose of this paper is to analyze threshold effects of official
development assistance (ODA) on economic growth in WAEMU zone countries. To
achieve this, the study is based on OECD and WDI data covering the period
1980-2015 and used Hansen's Panel Threshold Regression (PTR) model to
""bootstrap"" aid threshold above which its effectiveness is effective. The
evidence strongly supports the view that the relationship between aid and
economic growth is non-linear with a unique threshold which is 12.74% GDP.
Above this value, the marginal effect of aid is 0.69 points, ""all things being
equal to otherwise"". One of the main contribution of this paper is to show that
WAEMU countries need investments that could be covered by the foreign aid. This
later one should be considered just as a complementary resource. Thus, WEAMU
countries should continue to strengthen their efforts in internal resource
mobilization in order to fulfil this need.",Mathematics of Operations Research,http://arxiv.org/abs/1805.00435v1,econ.TH
140,The Model Selection Curse,"A ""statistician"" takes an action on behalf of an agent, based on the agent's
self-reported personal data and a sample involving other people. The action
that he takes is an estimated function of the agent's report. The estimation
procedure involves model selection. We ask the following question: Is
truth-telling optimal for the agent given the statistician's procedure? We
analyze this question in the context of a simple example that highlights the
role of model selection. We suggest that our simple exercise may have
implications for the broader issue of human interaction with ""machine learning""
algorithms.",Nature Climate Change,http://arxiv.org/abs/1810.02888v1,econ.GN
321,Technological Parasitism,"Technological parasitism is a new theory to explain the evolution of
technology in society. In this context, this study proposes a model to analyze
the interaction between a host technology (system) and a parasitic technology
(subsystem) to explain evolutionary pathways of technologies as complex
systems. The coefficient of evolutionary growth of the model here indicates the
typology of evolution of parasitic technology in relation to host technology:
i.e., underdevelopment, growth and development. This approach is illustrated
with realistic examples using empirical data of product and process
technologies. Overall, then, the theory of technological parasitism can be
useful for bringing a new perspective to explain and generalize the evolution
of technology and predict which innovations are likely to evolve rapidly in
society.",Nature Communications,http://arxiv.org/abs/1901.09073v1,physics.soc-ph
514,Fractional Top Trading Cycle on the Full Preference Domain,"Efficiency and fairness are two desiderata in market design. Fairness
requires randomization in many environments. Observing the inadequacy of Top
Trading Cycle (TTC) to incorporate randomization, Yu and Zhang (2020) propose
the class of Fractional TTC mechanisms to solve random allocation problems
efficiently and fairly. The assumption of strict preferences in the paper
restricts the application scope. This paper extends Fractional TTC to the full
preference domain in which agents can be indifferent between objects.
Efficiency and fairness of Fractional TTC are preserved. As a corollary, we
obtain an extension of the probabilistic serial mechanism in the house
allocation model to the full preference domain. Our extension does not require
any knowledge beyond elementary computation.",Nature Journal Scientific Reports ,http://arxiv.org/abs/2005.09340v1,econ.EM
490,Stability in EMU,"The public debt and deficit ceilings of the Maastricht Treaty are the subject
of recurring controversy. First, there is debate about the role and impact of
these criteria in the initial phase of the introduction of the single currency.
Secondly, it must be specified how these will then be applied, in a permanent
regime, when the single currency is well established.",Nature Review Physics ,http://arxiv.org/abs/1807.07730v1,physics.soc-ph
206,RPS(1) Preferences,"We consider a model for decision making based on an adaptive, k-period,
learning process where the priors are selected according to Von
Neumann-Morgenstern expected utility principle. A preference relation between
two prospects is introduced, defined by the condition which prospect is
selected more often. We show that the new preferences have similarities with
the preferences obtained by Kahneman and Tversky (1979) in the context of the
prospect theory. Additionally, we establish that in the limit of large learning
period, the new preferences coincide with the expected utility principle.",New Perspectives and Challenges in Econophysics and Sociophysics,http://arxiv.org/abs/1901.04995v3,q-fin.MF
577,"Nash SIR: An Economic-Epidemiological Model of Strategic Behavior During
  a Viral Epidemic","This paper develops a Nash-equilibrium extension of the classic SIR model of
infectious-disease epidemiology (""Nash SIR""), endogenizing people's decisions
whether to engage in economic activity during a viral epidemic and allowing for
complementarity in social-economic activity. An equilibrium epidemic is one in
which Nash equilibrium behavior during the epidemic generates the epidemic.
There may be multiple equilibrium epidemics, in which case the epidemic
trajectory can be shaped through the coordination of expectations, in addition
to other sorts of interventions such as stay-at-home orders and accelerated
vaccine development. An algorithm is provided to compute all equilibrium
epidemics.",Nonlinear Dynamics ,http://arxiv.org/abs/2006.10109v1,q-fin.ST
373,"One for all, all for one---von Neumann, Wald, Rawls, and Pareto","Applications of the maximin criterion extend beyond economics to statistics,
computer science, politics, and operations research. However, the maximin
criterion---be it von Neumann's, Wald's, or Rawls'---draws fierce criticism due
to its extremely pessimistic stance. I propose a novel concept, dubbed the
optimin criterion, which is based on (Pareto) optimizing the worst-case payoffs
of tacit agreements. The optimin criterion generalizes and unifies results in
various fields: It not only coincides with (i) Wald's statistical
decision-making criterion when Nature is antagonistic, (ii) the core in
cooperative games when the core is nonempty, though it exists even if the core
is empty, but it also generalizes (iii) Nash equilibrium in $n$-person
constant-sum games, (iv) stable matchings in matching models, and (v)
competitive equilibrium in the Arrow-Debreu economy. Moreover, every Nash
equilibrium satisfies the optimin criterion in an auxiliary game.",Operations Research ,http://arxiv.org/abs/1912.00211v4,math.OC
410,"A Consistent Variance Estimator for 2SLS When Instruments Identify
  Different LATEs","Under treatment effect heterogeneity, an instrument identifies the
instrument-specific local average treatment effect (LATE). With multiple
instruments, two-stage least squares (2SLS) estimand is a weighted average of
different LATEs. What is often overlooked in the literature is that the
postulated moment condition evaluated at the 2SLS estimand does not hold unless
those LATEs are the same. If so, the conventional heteroskedasticity-robust
variance estimator would be inconsistent, and 2SLS standard errors based on
such estimators would be incorrect. I derive the correct asymptotic
distribution, and propose a consistent asymptotic variance estimator by using
the result of Hall and Inoue (2003, Journal of Econometrics) on misspecified
moment condition models. This can be used to correctly calculate the standard
errors regardless of whether there is more than one LATE or not.",Operations Research and Decisions ,http://arxiv.org/abs/1806.01457v1,stat.AP
338,"The dynamic impact of monetary policy on regional housing prices in the
  US: Evidence based on factor-augmented vector autoregressions","In this study interest centers on regional differences in the response of
housing prices to monetary policy shocks in the US. We address this issue by
analyzing monthly home price data for metropolitan regions using a
factor-augmented vector autoregression (FAVAR) model. Bayesian model estimation
is based on Gibbs sampling with Normal-Gamma shrinkage priors for the
autoregressive coefficients and factor loadings, while monetary policy shocks
are identified using high-frequency surprises around policy announcements as
external instruments. The empirical results indicate that monetary policy
actions typically have sizeable and significant positive effects on regional
housing prices, revealing differences in magnitude and duration. The largest
effects are observed in regions located in states on both the East and West
Coasts, notably California, Arizona and Florida.",Operations Research Letters ,http://arxiv.org/abs/1802.05870v1,cs.GT
315,"How many people microwork in France? Estimating the size of a new labor
  force","Microwork platforms allocate fragmented tasks to crowds of providers with
remunerations as low as few cents. Instrumental to the development of today's
artificial intelligence, these micro-tasks push to the extreme the logic of
casualization already observed in ""uberized"" workers. The present article uses
the results of the DiPLab study to estimate the number of people who microwork
in France. We distinguish three categories of microworkers, corresponding to
different modes of engagement: a group of 14,903 ""very active"" microworkers,
most of whom are present on these platforms at least once a week; a second
featuring 52,337 ""routine"" microworkers, more selective and present at least
once a month; a third circle of 266,126 ""casual"" microworkers, more
heterogeneous and who alternate inactivity and various levels of work practice.
Our results show that microwork is comparable to, and even larger than, the
workforce of ride-sharing and delivery platforms in France. It is therefore not
an anecdotal phenomenon and deserves great attention from researchers, unions
and policy-makers.",Oxford Economic Papers ,http://arxiv.org/abs/1901.03889v1,econ.TH
466,On Capital Allocation under Information Constraints,"Attempts to allocate capital across a selection of different investments are
often hampered by the fact that investors' decisions are made under limited
information (no historical return data) and during an extremely limited
timeframe. Nevertheless, in some cases, rational investors with a certain level
of experience are able to ordinally rank investment alternatives through
relative assessments of the probabilities that investments will be successful.
However, to apply traditional portfolio optimization models, analysts must use
historical (or simulated/expected) return data as the basis for their
calculations. This paper develops an alternative portfolio optimization
framework that is able to handle this kind of information (given by an ordinal
ranking of investment alternatives) and to calculate an optimal capital
allocation based on a Cobb-Douglas function, which we call the Sorted Weighted
Portfolio (SWP). Considering risk-neutral investors, we show that the results
of this portfolio optimization model usually outperform the output generated by
the (intuitive) Equally Weighted Portfolio (EWP) of different investment
alternatives, which is the result of optimization when one is unable to
incorporate additional data (the ordinal ranking of the alternatives). To
further extend this work, we show that our model can also address risk-averse
investors to capture correlation effects.",Papers in Applied Geography ,http://arxiv.org/abs/1906.10624v2,stat.AP
117,"Sufficient Statistics for Unobserved Heterogeneity in Structural Dynamic
  Logit Models","We study the identification and estimation of structural parameters in
dynamic panel data logit models where decisions are forward-looking and the
joint distribution of unobserved heterogeneity and observable state variables
is nonparametric, i.e., fixed-effects model. We consider models with two
endogenous state variables: the lagged decision variable, and the time duration
in the last choice. This class of models includes as particular cases important
economic applications such as models of market entry-exit, occupational choice,
machine replacement, inventory and investment decisions, or dynamic demand of
differentiated products. The identification of structural parameters requires a
sufficient statistic that controls for unobserved heterogeneity not only in
current utility but also in the continuation value of the forward-looking
decision problem. We obtain the minimal sufficient statistic and prove
identification of some structural parameters using a conditional likelihood
approach. We apply this estimator to a machine replacement model.","Peace Economics, Peace Science and Public Policy",http://arxiv.org/abs/1805.04048v1,econ.GN
363,Disclosure Games with Large Evidence Spaces,"We study a disclosure game with a large evidence space. There is an unknown
binary state. A sender observes a sequence of binary signals about the state
and discloses a left truncation of the sequence to a receiver in order to
convince him that the state is good. We focus on truth-leaning equilibria (cf.
Hart et al. (2017)), where the sender discloses truthfully when doing so is
optimal, and the receiver takes off-path disclosure at face value. In
equilibrium, seemingly sub-optimal truncations are disclosed, and the
disclosure contains the longest truncation that yields the maximal difference
between the number of good and bad signals. We also study a general framework
of disclosure games which is compatible with large evidence spaces, a wide
range of disclosure technologies, and finitely many states. We characterize the
unique equilibrium value function of the sender and propose a method to
construct equilibria for a broad class of games.",Philosophy and Phenomenological Research ,http://arxiv.org/abs/1910.13633v3,cs.AI
101,"An Note on Why Geographically Weighted Regression Overcomes
  Multidimensional-Kernel-Based Varying-Coefficient Model","It is widely known that geographically weighted regression(GWR) is
essentially same as varying-coefficient model. In the former research about
varying-coefficient model, scholars tend to use multidimensional-kernel-based
locally weighted estimation(MLWE) so that information of both distance and
direction is considered. However, when we construct the local weight matrix of
geographically weighted estimation, distance among the locations in the
neighbor is the only factor controlling the value of entries of weight matrix.
In other word, estimation of GWR is distance-kernel-based. Thus, in this paper,
under stationary and limited dependent data with multidimensional subscripts,
we analyze the local mean squared properties of without any assumption of the
form of coefficient functions and compare it with MLWE. According to the
theoretical and simulation results, geographically-weighted locally linear
estimation(GWLE) is asymptotically more efficient than MLWE. Furthermore, a
relationship between optimal bandwith selection and design of scale parameters
is also obtained.",Physica A,http://arxiv.org/abs/1803.01402v2,econ.TH
365,Time discounting under uncertainty,"We study intertemporal decision making under uncertainty. We fully
characterize discounted expected utility in a framework \`a la Savage. Despite
the popularity of this model, no characterization is available in this setting.
The concept of stationarity, introduced by Koopmans for deterministic
discounted utility, plays a central role for both attitudes towards time and
towards uncertainty. We show that a strong stationarity axiom characterizes
discounted expected utility. When hedging considerations are taken into
account, a weaker stationarity axiom generalizes discounted expected utility to
Choquet discounted expected utility, allowing for non-neutral attitudes towards
uncertainty.",Physica A ,http://arxiv.org/abs/1911.00370v2,physics.soc-ph
397,"A Game Theoretic Setting of Capitation Versus Fee-For-Service Payment
  Systems","We aim to determine whether a game-theoretic model between an insurer and a
healthcare practice yields a predictive equilibrium that incentivizes either
player to deviate from a fee-for-service to capitation payment system. Using
United States data from various primary care surveys, we find that non-extreme
equilibria (i.e., shares of patients, or shares of patient visits, seen under a
fee-for-service payment system) can be derived from a Stackelberg game if
insurers award a non-linear bonus to practices based on performance. Overall,
both insurers and practices can be incentivized to embrace capitation payments
somewhat, but potentially at the expense of practice performance.",Physica A ,http://arxiv.org/abs/1904.11604v2,q-fin.GN
481,Simulation Modelling of Inequality in Cancer Service Access,"This paper applies economic concepts from measuring income inequality to an
exercise in assessing spatial inequality in cancer service access in regional
areas. We propose a mathematical model for accessing chemotherapy among local
government areas (LGAs). Our model incorporates a distance factor. With a
simulation we report results for a single inequality measure: the Lorenz curve
is depicted for our illustrative data. We develop this approach in order to
move incrementally towards its application to actual data and real-world health
service regions. We seek to develop the exercises that can lead policy makers
to relevant policy information on the most useful data collections to be
collected and modeling for cancer service access in regional areas.",Physica A ,http://arxiv.org/abs/1807.03048v1,q-fin.ST
515,Communication and Cooperation in Markets,"Many markets rely on traders truthfully communicating who has cheated in the
past and ostracizing those traders from future trade. This paper investigates
when truthful communication is incentive compatible. We find that if each side
has a myopic incentive to deviate, then communication incentives are satisfied
only when the volume of trade is low. By contrast, if only one side has a
myopic incentive to deviate, then communication incentives do not constrain the
volume of supportable trade. Accordingly, there are strong gains from
structuring trade so that one side either moves first or has its cooperation
guaranteed by external enforcement.",Physica A ,http://arxiv.org/abs/2005.09839v1,physics.soc-ph
301,"Evolution and structure of technological systems - An innovation output
  network","This study examines the network of supply and use of significant innovations
across industries in Sweden, 1970-2013. It is found that 30% of innovation
patterns can be predicted by network stimulus from backward and forward
linkages. The network is hierarchical, characterized by hubs that connect
diverse industries in closely knitted communities. To explain the network
structure, a preferential weight assignment process is proposed as an
adaptation of the classical preferential attachment process to weighted
directed networks. The network structure is strongly predicted by this process
where historical technological linkages and proximities matter, while human
capital flows and economic input-output flows have conflicting effects on link
formation. The results are consistent with the idea that innovations emerge in
closely connected communities, but suggest that the transformation of
technological systems are shaped by technological requirements, imbalances and
opportunities that are not straightforwardly related to other proximities.",Physica A Statistical Mechanics and its Applications,http://arxiv.org/abs/1811.06772v1,q-fin.ST
370,Information Disclosure and Promotion Policy Design for Platforms,"We consider a platform facilitating trade between sellers and buyers with the
objective of maximizing consumer surplus. Even though in many such marketplaces
prices are set by revenue-maximizing sellers, platforms can influence prices
through (i) price-dependent promotion policies that can increase demand for a
product by featuring it in a prominent position on the webpage and (ii) the
information revealed to sellers about the value of being promoted. Identifying
effective joint information design and promotion policies is a challenging
dynamic problem as sellers can sequentially learn the promotion value from
sales observations and update prices accordingly. We introduce the notion of
confounding promotion policies, which are designed to prevent a Bayesian seller
from learning the promotion value (at the expense of the short-run loss of
diverting consumers from the best product offering). Leveraging these policies,
we characterize the maximum long-run average consumer surplus that is
achievable through joint information design and promotion policies when the
seller sets prices myopically. We then construct a Bayesian Nash equilibrium in
which the seller's best response to the platform's optimal policy is to price
myopically at every period. Moreover, the equilibrium we identify is
platform-optimal within the class of horizon-maximin equilibria, in which
strategies are not predicated on precise knowledge of the horizon length, and
are designed to maximize payoff over the worst-case horizon. Our analysis
allows one to identify practical long-run average optimal platform policies in
a broad range of demand models.",Physica A Statistical Mechanics and its Applications,http://arxiv.org/abs/1911.09256v3,physics.soc-ph
476,On the Identifying Content of Instrument Monotonicity,"This paper studies the identifying content of the instrument monotonicity
assumption of Imbens and Angrist (1994) on the distribution of potential
outcomes in a model with a binary outcome, a binary treatment and an exogenous
binary instrument. Specifically, I derive necessary and sufficient conditions
on the distribution of the data under which the identified set for the
distribution of potential outcomes when the instrument monotonicity assumption
is imposed can be a strict subset of that when it is not imposed.",Physica A Statistical Mechanics and its Applications,http://arxiv.org/abs/1807.01661v2,stat.AP
189,Functional Sequential Treatment Allocation,"Consider a setting in which a policy maker assigns subjects to treatments,
observing each outcome before the next subject arrives. Initially, it is
unknown which treatment is best, but the sequential nature of the problem
permits learning about the effectiveness of the treatments. While the
multi-armed-bandit literature has shed much light on the situation when the
policy maker compares the effectiveness of the treatments through their mean,
much less is known about other targets. This is restrictive, because a cautious
decision maker may prefer to target a robust location measure such as a
quantile or a trimmed mean. Furthermore, socio-economic decision making often
requires targeting purpose specific characteristics of the outcome
distribution, such as its inherent degree of inequality, welfare or poverty. In
the present paper we introduce and study sequential learning algorithms when
the distributional characteristic of interest is a general functional of the
outcome distribution. Minimax expected regret optimality results are obtained
within the subclass of explore-then-commit policies, and for the unrestricted
class of all policies.",Physica A: Statistical Mechanics and its Applications,http://arxiv.org/abs/1812.09408v8,econ.TH
499,Nonparametric Regression with Selectively Missing Covariates,"We consider the problem of regression with selectively observed covariates in
a nonparametric framework. Our approach relies on instrumental variables that
explain variation in the latent covariates but have no direct effect on
selection. The regression function of interest is shown to be a weighted
version of observed conditional expectation where the weighting function is a
fraction of selection probabilities. Nonparametric identification of the
fractional probability weight (FPW) function is achieved via a partial
completeness assumption. We provide primitive functional form assumptions for
partial completeness to hold. The identification result is constructive for the
FPW series estimator. We derive the rate of convergence and also the pointwise
asymptotic distribution. In both cases, the asymptotic performance of the FPW
series estimator does not suffer from the inverse problem which derives from
the nonparametric instrumental variable approach. In a Monte Carlo study, we
analyze the finite sample properties of our estimator and we compare our
approach to inverse probability weighting, which can be used alternatively for
unconditional moment estimation. In the empirical application, we focus on two
different applications. We estimate the association between income and health
using linked data from the SHARE survey and administrative pension information
and use pension entitlements as an instrument. In the second application we
revisit the question how income affects the demand for housing based on data
from the German Socio-Economic Panel Study (SOEP). In this application we use
regional income information on the residential block level as an instrument. In
both applications we show that income is selectively missing and we demonstrate
that standard methods that do not account for the nonrandom selection process
lead to significantly biased estimates for individuals with low income.",Physica D Nonlinear Phenomena ,http://arxiv.org/abs/1810.00411v4,physics.soc-ph
208,A Noncooperative Model of Contest Network Formation,"In this paper we study a model of weighted network formation. The bilateral
interaction is modeled as a Tullock contest game with the possibility of a
draw. We describe stable networks under different concepts of stability. We
show that a Nash stable network is either the empty network or the complete
network. The complete network is not immune to bilateral deviations. When we
allow for limited farsightedness, stable networks immune to bilateral
deviations must be complete $M$-partite networks, with partitions of different
sizes. The empty network is the efficient network. We provide several
comparative statics results illustrating the importance of network structure in
mediating the effects of shocks and interventions. In particular, we show that
an increase in the likelihood of a draw has a non-monotonic effect on the level
of wasteful contest spending in the society. To the best of our knowledge, this
paper is the first attempt to model weighted network formation when the actions
of individuals are neither strategic complements nor strategic substitutes.",Physica D Nonlinear Phenomena ,http://arxiv.org/abs/1901.07605v4,econ.GN
296,Equilibrium in Production Chains with Multiple Upstream Partners,"In this paper, we extend and improve the production chain model introduced by
Kikuchi et al. (2018). Utilizing the theory of monotone concave operators, we
prove the existence, uniqueness, and global stability of equilibrium price,
hence improving their results on production networks with multiple upstream
partners. We propose an algorithm for computing the equilibrium price function
that is more than ten times faster than successive evaluations of the operator.
The model is then generalized to a stochastic setting that offers richer
implications for the distribution of firms in a production network.",Physical Review E,http://arxiv.org/abs/1908.08208v1,q-fin.RM
379,"A lifestyle-based model of household neighbourhood location and
  individual travel mode choice behaviours","Issues such as urban sprawl, congestion, oil dependence, climate change and
public health, are prompting urban and transportation planners to turn to land
use and urban design to rein in automobile use. One of the implicit beliefs in
this effort is that the right land-use policies will, in fact, help to reduce
automobile use and increase the use of alternative modes of transportation.
Thus, planners and transport engineers are increasingly viewing land use
policies and lifestyle patterns as a way to manage transportation demand. While
a substantial body of work has looked at the relationship between the built
environment and travel behaviour, as well as the influence of lifestyles and
lifestyle-related decisions on using different travel modes and activity
behaviours, limited work has been done in capturing these effects
simultaneously and also in exploring the effect of intra-household interaction
on individual attitudes and beliefs towards travel and activity behavior, and
their subsequent influence on lifestyles and modality styles. Therefore, for
this study we proposed a framework that captures the concurrent influence of
lifestyles and modality styles on both household-level decisions, such as
neighbourhood location, and individual-level decisions, such as travel mode
choices using a hierarchical Latent Class Choice Model.",Physical Review E,http://arxiv.org/abs/1902.01986v2,physics.soc-ph
383,"Preserve or retreat? Willingness-to-pay for Coastline Protection in New
  South Wales","Coastal erosion is a global and pervasive phenomenon that predicates a need
for a strategic approach to the future management of coastal values and assets
(both built and natural), should we invest in protective structures like
seawalls that aim to preserve specific coastal features, or allow natural
coastline retreat to preserve sandy beaches and other coastal ecosystems.
Determining the most suitable management approach in a specific context
requires a better understanding of the full suite of economic values the
populations holds for coastal assets, including non-market values. In this
study, we characterise New South Wales residents willingness to pay to maintain
sandy beaches (width and length). We use an innovative application of a Latent
Class Binary Logit model to deal with Yea-sayers and Nay-sayers, as well as
revealing the latent heterogeneity among sample members. We find that 65% of
the population would be willing to pay some amount of levy, dependent on the
policy setting. In most cases, there is no effect of degree of beach
deterioration characterised as loss of width and length of sandy beaches of
between 5% and 100% on respondents willingness to pay for a management levy.
This suggests that respondents who agreed to pay a management levy were
motivated to preserve sandy beaches in their current state irrespective of the
severity of sand loss likely to occur as a result of coastal erosion.
Willingness to pay also varies according to beach type (amongst Iconic, Main,
Bay and Surf beaches) a finding that can assist with spatial prioritisation of
coastal management. Not recognizing the presence of nay-sayers in the data or
recognizing them but eliminating them from the estimation will result in biased
WTP results and, consequently, biased policy propositions by coastal managers.",Physical Review E,http://arxiv.org/abs/1902.03310v1,q-fin.TR
393,"Can Mobility-on-Demand services do better after discerning reliability
  preferences of riders?","We formalize one aspect of reliability in the context of Mobility-on-Demand
(MoD) systems by acknowledging the uncertainty in the pick-up time of these
services. This study answers two key questions: i) how the difference between
the stated and actual pick-up times affect the propensity of a passenger to
choose an MoD service? ii) how an MoD service provider can leverage this
information to increase its ridership? We conduct a discrete choice experiment
in New York to answer the former question and adopt a micro-simulation-based
optimization method to answer the latter question. In our experiments, the
ridership of an MoD service could be increased by up to 10\% via displaying the
predicted wait time strategically.",Physical Review E,http://arxiv.org/abs/1904.07987v1,physics.soc-ph
407,Quasi-Experimental Shift-Share Research Designs,"Many studies use shift-share (or ``Bartik'') instruments, which average a set
of shocks with exposure share weights. We provide a new econometric framework
for shift-share instrumental variable (SSIV) regressions in which
identification follows from the quasi-random assignment of shocks, while
exposure shares are allowed to be endogenous. The framework is motivated by an
equivalence result: the orthogonality between a shift-share instrument and an
unobserved residual can be represented as the orthogonality between the
underlying shocks and a shock-level unobservable. SSIV regression coefficients
can similarly be obtained from an equivalent shock-level regression, motivating
shock-level conditions for their consistency. We discuss and illustrate several
practical insights of this framework in the setting of Autor et al. (2013),
estimating the effect of Chinese import competition on manufacturing employment
across U.S. commuting zones.",Physical Review E,http://arxiv.org/abs/1806.01221v9,physics.soc-ph
451,Data Analytics in Operations Management: A Review,"Research in operations management has traditionally focused on models for
understanding, mostly at a strategic level, how firms should operate. Spurred
by the growing availability of data and recent advances in machine learning and
optimization methodologies, there has been an increasing application of data
analytics to problems in operations management. In this paper, we review recent
applications of data analytics to operations management, in three major areas
-- supply chain management, revenue management and healthcare operations -- and
highlight some exciting directions for the future.",Physical Review E,http://arxiv.org/abs/1905.00556v1,physics.soc-ph
207,Relational Communication,"We study a communication game between an informed sender and an uninformed
receiver with repeated interactions and voluntary transfers. Transfers motivate
the receiver's decision-making and signal the sender's information. Although
full separation can always be supported in equilibrium, partial or complete
pooling is optimal if the receiver's decision-making is highly responsive to
information. In this case, the receiver's decision-making is disciplined by
pooling extreme states where she is most tempted to defect.",Physical Review E,http://arxiv.org/abs/1901.05645v3,econ.TH
426,Voluntary Disclosure and Personalized Pricing,"Central to privacy concerns is that firms may use consumer data to price
discriminate. A common policy response is that consumers should be given
control over which firms access their data and how. Since firms learn about a
consumer's preferences based on the data seen and the consumer's disclosure
choices, the equilibrium implications of consumer control are unclear. We study
whether such measures improve consumer welfare in monopolistic and competitive
markets. We find that consumer control can improve consumer welfare relative to
both perfect price discrimination and no personalized pricing. First, consumers
can use disclosure to amplify competitive forces. Second, consumers can
disclose information to induce even a monopolist to lower prices. Whether
consumer control improves welfare depends on the disclosure technology and
market competitiveness. Simple disclosure technologies suffice in competitive
markets. When facing a monopolist, a consumer needs partial disclosure
possibilities to obtain any welfare gains.",Physical Review Research,http://arxiv.org/abs/1912.04774v2,physics.soc-ph
371,Guarantees in Fair Division: general or monotone preferences,"To divide a ""manna"" {\Omega} of private items (commodities, workloads, land,
time intervals) between n agents, the worst case measure of fairness is the
welfare guaranteed to each agent, irrespective of others' preferences. If the
manna is non atomic and utilities are continuous (not necessarily monotone or
convex), we can guarantee the minMax utility: that of our agent's best share in
her worst partition of the manna; and implement it by Kuhn's generalisation of
Divide and Choose. The larger Maxmin utility -- of her worst share in her best
partition -- cannot be guaranteed, even for two agents. If for all agents more
manna is better than less (or less is better than more), our Bid & Choose rules
implement guarantees between minMax and Maxmin by letting agents bid for the
smallest (or largest) size of a share they find acceptable.",Physics Reports ,http://arxiv.org/abs/1911.10009v3,physics.soc-ph
399,"Identification of Key Companies for International Profit Shifting in the
  Global Ownership Network","In the global economy, the intermediate companies owned by multinational
corporations are becoming an important policy issue as they are likely to cause
international profit shifting and diversion of foreign direct investments. The
purpose of this analysis is to call the intermediate companies with high risk
of international profit shifting as key firms and to identifying and clarify
them. For this aim, we propose a model that focuses on each affiliate's
position on the ownership structure of each multinational corporation. Based on
the information contained in the Orbis database, we constructed the Global
Ownership Network, reflecting the relationship that can give significant
influence to a firm, and analyzed for large multinational corporations listed
in Fortune Global 500. In this analysis, first, we confirmed the validity of
this model by identifying affiliates playing an important role in international
tax avoidance at a certain degree. Secondly, intermediate companies are mainly
found in the Netherlands and the United Kingdom, etc., and tended to be located
in jurisdictions favorable to treaty shopping. And it was found that such key
firms are concentrated on the IN component of the bow-tie structure that the
giant weakly connected component of the Global Ownership Network consist of.
Therefore, it clarifies that the key firms are geographically located in
specific jurisdictions, and concentrates on specific components in the Global
Ownership Network. The location of key firms are related with the ease of
treaty shopping, and there is a difference in the jurisdiction where key firms
are located depending on the location of the multinational corporations.",Physics Reports ,http://arxiv.org/abs/1904.12397v1,physics.soc-ph
534,"Review of the Plan for Integrating Big Data Analytics Program for the
  Electronic Marketing System and Customer Relationship Management: A Case
  Study XYZ Institution","This research aims to explore business processes and what the factors have
major influence on electronic marketing and CRM systems? Which data needs to be
analyzed and integrated in the system, and how to do that? How effective of
integration the electronic marketing and CRM with big data enabled to support
Marketing and Customer Relation operations. Research based on case studies at
XYZ Organization: International Language Education Service in Surabaya.
Research is studying secondary data which is supported by qualitative research
methods. Using purposive sampling technique with observation and interviewing
several respondents who need the system integration. The documentation of
interview is coded to keep confidentiality of the informant. Method of
extending participation, triangulation of data sources, discussions and the
adequacy of the theory are uses to validate data. Miles and Huberman models is
uses to do analysis the data interview. Results of the research are expected to
become a holistic approach to fully integrate the Big Data Analytics program
with electronic marketing and CRM systems.",Physics Reports ,http://arxiv.org/abs/1908.02430v1,q-fin.ST
377,Surprised by the Hot Hand Fallacy? A Truth in the Law of Small Numbers,"We prove that a subtle but substantial bias exists in a common measure of the
conditional dependence of present outcomes on streaks of past outcomes in
sequential data. The magnitude of this streak selection bias generally
decreases as the sequence gets longer, but increases in streak length, and
remains substantial for a range of sequence lengths often used in empirical
work. We observe that the canonical study in the influential hot hand fallacy
literature, along with replications, are vulnerable to the bias. Upon
correcting for the bias we find that the long-standing conclusions of the
canonical study are reversed.",PLoS Computational Biology,http://arxiv.org/abs/1902.01265v1,physics.soc-ph
10,Economic Development and Inequality: a complex system analysis,"By borrowing methods from complex system analysis, in this paper we analyze
the features of the complex relationship that links the development and the
industrialization of a country to economic inequality. In order to do this, we
identify industrialization as a combination of a monetary index, the GDP per
capita, and a recently introduced measure of the complexity of an economy, the
Fitness. At first we explore these relations on a global scale over the time
period 1990--2008 focusing on two different dimensions of inequality: the
capital share of income and a Theil measure of wage inequality. In both cases,
the movement of inequality follows a pattern similar to the one theorized by
Kuznets in the fifties. We then narrow down the object of study ad we
concentrate on wage inequality within the United States. By employing data on
wages and employment on the approximately 3100 US counties for the time
interval 1990--2014, we generalize the Fitness-Complexity algorithm for
counties and NAICS sectors, and we investigate wage inequality between
industrial sectors within counties. At this scale, in the early nineties we
recover a behavior similar to the global one. While, in more recent years, we
uncover a trend reversal: wage inequality monotonically increases as
industrialization levels grow. Hence at a county level, at net of the social
and institutional factors that differ among countries, we not only observe an
upturn in inequality but also a change in the structure of the relation between
wage inequality and development.",PLOS One,http://arxiv.org/abs/1605.03133v1,econ.GN
33,Inference based on Kotlarski's Identity,"Kotlarski's identity has been widely used in applied economic research.
However, how to conduct inference based on this popular identification approach
has been an open question for two decades. This paper addresses this open
problem by constructing a novel confidence band for the density function of a
latent variable in repeated measurement error model. The confidence band builds
on our finding that we can rewrite Kotlarski's identity as a system of linear
moment restrictions. The confidence band controls the asymptotic size uniformly
over a class of data generating processes, and it is consistent against all
fixed alternatives. Simulation studies support our theoretical results.",PLOS One,http://arxiv.org/abs/1808.09375v3,econ.GN
110,Two-way fixed effects estimators with heterogeneous treatment effects,"Linear regressions with period and group fixed effects are widely used to
estimate treatment effects. We show that they estimate weighted sums of the
average treatment effects (ATE) in each group and period, with weights that may
be negative. Due to the negative weights, the linear regression coefficient may
for instance be negative while all the ATEs are positive. We propose another
estimator that solves this issue. In the two applications we revisit, it is
significantly different from the linear regression estimator.",PLOS One,http://arxiv.org/abs/1803.08807v7,econ.GN
360,The Persuasion Duality,"We present a unified duality approach to Bayesian persuasion. The optimal
dual variable, interpreted as a price function, is shown to be a supergradient
of the concave closure of the objective function at the prior belief. Under
regularity conditions, our general duality result implies known results for the
case when the objective function depends only on the expected state. We apply
our approach to characterize the optimal signal in the case when the state is
two-dimensional.",PLOS One,http://arxiv.org/abs/1910.11392v1,q-fin.GN
547,"Interaction of a Hydrogen Refueling Station Network for Heavy-Duty
  Vehicles and the Power System in Germany for 2050","A potential solution to reduce greenhouse gas (GHG) emissions in the
transport sector is to use alternatively fueled vehicles (AFV). Heavy-duty
vehicles (HDV) emit a large share of GHG emissions in the transport sector and
are therefore the subject of growing attention from global regulators. Fuel
cell and green hydrogen technologies are a promising option to decarbonize
HDVs, as their fast refueling and long vehicle ranges are in line with current
logistic operation concepts. Moreover, the application of green hydrogen in
transport could enable more effective integration of renewable energies (RE)
across different energy sectors. This paper explores the interplay between HDV
Hydrogen Refueling Stations (HRS) that produce hydrogen locally and the power
system by combining an infrastructure location planning model and an energy
system optimization model that takes grid expansion options into account. Two
scenarios - one sizing refueling stations in symbiosis with the power system
and one sizing them independently of it - are assessed regarding their impacts
on the total annual energy system costs, regional RE integration and the
levelized cost of hydrogen (LCOH). The impacts are calculated based on
locational marginal pricing for 2050. Depending on the integration scenario, we
find average LCOH of between 5.66 euro/kg and 6.20 euro/kg, for which nodal
electricity prices are the main determining factor as well as a strong
difference in LCOH between north and south Germany. From a system perspective,
investing in HDV-HRS in symbiosis with the power system rather than
independently promises cost savings of around one billion-euros per annum. We
therefore conclude that the co-optimization of multiple energy sectors is
important for investment planning and has the potential to exploit synergies.",PLOS One,http://arxiv.org/abs/1908.10119v1,econ.GN
493,"Efficient Difference-in-Differences Estimation with High-Dimensional
  Common Trend Confounding","This study considers various semiparametric difference-in-differences models
under different assumptions on the relation between the treatment group
identifier, time and covariates for cross-sectional and panel data. The
variance lower bound is shown to be sensitive to the model assumptions imposed
implying a robustness-efficiency trade-off. The obtained efficient influence
functions lead to estimators that are rate double robust and have desirable
asymptotic properties under weak first stage convergence conditions. This
enables to use sophisticated machine-learning algorithms that can cope with
settings where common trend confounding is high-dimensional. The usefulness of
the proposed estimators is assessed in an empirical example. It is shown that
the efficiency-robustness trade-offs and the choice of first stage predictors
can lead to divergent empirical results in practice.",PLOS One,http://arxiv.org/abs/1809.01643v5,physics.soc-ph
513,"Optimal Trade-Off Between Economic Activity and Health During an
  Epidemic","This paper considers a simple model where a social planner can influence the
spread-intensity of an infection wave, and, consequently, also the economic
activity and population health, through a single parameter. Population health
is assumed to only be negatively affected when the number of simultaneously
infected exceeds health care capacity. The main finding is that if (i) the
planner attaches a positive weight on economic activity and (ii) it is more
harmful for the economy to be locked down for longer than shorter time periods,
then the optimal policy is to (weakly) exceed health care capacity at some
time.",PLOS One,http://arxiv.org/abs/2005.07590v1,cs.IR
467,The Syntax of the Accounting Language: A First Step,"We review and interpret two basic propositions published by Ellerman (2014).
The propositions address the algebraic structure of T accounts and double entry
bookkeeping (DEB). The paper builds on this previous contribution with the view
of reconciling the two, apparently dichotomous, perspectives of accounting
measurement: the one that focuses preferably on the stock of wealth and to the
one that focuses preferably on the flow of income. The paper claims that
T-accounts and DEB have an underlying algebraic structure suitable for
approaching measurement from either or both perspectives. Accountants
preferences for stocks or flows can be framed in ways which are mutually
consistent. The paper is a first step in addressing this consistency issue. It
avoids the difficult mathematics of abstract algebra by applying the concept of
syntax to accounting numbers such that the accounting procedure qualifies as a
formal language with which accountants convey meaning.",PLOS One,http://arxiv.org/abs/1906.10865v1,econ.GN
510,"Instability of Defection in the Prisoner's Dilemma Under Best
  Experienced Payoff Dynamics","We study population dynamics under which each revising agent tests each
strategy k times, with each trial being against a newly drawn opponent, and
chooses the strategy whose mean payoff was highest. When k = 1, defection is
globally stable in the prisoner`s dilemma. By contrast, when k > 1 we show that
there exists a globally stable state in which agents cooperate with probability
between 28% and 50%. Next, we characterize stability of strict equilibria in
general games. Our results demonstrate that the empirically plausible case of k
> 1 can yield qualitatively different predictions than the case of k = 1 that
is commonly studied in the literature.",PLOS One,http://arxiv.org/abs/2005.05779v3,econ.GN
571,Estimation of High-Dimensional Seemingly Unrelated Regression Models,"In this paper, we investigate seemingly unrelated regression (SUR) models
that allow the number of equations (N) to be large, and to be comparable to the
number of the observations in each equation (T). It is well known in the
literature that the conventional SUR estimator, for example, the generalized
least squares (GLS) estimator of Zellner (1962) does not perform well. As the
main contribution of the paper, we propose a new feasible GLS estimator called
the feasible graphical lasso (FGLasso) estimator. For a feasible implementation
of the GLS estimator, we use the graphical lasso estimation of the precision
matrix (the inverse of the covariance matrix of the equation system errors)
assuming that the underlying unknown precision matrix is sparse. We derive
asymptotic theories of the new estimator and investigate its finite sample
properties via Monte-Carlo simulations.",PLOS One,http://arxiv.org/abs/1811.05567v1,q-fin.ST
585,"A closed-form solution to the risk-taking motivation of subordinated
  debtholders","Black and Cox (1976) claim that the value of junior debt is increasing in
asset risk when the firm's value is low. We show, using closed-form solution,
that the junior debt's value is hump-shaped. This has interesting implications
for the market-discipline role of banks' junior debt.",PLOS One,http://arxiv.org/abs/2006.15309v1,physics.soc-ph
74,Payoff Information and Learning in Signaling Games,"We add the assumption that players know their opponents' payoff functions and
rationality to a model of non-equilibrium learning in signaling games. Agents
are born into player roles and play against random opponents every period.
Inexperienced agents are uncertain about the prevailing distribution of
opponents' play, but believe that opponents never choose conditionally
dominated strategies. Agents engage in active learning and update beliefs based
on personal observations. Payoff information can refine or expand learning
predictions, since patient young senders' experimentation incentives depend on
which receiver responses they deem plausible. We show that with payoff
knowledge, the limiting set of long-run learning outcomes is bounded above by
rationality-compatible equilibria (RCE), and bounded below by uniform RCE. RCE
refine the Intuitive Criterion (Cho and Kreps, 1987) and include all divine
equilibria (Banks and Sobel, 1987). Uniform RCE sometimes but not always
exists, and implies universally divine equilibrium.",PLOS One,http://arxiv.org/abs/1709.01024v5,econ.GN
307,"Modelling Social Evolutionary Processes and Peer Effects in Agricultural
  Trade Networks: the Rubber Value Chain in Indonesia","Understanding market participants' channel choices is important to policy
makers because it yields information on which channels are effective in
transmitting information. These channel choices are the result of a recursive
process of social interactions and determine the observable trading networks.
They are characterized by feedback mechanisms due to peer interaction and
therefore need to be understood as complex adaptive systems (CAS). When
modelling CAS, conventional approaches like regression analyses face severe
drawbacks since endogeneity is omnipresent. As an alternative, process-based
analyses allow researchers to capture these endogenous processes and multiple
feedback loops. This paper applies an agent-based modelling approach (ABM) to
the empirical example of the Indonesian rubber trade. The feedback mechanisms
are modelled via an innovative approach of a social matrix, which allows
decisions made in a specific period to feed back into the decision processes in
subsequent periods, and allows agents to systematically assign different
weights to the decision parameters based on their individual characteristics.
In the validation against the observed network, uncertainty in the found
estimates, as well as under determination of the model, are dealt with via an
approach of evolutionary calibration: a genetic algorithm finds the combination
of parameters that maximizes the similarity between the simulated and the
observed network. Results indicate that the sellers' channel choice decisions
are mostly driven by physical distance and debt obligations, as well as
peer-interaction. Within the social matrix, the most influential individuals
are sellers that live close by to other traders, are active in social groups
and belong to the ethnic majority in their village.",PNAS,http://arxiv.org/abs/1811.11476v1,econ.EM
475,Indirect inference through prediction,"By recasting indirect inference estimation as a prediction rather than a
minimization and by using regularized regressions, we can bypass the three
major problems of estimation: selecting the summary statistics, defining the
distance function and minimizing it numerically. By substituting regression
with classification we can extend this approach to model selection as well. We
present three examples: a statistical fit, the parametrization of a simple real
business cycle model and heuristics selection in a fishery agent-based model.
The outcome is a method that automatically chooses summary statistics, weighs
them and use them to parametrize models without running any direct
minimization.",PNAS,http://arxiv.org/abs/1807.01579v1,econ.GN
358,"Transboundary Pollution Externalities: Think Globally, Act Locally?","We analyze the implications of transboundary pollution externalities on
environmental policymaking in a spatial and finite time horizon setting. We
focus on a simple regional optimal pollution control problem in order to
compare the global and local solutions in which, respectively, the
transboundary externality is and is not taken into account in the determination
of the optimal policy by individual local policymakers. We show that the local
solution is suboptimal and as such a global approach to environmental problems
is effectively needed. Our conclusions hold true in different frameworks,
including situations in which the spatial domain is either bounded or
unbounded, and situations in which macroeconomic-environmental feedback effects
are taken into account. We also show that if every local economy implements an
environmental policy stringent enough, then the global average level of
pollution will fall. If this is the case, over the long run the entire global
economy will be able to achieve a completely pollution-free status.",Political Analysis,http://arxiv.org/abs/1910.04469v1,physics.soc-ph
234,"Integrating electricity markets: Impacts of increasing trade on prices
  and emissions in the western United States","This paper presents empirically-estimated average hourly relationships
between regional electricity trade in the United States and prices, emissions,
and generation from 2015 through 2018. Consistent with economic theory, the
analysis finds a negative relationship between electricity prices in California
and regional trade, conditional on local demand. Each 1 gigawatt-hour increase
in California electricity imports is associated with an average $0.15 per
megawatt-hour decrease in the California Independent System Operator's
wholesale electricity price. There is a net-negative short term relationship
between carbon dioxide emissions in California and electricity imports that is
partially offset by positive emissions from exporting neighbors. Specifically,
each 1 GWh increase in regional trade is associated with a net 70-ton average
decrease in CO2 emissions across the western U.S., conditional on demand
levels. The results provide evidence that electricity imports mostly displace
natural gas generation on the margin in the California electricity market. A
small positive relationship is observed between short-run SO2 and NOx emissions
in neighboring regions and California electricity imports. The magnitude of the
SO2 and NOx results suggest an average increase of 0.1 MWh from neighboring
coal plants is associated with a 1 MWh increase in imports to California.",Positivity and its Applications ,http://arxiv.org/abs/1810.04759v3,econ.TH
106,Business Cycles in Economics,"The business cycles are generated by the oscillating macro-/micro-/nano-
economic output variables in the economy of the scale and the scope in the
amplitude/frequency/phase/time domains in the economics. The accurate forward
looking assumptions on the business cycles oscillation dynamics can optimize
the financial capital investing and/or borrowing by the economic agents in the
capital markets. The book's main objective is to study the business cycles in
the economy of the scale and the scope, formulating the Ledenyov unified
business cycles theory in the Ledenyov classic and quantum econodynamics.",Prague Economic Papers,http://arxiv.org/abs/1803.06108v1,econ.GN
347,"Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous
  Treatment Effects","To estimate the dynamic effects of an absorbing treatment, researchers often
use two-way fixed effects regressions that include leads and lags of the
treatment. We show that in settings with variation in treatment timing across
units, the coefficient on a given lead or lag can be contaminated by effects
from other periods, and apparent pretrends can arise solely from treatment
effects heterogeneity. We propose an alternative estimator that is free of
contamination, and illustrate the relative shortcomings of two-way fixed
effects regressions with leads and lags through an empirical application.",Project Management Journal,http://arxiv.org/abs/1804.05785v2,econ.GN
175,"A Double Machine Learning Approach to Estimate the Effects of Musical
  Practice on Student's Skills","This study investigates the dose-response effects of making music on youth
development. Identification is based on the conditional independence assumption
and estimation is implemented using a recent double machine learning estimator.
The study proposes solutions to two highly practically relevant questions that
arise for these new methods: (i) How to investigate sensitivity of estimates to
tuning parameter choices in the machine learning part? (ii) How to assess
covariate balancing in high-dimensional settings? The results show that
improvements in objectively measured cognitive skills require at least medium
intensity, while improvements in school grades are already observed for low
intensity of practice.",Quantitative Economics,http://arxiv.org/abs/1805.10300v2,econ.EM
249,Navigating the Cryptocurrency Landscape: An Islamic Perspective,"Almost a decade on from the launch of Bitcoin, cryptocurrencies continue to
generate headlines and intense debate. What started as an underground
experiment by a rag tag group of programmers armed with a Libertarian manifesto
has now resulted in a thriving $230 billion ecosystem, with constant on-going
innovation. Scholars and researchers alike are realizing that cryptocurrencies
are far more than mere technical innovation; they represent a distinct and
revolutionary new economic paradigm tending towards decentralization.
Unfortunately, this bold new universe is little explored from the perspective
of Islamic economics and finance. Our work aims to address these deficiencies.
Our paper makes the following distinct contributions We significantly expand
the discussion on whether cryptocurrencies qualify as ""money"" from an Islamic
perspective and we argue that this debate necessitates rethinking certain
fundamental definitions. We conclude that the cryptocurrency phenomenon, with
its radical new capabilities, may hold considerable opportunity which merits
deeper investigation.",Quantitative Economics,http://arxiv.org/abs/1811.05935v1,econ.EM
24,Spreading of an infectious disease between different locations,"The endogenous adaptation of agents, that may adjust their local contact
network in response to the risk of being infected, can have the perverse effect
of increasing the overall systemic infectiveness of a disease. We study a
dynamical model over two geographically distinct but interacting locations, to
better understand theoretically the mechanism at play. Moreover, we provide
empirical motivation from the Italian National Bovine Database, for the period
2006-2013.",Quantitative Economics ,http://arxiv.org/abs/1812.07827v1,econ.EM
359,"Necessary and sufficient condition for equilibrium of the Hotelling
  model on a circle","We study a model of vendors competing to sell a homogeneous product to
customers spread evenly along a circular city. This model is based on
Hotelling's celebrated paper in 1929. Our aim in this paper is to present a
necessary and sufficient condition for the equilibrium. This yields a
representation for the equilibrium. To achieve this, we first formulate the
model mathematically. Next, we prove that the condition holds if and only if
vendors are equilibrium.",Quantitative Economics ,http://arxiv.org/abs/1910.11154v2,econ.EM
430,Efficient allocations in double auction markets,"This paper proposes a simple descriptive model of discrete-time double
auction markets for divisible assets. As in the classical models of exchange
economies, we consider a finite set of agents described by their initial
endowments and preferences. Instead of the classical Walrasian-type market
models, however, we assume that all trades take place in a centralized double
auction where the agents communicate through sealed limit orders for buying and
selling. We find that, under nonstrategic bidding, the double auction clears
with zero trades precisely when the agents' current holdings are on the Pareto
frontier. More interestingly, the double auctions implement Adam Smith's
""invisible hand"" in the sense that, when starting from disequilibrium, repeated
double auctions lead to a sequence of allocations that converges to
individually rational Pareto allocations.",Quantitative Economics ,http://arxiv.org/abs/2001.02071v2,econ.EM
174,"Selectivity correction in discrete-continuous models for the willingness
  to work as crowd-shippers and travel time tolerance","The objective of this study is to understand the different behavioral
considerations that govern the choice of people to engage in a crowd-shipping
market. Using novel data collected by the researchers in the US, we develop
discrete-continuous models. A binary logit model has been used to estimate
crowd-shippers' willingness to work, and an ordinary least-square regression
model has been employed to calculate crowd-shippers' maximum tolerance for
shipping and delivery times. A selectivity-bias term has been included in the
model to correct for the conditional relationships of the crowd-shipper's
willingness to work and their maximum travel time tolerance. The results show
socio-demographic characteristics (e.g. age, gender, race, income, and
education level), transporting freight experience, and number of social media
usages significant influence the decision to participate in the crowd-shipping
market. In addition, crowd-shippers pay expectations were found to be
reasonable and concurrent with the literature on value-of-time. Findings from
this research are helpful for crowd-shipping companies to identify and attract
potential shippers. In addition, an understanding of crowd-shippers - their
behaviors, perceptions, demographics, pay expectations, and in which contexts
they are willing to divert from their route - are valuable to the development
of business strategies such as matching criteria and compensation schemes for
driver-partners.",Quantitative Finance,http://arxiv.org/abs/1810.00985v1,econ.EM
59,The cost of information,"We develop an axiomatic theory of information acquisition that captures the
idea of constant marginal costs in information production: the cost of
generating two independent signals is the sum of their costs, and generating a
signal with probability half costs half its original cost. Together with
Blackwell monotonicity and a continuity condition, these axioms determine the
cost of a signal up to a vector of parameters. These parameters have a clear
economic interpretation and determine the difficulty of distinguishing states.
We argue that this cost function is a versatile modeling tool that leads to
more realistic predictions than mutual information.",Quarterly Journal of Economics,http://arxiv.org/abs/1812.04211v3,econ.EM
489,A New Index of Human Capital to Predict Economic Growth,"The accumulation of knowledge required to produce economic value is a process
that often relates to nations economic growth. Such a relationship, however, is
misleading when the proxy of such accumulation is the average years of
education. In this paper, we show that the predictive power of this proxy
started to dwindle in 1990 when nations schooling began to homogenized. We
propose a metric of human capital that is less sensitive than average years of
education and remains as a significant predictor of economic growth when tested
with both cross-section data and panel data. We argue that future research on
economic growth will discard educational variables based on quantity as
predictor given the thresholds that these variables are reaching.",R Journal ,http://arxiv.org/abs/1807.07051v1,stat.CO
463,Ergodicity-breaking reveals time optimal decision making in humans,"Ergodicity describes an equivalence between the expectation value and the
time average of observables. Applied to human behaviour, ergodic theories of
decision-making reveal how individuals should tolerate risk in different
environments. To optimise wealth over time, agents should adapt their utility
function according to the dynamical setting they face. Linear utility is
optimal for additive dynamics, whereas logarithmic utility is optimal for
multiplicative dynamics. Whether humans approximate time optimal behavior
across different dynamics is unknown. Here we compare the effects of additive
versus multiplicative gamble dynamics on risky choice. We show that utility
functions are modulated by gamble dynamics in ways not explained by prevailing
decision theory. Instead, as predicted by time optimality, risk aversion
increases under multiplicative dynamics, distributing close to the values that
maximise the time average growth of wealth. We suggest that our findings
motivate a need for explicitly grounding theories of decision-making on ergodic
considerations.",Regional Development Studies ,http://arxiv.org/abs/1906.04652v5,stat.AP
227,"Exploring the nuances in the relationship ""culture-strategy"" for the
  business world","The current article explores interesting, significant and recently identified
nuances in the relationship ""culture-strategy"". The shared views of leading
scholars at the University of National and World Economy in relation with the
essence, direction, structure, role and hierarchy of ""culture-strategy""
relation are defined as a starting point of the analysis. The research emphasis
is directed on recent developments in interpreting the observed realizations of
the aforementioned link among the community of international scholars and
consultants, publishing in selected electronic scientific databases. In this
way a contemporary notion of the nature of ""culture-strategy"" relationship for
the entities from the world of business is outlined.",Renew Sustain Energy Rev,http://arxiv.org/abs/1810.02613v1,econ.GN
57,"The Income Fluctuation Problem with Capital Income Risk: Optimality and
  Stability","This paper studies the income fluctuation problem with capital income risk
(i.e., dispersion in the rate of return to wealth). Wealth returns and labor
earnings are allowed to be serially correlated and mutually dependent. Rewards
can be bounded or unbounded. Under rather general conditions, we develop a set
of new results on the existence and uniqueness of solutions, stochastic
stability of the model economy, as well as efficient computation of the ergodic
wealth distribution. A variety of applications are discussed. Quantitative
analysis shows that both stochastic volatility and mean persistence in wealth
returns have nontrivial impact on wealth inequality.",Research in Applied Economics ,http://arxiv.org/abs/1812.01320v1,econ.EM
391,Rational Inattention and Retirement Puzzles,"I present evidence incorporating costly thought solves three puzzles in the
retirement literature. The first puzzle is, given incentives, the extent of
bunching of labour market exits at legislated state pension ages (SPA) seems
incompatible with rational expectations. Adding to the evidence for this
puzzle, I include an empirical analysis focusing on whether liquidity
constraints can explain this bunching and find they cannot. The nature of this
puzzle is clarified by exploring a life-cycle model with rational agents that
matches aggregate profiles. This model succeeds in matching aggregates by
overestimating the impact of the SPA on poorer individuals whilst
underestimating its impact on wealthier people. The second puzzle is people are
often mistaken about their own pension provisions. Concerning the second
puzzle, I incorporate rational inattention to the SPA into the aforementioned
life-cycle model, allowing for mistaken beliefs. To the best of my knowledge,
this paper is the first not only to incorporate rational inattention into a
life-cycle model but also to assess a rationally inattentive model against
non-experimental individual choice data. This facilitates another important
contribution: discipling the cost of attention with subjective belief data.
Preliminary results indicate rational inattention improves the aggregate fit
and better matches the response of participation to the SPA across the wealth
distribution, hence offering a resolution to the first puzzle. The third puzzle
is despite actuarially advantageous options to defer receipt of pension
benefits, take up is extremely low. An extension of the model generates an
explanation of this last puzzle: the actuarial calculations implying deferral
is preferable ignore the utility cost of tracking your pension which can be
avoided by claiming. These puzzles are researched in the context of the reform
to the UK female SPA.",Research in Economics ,http://arxiv.org/abs/1904.06520v1,econ.GN
608,Informe-pais Brasil,"Structural socioeconomic analysis of Brazil. All basic information about this
South American country is gathered in a comprehensive outlook that includes the
challenges Brazil faces, as well as their causes and posible economic
solutions.",Research in Economics ,http://arxiv.org/abs/1909.08564v1,econ.GN
41,A Method for Winning at Lotteries,"We report a new result on lotteries --- that a well-funded syndicate has a
purely mechanical strategy to achieve expected returns of 10\% to 25\% in an
equiprobable lottery with no take and no carryover pool. We prove that an
optimal strategy (Nash equilibrium) in a game between the syndicate and other
players consists of betting one of each ticket (the ""trump ticket""), and extend
that result to proportional ticket selection in non-equiprobable lotteries. The
strategy can be adjusted to accommodate lottery taxes and carryover pools. No
""irrationality"" need be involved for the strategy to succeed --- it requires
only that a large group of non-syndicate bettors each choose a few tickets
independently.",Research Policy ,http://arxiv.org/abs/1801.02958v1,econ.GN
447,A Search Model of Statistical Discrimination,"We offer a search-theoretic model of statistical discrimination, in which
firms treat identical groups unequally based on their occupational choices. The
model admits symmetric equilibria in which the group characteristic is ignored,
but also asymmetric equilibria in which a group is statistically discriminated
against, even when symmetric equilibria are unique. Moreover, a robust
possibility is that symmetric equilibria become unstable when the group
characteristic is introduced. Unlike most previous literature, our model can
justify affirmative action since it eliminates asymmetric equilibria without
distorting incentives.",Resources Conservation and Recycling ,http://arxiv.org/abs/2004.06645v2,physics.soc-ph
31,Tests for price indices in a dynamic item universe,"There is generally a need to deal with quality change and new goods in the
consumer price index due to the underlying dynamic item universe. Traditionally
axiomatic tests are defined for a fixed universe. We propose five tests
explicitly formulated for a dynamic item universe, and motivate them both from
the perspectives of a cost-of-goods index and a cost-of-living index. None of
the indices satisfies all the tests at the same time, which are currently
available for making use of scanner data that comprises the whole item
universe. The set of tests provides a rigorous diagnostic for whether an index
is completely appropriate in a dynamic item universe, as well as pointing
towards the directions of possible remedies. We thus outline a large index
family that potentially can satisfy all the tests.",Resources Policy ,http://arxiv.org/abs/1808.08995v2,econ.GN
280,"Addictive Auctions: using lucky-draw and gambling addiction to increase
  participation during auctioning","Auction theories are believed to provide a better selling opportunity for the
resources to be allocated. Various organizations have taken measures to
increase trust among participants towards their auction system, but trust alone
cannot ensure a high level of participation. We propose a new type of auction
system which takes advantage of lucky draw and gambling addictions to increase
the engagement level of candidates in an auction. Our system makes use of
security features present in existing auction systems for ensuring fairness and
maintaining trust among participants.",Review of Economics and Statistics ,http://arxiv.org/abs/1906.03237v1,econ.EM
304,"Long-run Consequences of Health Insurance Promotion When Mandates are
  Not Enforceable: Evidence from a Field Experiment in Ghana","We study long-run selection and treatment effects of a health insurance
subsidy in Ghana, where mandates are not enforceable. We randomly provide
different levels of subsidy (1/3, 2/3, and full), with follow-up surveys seven
months and three years after the initial intervention. We find that a one-time
subsidy promotes and sustains insurance enrollment for all treatment groups,
but long-run health care service utilization increases only for the partial
subsidy groups. We find evidence that selection explains this pattern: those
who were enrolled due to the subsidy, especially the partial subsidy, are more
ill and have greater health care utilization.",Review of Economics and Statistics ,http://arxiv.org/abs/1811.09004v2,econ.TH
369,Distributionally Robust Optimal Auction Design under Mean Constraints,"We study a seller who sells a single good to multiple bidders with
uncertainty over the joint distribution of bidders' valuations, as well as
bidders' higher-order beliefs about their opponents. The seller only knows the
(possibly asymmetric) means of the marginal distributions of each bidder's
valuation and the range. An adversarial nature chooses the worst-case
distribution within this ambiguity set along with the worst-case information
structure. We find that a second-price auction with a symmetric, random reserve
price obtains the optimal revenue guarantee within a broad class of mechanisms
we refer to as competitive mechanisms, which include standard auction formats,
including the first-price auction, with or without reserve prices. The optimal
mechanism possesses two notable characteristics. First, the mechanism treats
all bidders identically even in the presence of ex-ante asymmetries. Second,
when bidders are identical and the number of bidders $n$ grows large, the
seller's optimal reserve price converges in probability to a non-binding
reserve price and the revenue guarantee converges to the best possible revenue
guarantee at rate $O(1/n)$.",Review of Economics and Statistics ,http://arxiv.org/abs/1911.07103v3,econ.EM
401,Empirical Equilibrium,"We study the foundations of empirical equilibrium, a refinement of Nash
equilibrium that is based on a non-parametric characterization of empirical
distributions of behavior in games (Velez and Brown,2020b arXiv:1907.12408).
The refinement can be alternatively defined as those Nash equilibria that do
not refute the regular QRE theory of Goeree, Holt, and Palfrey (2005). By
contrast, some empirical equilibria may refute monotone additive randomly
disturbed payoff models. As a by product, we show that empirical equilibrium
does not coincide with refinements based on approximation by monotone additive
randomly disturbed payoff models, and further our understanding of the
empirical content of these models.",Review of Economics and Statistics ,http://arxiv.org/abs/1804.07986v3,econ.TH
468,"P-hacking in clinical trials and how incentives shape the distribution
  of results across phases","Clinical research should conform to high standards of ethical and scientific
integrity, given that human lives are at stake. However, economic incentives
can generate conflicts of interest for investigators, who may be inclined to
withhold unfavorable results or even tamper with data in order to achieve
desired outcomes. To shed light on the integrity of clinical trial results,
this paper systematically analyzes the distribution of p-values of primary
outcomes for phase II and phase III drug trials reported to the
ClinicalTrials.gov registry. First, we detect no bunching of results just above
the classical 5% threshold for statistical significance. Second, a density
discontinuity test reveals an upward jump at the 5% threshold for phase III
results by small industry sponsors. Third, we document a larger fraction of
significant results in phase III compared to phase II. Linking trials across
phases, we find that early favorable results increase the likelihood of
continuing into the next phase. Once we take into account this selective
continuation, we can explain almost completely the excess of significant
results in phase III for trials conducted by large industry sponsors. For small
industry sponsors, instead, part of the excess remains unexplained.",Review of Economics and Statistics ,http://arxiv.org/abs/1907.00185v3,econ.EM
137,Selling Information,"I consider the monopolistic pricing of informational good. A buyer's
willingness to pay for information is from inferring the unknown payoffs of
actions in decision making. A monopolistic seller and the buyer each observes a
private signal about the payoffs. The seller's signal is binary and she can
commit to sell any statistical experiment of her signal to the buyer. Assuming
that buyer's decision problem involves rich actions, I characterize the profit
maximizing menu. It contains a continuum of experiments, each containing
different amount of information. I also find a complementarity between buyer's
private information and information provision: when buyer's private signal is
more informative, the optimal menu contains more informative experiments.","Review of Keynesian Studies
 ",http://arxiv.org/abs/1809.06770v2,econ.GN
67,Persuading part of an audience,"I propose a cheap-talk model in which the sender can use private messages and
only cares about persuading a subset of her audience. For example, a candidate
only needs to persuade a majority of the electorate in order to win an
election. I find that senders can gain credibility by speaking truthfully to
some receivers while lying to others. In general settings, the model admits
information transmission in equilibrium for some prior beliefs. The sender can
approximate her preferred outcome when the fraction of the audience she needs
to persuade is sufficiently small. I characterize the sender-optimal
equilibrium and the benefit of not having to persuade your whole audience in
separable environments. I also analyze different applications and verify that
the results are robust to some perturbations of the model, including
non-transparent motives as in Crawford and Sobel (1982), and full commitment as
in Kamenica and Gentzkow (2011).",Revista Contabilidade e Financas,http://arxiv.org/abs/1903.00129v1,econ.GN
325,"The Research on the Stagnant Development of Shantou Special Economic
  Zone Under Reform and Opening-Up Policy","This study briefly introduces the development of Shantou Special Economic
Zone under Reform and Opening-Up Policy from 1980 through 2016 with a focus on
policy making issues and its influences on local economy. This paper is divided
into two parts, 1980 to 1991, 1992 to 2016 in accordance with the separation of
the original Shantou District into three cities: Shantou, Chaozhou and Jieyang
in the end of 1991. This study analyzes the policy making issues in the
separation of the original Shantou District, the influences of the policy on
Shantou's economy after separation, the possibility of merging the three cities
into one big new economic district in the future and reasons that lead to the
stagnant development of Shantou in recent 20 years. This paper uses statistical
longitudinal analysis in analyzing economic problems with applications of
non-parametric statistics through generalized additive model and time series
forecasting methods. The paper is authored by Bowen Cai solely, who is the
graduate student in the PhD program of Applied and Computational Mathematics
and Statistics at the University of Notre Dame with concentration in big data
analysis.",Revista de Historiografia,http://arxiv.org/abs/1711.08877v1,econ.GN
150,The Impact of Renewable Energy Forecasts on Intraday Electricity Prices,"In this paper we study the impact of errors in wind and solar power forecasts
on intraday electricity prices. We develop a novel econometric model which is
based on day-ahead wholesale auction curves data and errors in wind and solar
power forecasts. The model shifts day-ahead supply curves to calculate intraday
prices. We apply our model to the German EPEX SPOT SE data. Our model
outperforms both linear and non-linear benchmarks. Our study allows us to
conclude that errors in renewable energy forecasts exert a non-linear impact on
intraday prices. We demonstrate that additional wind and solar power capacities
induce non-linear changes in the intraday price volatility. Finally, we comment
on economical and policy implications of our findings.",Revista Espacios ,http://arxiv.org/abs/1903.09641v2,econ.GN
169,"The ""power"" dimension in a process of exchange","The field of study of this paper is the analysis of the exchange between two
subjects. Circumscribed to the micro dimension, it is however expanded with
respect to standard economic theory by introducing both the dimension of power
and the motivation to exchange. The basic reference is made by the reflections
of those economists, preeminently John Kenneth Galbraith, who criticize the
removal from the neoclassical economy of the ""power"" dimension. We have also
referred to the criticism that Galbraith, among others, makes to the assumption
of neoclassical economists that the ""motivation"" in exchanges is solely linked
to the reward, to the money obtained in the exchange. We have got around the
problem of having a large number of types of power and also a large number of
forms of motivation by directly taking into account the effects on the welfare
of each subject, regardless of the means with which they are achieved: that is,
referring to everything that happens in the negotiation process to the
potential or real variations of the welfare function induced in each subject
due to the exercise of the specific form of power, on a case by case basis, and
of the intensity of the motivation to perform the exchange. In the construction
of a mathematical model we paid great attention to its usability in field
testing.",Revue d'Economie Politique ,http://arxiv.org/abs/1809.08293v3,econ.GN
390,"Understanding consumer demand for new transport technologies and
  services, and implications for the future of mobility","The transport sector is witnessing unprecedented levels of disruption.
Privately owned cars that operate on internal combustion engines have been the
dominant modes of passenger transport for much of the last century. However,
recent advances in transport technologies and services, such as the development
of autonomous vehicles, the emergence of shared mobility services, and the
commercialization of alternative fuel vehicle technologies, promise to
revolutionise how humans travel. The implications are profound: some have
predicted the end of private car dependent Western societies, others have
portended greater suburbanization than has ever been observed before. If
transport systems are to fulfil current and future needs of different
subpopulations, and satisfy short and long-term societal objectives, it is
imperative that we comprehend the many factors that shape individual behaviour.
This chapter introduces the technologies and services most likely to disrupt
prevailing practices in the transport sector. We review past studies that have
examined current and future demand for these new technologies and services, and
their likely short and long-term impacts on extant mobility patterns. We
conclude with a summary of what these new technologies and services might mean
for the future of mobility.",Risks ,http://arxiv.org/abs/1904.05554v1,econ.GN
606,"A new concept of technology with systemic-purposeful perpsective:
  theory, examples and empirical application","Although definitions of technology exist to explain the patterns of
technological innovations, there is no general definition that explain the role
of technology for humans and other animal species in environment. The goal of
this study is to suggest a new concept of technology with a systemic-purposeful
perspective for technology analysis. Technology here is a complex system of
artifact, made and_or used by living systems, that is composed of more than one
entity or sub-system and a relationship that holds between each entity and at
least one other entity in the system, selected considering practical, technical
and_or economic characteristics to satisfy needs, achieve goals and_or solve
problems of users for purposes of adaptation and_or survival in environment.
Technology T changes current modes of cognition and action to enable makers
and_or users to take advantage of important opportunities or to cope with
consequential environmental threats. Technology, as a complex system, is formed
by different elements given by incremental and radical innovations.
Technological change generates the progress from a system T1 to T2, T3, etc.
driven by changes of technological trajectories and technological paradigms.
Several examples illustrate here these concepts and a simple model with a
preliminary empirical analysis shows how to operationalize the suggested
definition of technology. Overall, then, the role of adaptation (i.e.
reproductive advantage) can be explained as a main driver of technology use for
adopters to take advantage of important opportunities or to cope with
environmental threats. This study begins the process of clarifying and
generalizing, as far as possible, the concept of technology with a new
perspective that it can lay a foundation for the development of more
sophisticated concepts and theories to explain technological and economic
change in environment.",Risks ,http://arxiv.org/abs/1909.05689v1,econ.GN
345,Inference on Local Average Treatment Effects for Misclassified Treatment,"We develop point-identification for the local average treatment effect when
the binary treatment contains a measurement error. The standard instrumental
variable estimator is inconsistent for the parameter since the measurement
error is non-classical by construction. We correct the problem by identifying
the distribution of the measurement error based on the use of an exogenous
variable that can even be a binary covariate. The moment conditions derived
from the identification lead to generalized method of moments estimation with
asymptotically valid inferences. Monte Carlo simulations and an empirical
illustration demonstrate the usefulness of the proposed procedure.",Royal Society Open Science ,http://arxiv.org/abs/1804.03349v1,q-bio.PE
46,Quantifying Health Shocks Over the Life Cycle,"We first show (1) the importance of investigating health expenditure process
using the order two Markov chain model, rather than the standard order one
model, which is widely used in the literature. Markov chain of order two is the
minimal framework that is capable of distinguishing those who experience a
certain health expenditure level for the first time from those who have been
experiencing that or other levels for some time. In addition, using the model
we show (2) that the probability of encountering a health shock first de-
creases until around age 10, and then increases with age, particularly, after
age 40, (3) that health shock distributions among different age groups do not
differ until their percentiles reach the median range, but that above the
median the health shock distributions of older age groups gradually start to
first-order dominate those of younger groups, and (4) that the persistency of
health shocks also shows a U-shape in relation to age.",Science,http://arxiv.org/abs/1801.08746v1,econ.EM
58,Mutual Conversion Between Preference Maps And Cook-Seiford Vectors,"In group decision making, the preference map and Cook-Seiford vector are two
concepts as ways of describing ties-permitted ordinal rankings. This paper
shows that they are equivalent for representing ties-permitted ordinal
rankings. Transformation formulas from one to the other are given and the
inherent consistency of the mutual conversion is discussed. The proposed
methods are illustrated by some examples. Some possible future applications of
the proposed formulas are also pointed out.",Scientific Journal of Agricultural Economics,http://arxiv.org/abs/1812.03566v1,econ.EM
403,"Statistical and Economic Evaluation of Time Series Models for
  Forecasting Arrivals at Call Centers","Call centers' managers are interested in obtaining accurate point and
distributional forecasts of call arrivals in order to achieve an optimal
balance between service quality and operating costs. We present a strategy for
selecting forecast models of call arrivals which is based on three pillars: (i)
flexibility of the loss function; (ii) statistical evaluation of forecast
accuracy; (iii) economic evaluation of forecast performance using money
metrics. We implement fourteen time series models and seven forecast
combination schemes on three series of daily call arrivals. Although we focus
mainly on point forecasts, we also analyze density forecast evaluation. We show
that second moments modeling is important both for point and density
forecasting and that the simple Seasonal Random Walk model is always
outperformed by more general specifications. Our results suggest that call
center managers should invest in the use of forecast models which describe both
first and second moments of call arrivals.",Scientific Reports,http://arxiv.org/abs/1804.08315v1,q-fin.GN
238,"Does the price of strategic commodities respond to U.S. Partisan
  Conflict?","A noteworthy feature of U.S. politics in recent years is serious partisan
conflict, which has led to intensifying polarization and exacerbating high
policy uncertainty. The US is a significant player in oil and gold markets. Oil
and gold also form the basis of important strategic reserves in the US. We
investigate whether U.S. partisan conflict affects the returns and price
volatility of oil and gold using a parametric test of Granger causality in
quantiles. The empirical results suggest that U.S. partisan conflict has an
effect on the returns of oil and gold, and the effects are concentrated at the
tail of the conditional distribution of returns. More specifically, the
partisan conflict mainly affects oil returns when the crude oil market is in a
bearish state (lower quantiles). By contrast, partisan conflict matters for
gold returns only when the gold market is in a bullish scenario (higher
quantiles). In addition, for the volatility of oil and gold, the predictability
of partisan conflict index virtually covers the entire distribution of
volatility.",Scientific Reports,http://arxiv.org/abs/1810.08396v2,physics.soc-ph
540,"A complex net of intertwined complements: Measuring interdimensional
  dependence among the poor","The choice of appropriate measures of deprivation, identification and
aggregation of poverty has been a challenge for many years. The works of Sen,
Atkinson and others have been the cornerstone for most of the literature on
poverty measuring. Recent contributions have focused in what we now know as
multidimensional poverty measuring. Current aggregation and identification
measures for multidimensional poverty make the implicit assumption that
dimensions are independent of each other, thus ignoring the natural dependence
between them. In this article a variant of the usual method of deprivation
measuring is presented. It allows the existence of the forementioned
connections by drawing from geometric and networking notions. This new
methodology relies on previous identification and aggregation methods, but with
small modifications to prevent arbitrary manipulations. It is also proved that
this measure still complies with the axiomatic framework of its predecessor.
Moreover, the general form of latter can be considered a particular case of
this new measure, although this identification is not unique.",Scientific Reports,http://arxiv.org/abs/1908.07870v1,physics.soc-ph
458,"Autonomous Driving and Residential Location Preferences: Evidence from a
  Stated Choice Survey","The literature suggests that autonomous vehicles (AVs) may drastically change
the user experience of private automobile travel by allowing users to engage in
productive or relaxing activities while travelling. As a consequence, the
generalised cost of car travel may decrease, and car users may become less
sensitive to travel time. By facilitating private motorised mobility, AVs may
eventually impact land use and households' residential location choices. This
paper seeks to advance the understanding of the potential impacts of AVs on
travel behaviour and land use by investigating stated preferences for
combinations of residential locations and travel options for the commute in the
context of autonomous automobile travel. Our analysis draws from a stated
preference survey, which was completed by 512 commuters from the Sydney
metropolitan area in Australia and provides insights into travel time
valuations in a long-term decision-making context. For the analysis of the
stated choice data, mixed logit models are estimated. Based on the empirical
results, no changes in the valuation of travel time due to the advent of AVs
should be expected. However, given the hypothetical nature of the stated
preference survey, the results may be affected by methodological limitations.",Scientific Reports,http://arxiv.org/abs/1905.11486v3,physics.soc-ph
294,The interest rate for saving as a possibilistic risk,"In the paper there is studied an optimal saving model in which the
interest-rate risk for saving is a fuzzy number. The total utility of
consumption is defined by using a concept of possibilistic expected utility. A
notion of possibilistic precautionary saving is introduced as a measure of the
variation of optimal saving level when moving from a sure saving model to a
possibilistic risk model. A first result establishes a necessary and sufficient
condition that the presence of a possibilistic interest-rate risk generates an
extra-saving. This result can be seen as a possibilistic version of a
Rothschilld and Stiglitz theorem on a probabilistic model of saving. A second
result of the paper studies the variation of the optimal saving level when
moving from a probabilistic model (the interest-rate risk is a random variable)
to a possibilistic model (the interest-rate risk is a fuzzy number).",Scientific Reports ,http://arxiv.org/abs/1908.00445v1,physics.soc-ph
366,"Aggregation for potentially infinite populations without continuity or
  completeness","We present an abstract social aggregation theorem. Society, and each
individual, has a preorder that may be interpreted as expressing values or
beliefs. The preorders are allowed to violate both completeness and continuity,
and the population is allowed to be infinite. The preorders are only assumed to
be represented by functions with values in partially ordered vector spaces, and
whose product has convex range. This includes all preorders that satisfy strong
independence. Any Pareto indifferent social preorder is then shown to be
represented by a linear transformation of the representations of the individual
preorders. Further Pareto conditions on the social preorder correspond to
positivity conditions on the transformation. When all the Pareto conditions
hold and the population is finite, the social preorder is represented by a sum
of individual preorder representations. We provide two applications. The first
yields an extremely general version of Harsanyi's social aggregation theorem.
The second generalizes a classic result about linear opinion pooling.",Scientific Reports ,http://arxiv.org/abs/1911.00872v1,econ.GN
404,"Economic inequality and Islamic Charity: An exploratory agent-based
  modeling approach","Economic inequality is one of the pivotal issues for most of economic and
social policy makers across the world to insure the sustainable economic growth
and justice. In the mainstream school of economics, namely neoclassical
theories, economic issues are dealt with in a mechanistic manner. Such a
mainstream framework is majorly focused on investigating a socio-economic
system based on an axiomatic scheme where reductionism approach plays a vital
role. The major limitations of such theories include unbounded rationality of
economic agents, reducing the economic aggregates to a set of predictable
factors and lack of attention to adaptability and the evolutionary nature of
economic agents. In tackling deficiencies of conventional economic models, in
the past two decades, some new approaches have been recruited. One of those
novel approaches is the Complex adaptive systems (CAS) framework which has
shown a very promising performance in action. In contrast to mainstream school,
under this framework, the economic phenomena are studied in an organic manner
where the economic agents are supposed to be both boundedly rational and
adaptive. According to it, the economic aggregates emerge out of the ways
agents of a system decide and interact. As a powerful way of modeling CASs,
Agent-based models (ABMs) has found a growing application among academicians
and practitioners. ABMs show that how simple behavioral rules of agents and
local interactions among them at micro-scale can generate surprisingly complex
patterns at macro-scale. In this paper, ABMs have been used to show (1) how an
economic inequality emerges in a system and to explain (2) how sadaqah as an
Islamic charity rule can majorly help alleviating the inequality and how
resource allocation strategies taken by charity entities can accelerate this
alleviation.",Scientific Reports ,http://arxiv.org/abs/1804.09284v1,physics.soc-ph
423,Maastricht and Monetary Cooperation,"This paper describes the opportunities and also the difficulties of EMU with
regard to international monetary cooperation. Even though the institutional and
intellectual assistance to the coordination of monetary policy in the EU will
probably be strengthened with the EMU, among the shortcomings of the Maastricht
Treaty concerns the relationship between the founder members and those
countries who wish to remain outside monetary union.",Scientific Reports ,http://arxiv.org/abs/1807.00419v1,q-fin.GN
449,Dynamically Consistent Objective and Subjective Rationality,"A group of experts, for instance climate scientists, is to choose among two
policies $f$ and $g$. Consider the following decision rule. If all experts
agree that the expected utility of $f$ is higher than the expected utility of
$g$, the unanimity rule applies, and $f$ is chosen. Otherwise the precautionary
principle is implemented and the policy yielding the highest minimal expected
utility is chosen.
  This decision rule may lead to time inconsistencies when an intermediate
period of partial resolution of uncertainty is added. We provide axioms that
enlarge the initial group of experts with veto power, which leads to a set of
probabilistic beliefs that is ""rectangular"" in a minimal sense. This makes this
decision rule dynamically consistent and provides, as a byproduct, a novel
behavioral characterization of rectangularity.",Scientific Reports ,http://arxiv.org/abs/2004.12347v1,physics.soc-ph
482,Clustering Macroeconomic Time Series,"The data mining technique of time series clustering is well established in
many fields. However, as an unsupervised learning method, it requires making
choices that are nontrivially influenced by the nature of the data involved.
The aim of this paper is to verify usefulness of the time series clustering
method for macroeconomics research, and to develop the most suitable
methodology.
  By extensively testing various possibilities, we arrive at a choice of a
dissimilarity measure (compression-based dissimilarity measure, or CDM) which
is particularly suitable for clustering macroeconomic variables. We check that
the results are stable in time and reflect large-scale phenomena such as
crises. We also successfully apply our findings to analysis of national
economies, specifically to identifying their structural relations.",Scientific Reports ,http://arxiv.org/abs/1807.04004v2,physics.soc-ph
322,"Does Better Governance Guarantee Less Corruption? Evidence of Loss in
  Effectiveness of the Rule of Law","Corruption is an endemic societal problem with profound implications in the
development of nations. In combating this issue, cross-national evidence
supporting the effectiveness of the rule of law seems at odds with poorly
realized outcomes from reforms inspired in such literature. This paper provides
an explanation for such contradiction. By taking a computational approach, we
develop two methodological novelties into the empirical study of corruption:
(1) generating large within-country variation by means of simulation (instead
of cross-national data pooling), and (2) accounting for interactions between
covariates through a spillover network. The latter (the network), seems
responsible for a significant reduction in the effectiveness of the rule of
law; especially among the least developed countries. We also find that
effectiveness can be boosted by improving complementary policy issues that may
lie beyond the governance agenda. Moreover, our simulations suggest that
improvements to the rule of law are a necessary yet not sufficient condition to
curve corruption.",SIAM Journal on Discrete Mathematics ,http://arxiv.org/abs/1902.00428v1,cs.GT
543,Dynamic Programming with State-Dependent Discounting,"This paper extends the core results of discrete time infinite horizon dynamic
programming to the case of state-dependent discounting. We obtain a condition
on the discount factor process under which all of the standard optimality
results can be recovered. We also show that the condition cannot be
significantly weakened. Our framework is general enough to handle complications
such as recursive preferences and unbounded rewards. Economic and financial
applications are discussed.",Smart Energy,http://arxiv.org/abs/1908.08800v4,physics.soc-ph
166,The Ladder Theory of Behavioral Decision Making,"We study individual decision-making behavioral on generic view. Using a
formal mathematical model, we investigate the action mechanism of decision
behavioral under subjective perception changing of task attributes. Our model
is built on work in two kinds classical behavioral decision making theory:
""prospect theory (PT)"" and ""image theory (IT)"". We consider subjective
attributes preference of decision maker under the whole decision process.
Strategies collection and selection mechanism are induced according the
description of multi-attributes decision making. A novel behavioral
decision-making framework named ""ladder theory (LT)"" is proposed. By real four
cases comparing, the results shows that the LT have better explanation and
prediction ability then PT and IT under some decision situations. Furthermore,
we use our model to shed light on that the LT theory can cover PT and IT
ideally. It is the enrichment and development for classical behavioral decision
theory and, it has positive theoretical value and instructive significance for
explaining plenty of real decision-making phenomena. It may facilitate our
understanding of how individual decision-making performed actually.",Social Choice and Welfare,http://arxiv.org/abs/1809.03442v3,math.CO
181,Doubly Robust Difference-in-Differences Estimators,"This article proposes doubly robust estimators for the average treatment
effect on the treated (ATT) in difference-in-differences (DID) research
designs. In contrast to alternative DID estimators, the proposed estimators are
consistent if either (but not necessarily both) a propensity score or outcome
regression working models are correctly specified. We also derive the
semiparametric efficiency bound for the ATT in DID designs when either panel or
repeated cross-section data are available, and show that our proposed
estimators attain the semiparametric efficiency bound when the working models
are correctly specified. Furthermore, we quantify the potential efficiency
gains of having access to panel data instead of repeated cross-section data.
Finally, by paying articular attention to the estimation method used to
estimate the nuisance parameters, we show that one can sometimes construct
doubly robust DID estimators for the ATT that are also doubly robust for
inference. Simulation studies and an empirical application illustrate the
desirable finite-sample performance of the proposed estimators. Open-source
software for implementing the proposed policy evaluation tools is available.",Social Choice and Welfare,http://arxiv.org/abs/1812.01723v3,cs.GT
263,"Zero-rating of Content and its Effect on the Quality of Service in the
  Internet","The ongoing net neutrality debate has generated a lot of heated discussions
on whether or not monetary interactions should be regulated between content and
access providers. Among the several topics discussed, `differential pricing'
has recently received attention due to `zero-rating' platforms proposed by some
service providers. In the differential pricing scheme, Internet Service
Providers (ISPs) can exempt data access charges for on content from certain CPs
(zero-rated) while no exemption is on content from other CPs. This allows the
possibility for Content Providers (CPs) to make `sponsorship' agreements to
zero-rate their content and attract more user traffic. In this paper, we study
the effect of differential pricing on various players in the Internet. We first
consider a model with a monopolistic ISP and multiple CPs where users select
CPs based on the quality of service (QoS) and data access charges. We show that
in a differential pricing regime 1) a CP offering low QoS can make have higher
surplus than a CP offering better QoS through sponsorships. 2) Overall QoS
(mean delay) for end users can degrade under differential pricing schemes. In
the oligopolistic market with multiple ISPs, users tend to select the ISP with
lowest ISP resulting in same type of conclusions as in the monopolistic market.
We then study how differential pricing effects the revenue of ISPs.",Social Evolution & History ,http://arxiv.org/abs/1709.09334v2,econ.GN
123,"Bitcoin price and its marginal cost of production: support for a
  fundamental value","This study back-tests a marginal cost of production model proposed to value
the digital currency bitcoin. Results from both conventional regression and
vector autoregression (VAR) models show that the marginal cost of production
plays an important role in explaining bitcoin prices, challenging recent
allegations that bitcoins are essentially worthless. Even with markets pricing
bitcoin in the thousands of dollars each, the valuation model seems robust. The
data show that a price bubble that began in the Fall of 2017 resolved itself in
early 2018, converging with the marginal cost model. This suggests that while
bubbles may appear in the bitcoin market, prices will tend to this bound and
not collapse to zero.",Socio-Economic Planning Sciences,http://arxiv.org/abs/1805.07610v1,econ.GN
351,Illiquid Financial Markets and Monetary Policy,"This paper analyzes the role of money in asset markets characterized by
search frictions. We develop a dynamic framework that brings together a model
for illiquid financial assets `a la Duffie, Garleanu, and Pedersen, and a
search-theoretic model of monetary exchange `a la Lagos and Wright. The
presence of decentralized financial markets generates an essential role for
money, which helps investors re-balance their portfolios. We provide conditions
that guarantee the existence of a monetary equilibrium. In this case, asset
prices are always above their fundamental value, and this differential
represents a liquidity premium. We are able to derive an asset pricing theory
that delivers an explicit connection between monetary policy, asset prices, and
welfare. We obtain a negative relationship between inflation and equilibrium
asset prices. This key result stems from the complementarity between money and
assets in our framework.",Soybean Research ,http://arxiv.org/abs/1909.01889v1,econ.GN
202,Why are prices proportional to embodied energies?,"The observed proportionality between nominal prices and average embodied
energies cannot be interpreted with conventional economic theory. A model is
presented that places energy transfers as the focal point of scarcity based on
the idea that (1) goods are material rearrangements, and (2) humans can only
rearrange matter with energy transfers. Modified consumer and producer problems
for an autarkic agent show that the opportunity cost of goods are given by
their marginal energy transfers, which depend on subjective and objective
factors (e.g. consumer preferences and direct energy transfers). Allowing for
exchange and under perfect competition, nominal prices arise as social
manifestations of goods' marginal energy transfers. The proportionality between
nominal prices and average embodied energy follows given the relation between
the latter and marginal energy transfers.",SSRN Electronic Journal,http://arxiv.org/abs/1811.12502v1,econ.GN
193,"Predicting ""Design Gaps"" in the Market: Deep Consumer Choice Models
  under Probabilistic Design Constraints","Predicting future successful designs and corresponding market opportunity is
a fundamental goal of product design firms. There is accordingly a long history
of quantitative approaches that aim to capture diverse consumer preferences,
and then translate those preferences to corresponding ""design gaps"" in the
market. We extend this work by developing a deep learning approach to predict
design gaps in the market. These design gaps represent clusters of designs that
do not yet exist, but are predicted to be both (1) highly preferred by
consumers, and (2) feasible to build under engineering and manufacturing
constraints. This approach is tested on the entire U.S. automotive market using
of millions of real purchase data. We retroactively predict design gaps in the
market, and compare predicted design gaps with actual known successful designs.
Our preliminary results give evidence it may be possible to predict design
gaps, suggesting this approach has promise for early identification of market
opportunity.",SSRN Electronic Journal ,http://arxiv.org/abs/1812.11067v1,econ.GN
199,ppmlhdfe: Fast Poisson Estimation with High-Dimensional Fixed Effects,"In this paper we present ppmlhdfe, a new Stata command for estimation of
(pseudo) Poisson regression models with multiple high-dimensional fixed effects
(HDFE). Estimation is implemented using a modified version of the iteratively
reweighted least-squares (IRLS) algorithm that allows for fast estimation in
the presence of HDFE. Because the code is built around the reghdfe package, it
has similar syntax, supports many of the same functionalities, and benefits
from reghdfe's fast convergence properties for computing high-dimensional least
squares problems.
  Performance is further enhanced by some new techniques we introduce for
accelerating HDFE-IRLS estimation specifically. ppmlhdfe also implements a
novel and more robust approach to check for the existence of (pseudo) maximum
likelihood estimates.",SSRN Electronic Journal ,http://arxiv.org/abs/1903.01690v3,econ.GN
215,Optimal mechanism for the sale of a durable good,"A buyer wishes to purchase a durable good from a seller who in each period
chooses a mechanism under limited commitment. The buyer's valuation is binary
and fully persistent. We show that posted prices implement all equilibrium
outcomes of an infinite-horizon, mechanism selection game. Despite being able
to choose mechanisms, the seller can do no better and no worse than if he chose
prices in each period, so that he is subject to Coase's conjecture. Our
analysis marries insights from information and mechanism design with those from
the literature on durable goods. We do so by relying on the revelation
principle in Doval and Skreta (2020).",SSRN Electronic Journal ,http://arxiv.org/abs/1904.07456v7,econ.GN
557,"A Consistent Heteroskedasticity Robust LM Type Specification Test for
  Semiparametric Models","This paper develops a consistent heteroskedasticity robust Lagrange
Multiplier (LM) type specification test for semiparametric conditional mean
models. Consistency is achieved by turning a conditional moment restriction
into a growing number of unconditional moment restrictions using series
methods. The proposed test statistic is straightforward to compute and is
asymptotically standard normal under the null. Compared with the earlier
literature on series-based specification tests in parametric models, I rely on
the projection property of series estimators and derive a different
normalization of the test statistic. Compared with the recent test in Gupta
(2018), I use a different way of accounting for heteroskedasticity. I
demonstrate using Monte Carlo studies that my test has superior finite sample
performance compared with the existing tests. I apply the test to one of the
semiparametric gasoline demand specifications from Yatchew and No (2001) and
find no evidence against it.",Stata Journal ,http://arxiv.org/abs/1810.07620v3,stat.CO
462,Mapping the Sahelian Space,"This chapter examines the geographical meaning of the Sahel, its fluid
boundaries, and its spatial dynamics. Unlike other approaches that define the
Sahel as a bioclimatic zone or as an ungoverned area, it shows that the Sahel
is primarily a space of circulation in which uncertainty has historically been
overcome by mobility. The first part of the paper discusses how pre-colonial
empires relied on a network of markets and cities that facilitated trade and
social relationships across the region and beyond. The second part explores
changing regional mobility patterns precipitated by colonial powers and the new
approach they developed to control networks and flows. The third part discusses
the contradiction between the mobile strategies adopted by local herders,
farmers and traders in the Sahel and the territorial development initiatives of
modern states and international donors. Particular attention is paid in the
last section to how the Sahel was progressively redefined through a security
lens.","Statistics & Probability Letters
",http://arxiv.org/abs/1906.02223v1,math.ST
531,"Creation of knowledge through exchanges of knowledge: Evidence from
  Japanese patent data","This study shows evidence for collaborative knowledge creation among
individual researchers through direct exchanges of their mutual differentiated
knowledge. Using patent application data from Japan, the collaborative output
is evaluated according to the quality and novelty of the developed patents,
which are measured in terms of forward citations and the order of application
within their primary technological category, respectively. Knowledge exchange
is shown to raise collaborative productivity more through the extensive margin
(i.e., the number of patents developed) in the quality dimension, whereas it
does so more through the intensive margin in the novelty dimension (i.e.,
novelty of each patent).",Statistics and Probability Letters ,http://arxiv.org/abs/1908.01256v2,math.ST
170,Eliciting the Endowment Effect under Assigned Ownership,"In this study we present evidence that endowment effect can be elicited
merely by assigned ownership. Using Google Customer Survey, we administered a
survey were participants (n=495) were randomly split into 4 groups. Each group
was assigned ownership of either legroom or their ability to recline on an
airline. Using this experiment setup we were able to generate endowment effect,
a 15-20x (at p<0.05) increase between participant's willingness to pay (WTP)
and their willingness to accept (WTA).","Structural Econometric Models: Advances in Econometrics
",http://arxiv.org/abs/1809.08500v2,econ.EM
197,"Verifying the existence of maximum likelihood estimates for generalized
  linear models","A fundamental problem with nonlinear models is that maximum likelihood
estimates are not guaranteed to exist. Though nonexistence is a well known
problem in the binary choice literature, it presents significant challenges for
other models as well and is not as well understood in more general settings.
These challenges are only magnified for models that feature many fixed effects
and other high-dimensional parameters. We address the current ambiguity
surrounding this topic by studying the conditions that govern the existence of
estimates for (pseudo-)maximum likelihood estimators used to estimate a wide
class of generalized linear models (GLMs). We show that some, but not all, of
these GLM estimators can still deliver consistent estimates of at least some of
the linear parameters when these conditions fail to hold. We also demonstrate
how to verify these conditions in models with high-dimensional parameters, such
as panel data models with multiple levels of fixed effects.","Studies and Scientific Researches Economic Edition
",http://arxiv.org/abs/1903.01633v6,econ.GN
124,"Model Selection in Time Series Analysis: Using Information Criteria as
  an Alternative to Hypothesis Testing","The issue of model selection in applied research is of vital importance.
Since the true model in such research is not known, which model should be used
from among various potential ones is an empirical question. There might exist
several competitive models. A typical approach to dealing with this is classic
hypothesis testing using an arbitrarily chosen significance level based on the
underlying assumption that a true null hypothesis exists. In this paper we
investigate how successful this approach is in determining the correct model
for different data generating processes using time series data. An alternative
approach based on more formal model selection techniques using an information
criterion or cross-validation is suggested and evaluated in the time series
environment via Monte Carlo experiments. This paper also explores the
effectiveness of deciding what type of general relation exists between two
variables (e.g. relation in levels or relation in first differences) using
various strategies based on hypothesis testing and on information criteria with
the presence or absence of unit roots.",Studies in Computational Intelligence,http://arxiv.org/abs/1805.08991v1,econ.GN
259,Fixed Effect Estimation of Large T Panel Data Models,"This article reviews recent advances in fixed effect estimation of panel data
models for long panels, where the number of time periods is relatively large.
We focus on semiparametric models with unobserved individual and time effects,
where the distribution of the outcome variable conditional on covariates and
unobserved effects is specified parametrically, while the distribution of the
unobserved effects is left unrestricted. Compared to existing reviews on long
panels (Arellano and Hahn 2007; a section in Arellano and Bonhomme 2011) we
discuss models with both individual and time effects, split-panel Jackknife
bias corrections, unbalanced panels, distribution and quantile effects, and
other extensions. Understanding and correcting the incidental parameter bias
caused by the estimation of many fixed effects is our main focus, and the
unifying theme is that the order of this bias is given by the simple formula
p/n for all models discussed, with p the number of estimated parameters and n
the total sample size.",Sustainability,http://arxiv.org/abs/1709.08980v2,econ.GN
492,EMU and ECB Conflicts,"In dynamical framework the conflict between government and the central bank
according to the exchange Rate of payment of fixed rates and fixed rates of
fixed income (EMU) convergence criteria such that the public debt / GDP ratio
The method consists of calculating private public debt management in a public
debt management system purpose there is no mechanism to allow naturally for
this adjustment.",Sustainability,http://arxiv.org/abs/1807.08097v1,econ.GN
437,"The structure of two-valued strategy-proof social choice functions with
  indifference","We give a structure theorem for all coalitionally strategy-proof social
choice functions whose range is a subset of cardinality two of a given larger
set of alternatives.
  We provide this in the case where the voters/agents are allowed to express
indifference and the domain consists of profiles of preferences over a society
of arbitrary cardinality. The theorem, that takes the form of a representation
formula, can be used to construct all functions under consideration.",Sustainability ,http://arxiv.org/abs/2002.06341v2,cs.CY
539,"The inverted U-shaped effect of urban hotspots spatial compactness on
  urban economic growth","The compact city, as a sustainable concept, is intended to augment the
efficiency of urban function. However, previous studies have concentrated more
on morphology than on structure. The present study focuses on urban structural
elements, i.e., urban hotspots consisting of high-density and high-intensity
socioeconomic zones, and explores the economic performance associated with
their spatial structure. We use nighttime luminosity (NTL) data and the Loubar
method to identify and extract the hotspot and ultimately draw two conclusions.
First, with population increasing, the hotspot number scales sublinearly with
an exponent of approximately 0.50~0.55, regardless of the location in China,
the EU or the US, while the intersect values are totally different, which is
mainly due to different economic developmental level. Secondly, we demonstrate
that the compactness of hotspots imposes an inverted U-shaped influence on
economic growth, which implies that an optimal compactness coefficient does
exist. These findings are helpful for urban planning.","Technological Forecasting and Social Change
",http://arxiv.org/abs/1908.05530v1,cs.SI
130,Strictly strategy-proof auctions,"A strictly strategy-proof mechanism is one that asks agents to use strictly
dominant strategies. In the canonical one-dimensional mechanism design setting
with private values, we show that strict strategy-proofness is equivalent to
strict monotonicity plus the envelope formula, echoing a well-known
characterisation of (weak) strategy-proofness. A consequence is that
strategy-proofness can be made strict by an arbitrarily small modification, so
that strictness is 'essentially for free'.","Technological Forecasting and Social Change
",http://arxiv.org/abs/1807.11864v4,econ.GN
331,Cointegration in functional autoregressive processes,"This paper defines the class of $\mathcal{H}$-valued autoregressive (AR)
processes with a unit root of finite type, where $\mathcal{H}$ is an infinite
dimensional separable Hilbert space, and derives a generalization of the
Granger-Johansen Representation Theorem valid for any integration order
$d=1,2,\dots$. An existence theorem shows that the solution of an AR with a
unit root of finite type is necessarily integrated of some finite integer $d$
and displays a common trends representation with a finite number of common
stochastic trends of the type of (cumulated) bilateral random walks and an
infinite dimensional cointegrating space. A characterization theorem clarifies
the connections between the structure of the AR operators and $(i)$ the order
of integration, $(ii)$ the structure of the attractor space and the
cointegrating space, $(iii)$ the expression of the cointegrating relations, and
$(iv)$ the Triangular representation of the process. Except for the fact that
the number of cointegrating relations that are integrated of order 0 is
infinite, the representation of $\mathcal{H}$-valued ARs with a unit root of
finite type coincides with that of usual finite dimensional VARs, which
corresponds to the special case $\mathcal{H}=\mathbb{R}^p$.","Technological Forecasting and Social Change
",http://arxiv.org/abs/1712.07522v2,econ.GN
63,Cartel Stability under Quality Differentiation,"This note considers cartel stability when the cartelized products are
vertically differentiated. If market shares are maintained at pre-collusive
levels, then the firm with the lowest competitive price-cost margin has the
strongest incentive to deviate from the collusive agreement. The lowest-quality
supplier has the tightest incentive constraint when the difference in unit
production costs is sufficiently small.",Telematics and Informatics ,http://arxiv.org/abs/1812.10293v1,econ.GN
553,"The Incidental Parameters Problem in Testing for Remaining Cross-section
  Correlation","In this paper we consider the properties of the Pesaran (2004, 2015a) CD test
for cross-section correlation when applied to residuals obtained from panel
data models with many estimated parameters. We show that the presence of
period-specific parameters leads the CD test statistic to diverge as length of
the time dimension of the sample grows. This result holds even if cross-section
dependence is correctly accounted for and hence constitutes an example of the
Incidental Parameters Problem. The relevance of this problem is investigated
both for the classical Time Fixed Effects estimator as well as the Common
Correlated Effects estimator of Pesaran (2006). We suggest a weighted CD test
statistic which re-establishes standard normal inference under the null
hypothesis. Given the widespread use of the CD test statistic to test for
remaining cross-section correlation, our results have far reaching implications
for empirical researchers.",Test Engineering and Management,http://arxiv.org/abs/1810.03715v4,cs.IR
593,Learning from Manipulable Signals,"We study a dynamic stopping game between a principal and an agent. The agent
is privately informed about his type. The principal learns about the agent's
type from a noisy performance measure, which can be manipulated by the agent
via a costly and hidden action. We fully characterize the unique Markov
equilibrium of this game. We find that terminations/market crashes are often
preceded by a spike in (expected) performance. Our model also predicts that,
due to endogenous signal manipulation, too much transparency can inhibit
learning. As the players get arbitrarily patient, the principal elicits no
useful information from the observed signal.",Thammasat Economic Journal,http://arxiv.org/abs/2007.08762v4,econ.GN
565,Randomization Tests for Equality in Dependence Structure,"We develop a new statistical procedure to test whether the dependence
structure is identical between two groups. Rather than relying on a single
index such as Pearson's correlation coefficient or Kendall's Tau, we consider
the entire dependence structure by investigating the dependence functions
(copulas). The critical values are obtained by a modified randomization
procedure designed to exploit asymptotic group invariance conditions.
Implementation of the test is intuitive and simple, and does not require any
specification of a tuning parameter or weight function. At the same time, the
test exhibits excellent finite sample performance, with the null rejection
rates almost equal to the nominal level even when the sample size is extremely
small. Two empirical applications concerning the dependence between income and
consumption, and the Brexit effect on European financial market integration are
provided.","Thammasat Review of Economic and Social Policy 
",http://arxiv.org/abs/1811.02105v1,econ.GN
533,"Evaluating Pest Management Strategies: A Robust Method and its
  Application to Strawberry Disease Management","Farmers use pesticides to reduce yield losses. The efficacies of pesticide
treatments are often evaluated by analyzing the average treatment effects and
risks. The stochastic efficiency with respect to a function is often employed
in such evaluations through ranking the certainty equivalents of each
treatment. The main challenge of using this method is gathering an adequate
number of observations to produce results with statistical power. However, in
many cases, only a limited number of trials are replicated in field
experiments, leaving an inadequate number of observations. In addition, this
method focuses only on the farmer's profit without incorporating the impact of
disease pressure on yield and profit. The objective of our study is to propose
a methodology to address the issue of an insufficient number of observations
using simulations and take into account the effect of disease pressure on yield
through a quantile regression model. We apply this method to the case of
strawberry disease management in Florida.",The American Journal of Humanities and Social Sciences Research,http://arxiv.org/abs/1908.01808v1,econ.GN
261,Inference on Estimators defined by Mathematical Programming,"We propose an inference procedure for estimators defined by mathematical
programming problems, focusing on the important special cases of linear
programming (LP) and quadratic programming (QP). In these settings, the
coefficients in both the objective function and the constraints of the
mathematical programming problem may be estimated from data and hence involve
sampling error. Our inference approach exploits the characterization of the
solutions to these programming problems by complementarity conditions; by doing
so, we can transform the problem of doing inference on the solution of a
constrained optimization problem (a non-standard inference problem) into one
involving inference based on a set of inequalities with pre-estimated
coefficients, which is much better understood. We evaluate the performance of
our procedure in several Monte Carlo simulations and an empirical application
to the classic portfolio selection problem in finance.",The American Statistician ,http://arxiv.org/abs/1709.09115v1,econ.EM
38,Nonparametric Identification in Index Models of Link Formation,"We consider an index model of dyadic link formation with a homophily effect
index and a degree heterogeneity index. We provide nonparametric identification
results in a single large network setting for the potentially nonparametric
homophily effect function, the realizations of unobserved individual fixed
effects and the unknown distribution of idiosyncratic pairwise shocks, up to
normalization, for each possible true value of the unknown parameters. We
propose a novel form of scale normalization on an arbitrary interquantile
range, which is not only theoretically robust but also proves particularly
convenient for the identification analysis, as quantiles provide direct
linkages between the observable conditional probabilities and the unknown index
values. We then use an inductive ""in-fill and out-expansion"" algorithm to
establish our main results, and consider extensions to more general settings
that allow nonseparable dependence between homophily and degree heterogeneity,
as well as certain extents of network sparsity and weaker assumptions on the
support of unobserved heterogeneity. As a byproduct, we also propose a concept
called ""modeling equivalence"" as a refinement of ""observational equivalence"",
and use it to provide a formal discussion about normalization, identification
and their interplay with counterfactuals.","The BE Journal of Theoretical Economics 
 ",http://arxiv.org/abs/1710.11230v5,econ.TH
111,"How does monetary policy affect income inequality in Japan? Evidence
  from grouped data","We examine the effects of monetary policy on income inequality in Japan using
a novel econometric approach that jointly estimates the Gini coefficient based
on micro-level grouped data of households and the dynamics of macroeconomic
quantities. Our results indicate different effects on income inequality for
different types of households: A monetary tightening increases inequality when
income data is based on households whose head is employed (workers'
households), while the effect reverses over the medium term when considering a
broader definition of households. Differences in the relative strength of the
transmission channels can account for this finding. Finally we demonstrate that
the proposed joint estimation strategy leads to more informative inference
while results based on the frequently used two-step estimation approach yields
inconclusive results.",The Canadian Journal of Statistics ,http://arxiv.org/abs/1803.08868v2,econ.EM
212,Persuasion Meets Delegation,"A principal can restrict an agent's information (the persuasion problem) or
restrict an agent's discretion (the delegation problem). We show that these
problems are generally equivalent - solving one solves the other. We use tools
from the persuasion literature to generalize and extend many results in the
delegation literature, as well as to address novel delegation problems, such as
monopoly regulation with a participation constraint.",The Econometrics Journal,http://arxiv.org/abs/1902.02628v1,econ.GN
48,How Can We Induce More Women to Competitions?,"Why women avoid participating in a competition and how can we encourage them
to participate in it? In this paper, we investigate how social image concerns
affect women's decision to compete. We first construct a theoretical model and
show that participating in a competition, even under affirmative action
policies favoring women, is costly for women under public observability since
it deviates from traditional female gender norms, resulting in women's low
appearance in competitive environments. We propose and theoretically show that
introducing prosocial incentives in the competitive environment is effective
and robust to public observability since (i) it induces women who are
intrinsically motivated by prosocial incentives to the competitive environment
and (ii) it makes participating in a competition not costly for women from
social image point of view. We conduct a laboratory experiment where we
randomly manipulate the public observability of decisions to compete and test
our theoretical predictions. The results of the experiment are fairly
consistent with our theoretical predictions. We suggest that when designing
policies to promote gender equality in competitive environments, using
prosocial incentives through company philanthropy or other social
responsibility policies, either as substitutes or as complements to traditional
affirmative action policies, could be promising.",The European Physical Journal B,http://arxiv.org/abs/1801.10518v1,econ.EM
75,Social security and labor absenteeism in a regional health service,"Background: Absenteism can generate important economic costs. Aim: To analyze
the determinants of the time off work for sick leaves granted to workers of a
regional health service. Material and Methods: Information about 2033
individuals, working at a health service, that were granted at least one sick
leave during 2012, was analyzed. Personal identification was censored. Special
emphasis was given to the type of health insurance system of the workers
(public or private). Results: Workers ascribed to the Chilean public health
insurance system (FONASA) had 11 days more off work than their counterparts
ascribed to private health insurance systems. A higher amount of time off work
was observed among older subjects and women. Conclusions: Age, gender and the
type of health insurance system influence the number of day off work due to
sick leaves.",The International Journal of Business Management and Technology,http://arxiv.org/abs/1812.08091v1,econ.GN
389,A Normative Dual-value Theory for Bitcoin and other Cryptocurrencies,"Bitcoin as well as other cryptocurrencies are all plagued by the impact from
bifurcation. Since the marginal cost of bifurcation is theoretically zero, it
causes the coin holders to doubt on the existence of the coin's intrinsic
value. This paper suggests a normative dual-value theory to assess the
fundamental value of Bitcoin. We draw on the experience from the art market,
where similar replication problems are prevalent. The idea is to decompose the
total value of a cryptocurrency into two parts: one is its art value and the
other is its use value. The tradeoff between these two values is also analyzed,
which enlightens our proposal of an image coin for Bitcoin so as to elevate its
use value without sacrificing its art value. To show the general validity of
the dual-value theory, we also apply it to evaluate the prospects of four major
cryptocurrencies. We find this framework is helpful for both the investors and
the exchanges to examine a new coin's value when it first appears in the
market.",The Journal of Derivatives ,http://arxiv.org/abs/1904.05028v1,q-fin.PR
605,Taxing dissent: The impact of a social media tax in Uganda,"We examine the impact of a new tool for suppressing the expression of
dissent---a daily tax on social media use. Using a synthetic control framework,
we estimate that the tax reduced the number of georeferenced Twitter users in
Uganda by 13 percent. The estimated treatment effects are larger for poorer and
less frequent users. Despite the overall decline in Twitter use, tweets
referencing collective action increased by 31 percent and observed protests
increased by 47 percent. These results suggest that taxing social media use may
not be an effective tool for reducing political dissent.",The Journal of Derivatives ,http://arxiv.org/abs/1909.04107v1,q-fin.PR
132,The Indirect Cost of Information,"We study the indirect cost of information from sequential information cost
minimization. A key sub-additivity condition, together with monotonicity
equivalently characterizes the class of indirect cost functions generated from
any direct information cost. Adding an extra (uniform) posterior separability
condition equivalently characterizes the indirect cost generated from any
direct cost favoring incremental evidences. We also provide the necessary and
sufficient condition when prior independent direct cost generates posterior
separable indirect cost.",The Journal of the Economics of Ageing,http://arxiv.org/abs/1809.00697v2,econ.GN
87,"Using Artificial Intelligence to Recapture Norms: Did #metoo change
  gender norms in Sweden?","Norms are challenging to define and measure, but this paper takes advantage
of text data and the recent development in machine learning to create an
encompassing measure of norms. An LSTM neural network is trained to detect
gendered language. The network functions as a tool to create a measure on how
gender norms changes in relation to the Metoo movement on Swedish Twitter. This
paper shows that gender norms on average are less salient half a year after the
date of the first appearance of the hashtag #Metoo. Previous literature
suggests that gender norms change over generations, but the current result
suggests that norms can change in the short run.",The Review of Economic Studies,http://arxiv.org/abs/1903.00690v1,econ.TH
187,Multivariate Fractional Components Analysis,"We propose a setup for fractionally cointegrated time series which is
formulated in terms of latent integrated and short-memory components. It
accommodates nonstationary processes with different fractional orders and
cointegration of different strengths and is applicable in high-dimensional
settings. In an application to realized covariance matrices, we find that
orthogonal short- and long-memory components provide a reasonable fit and
competitive out-of-sample performance compared to several competing methods.",The Review of Economic Studies,http://arxiv.org/abs/1812.09149v2,econ.EM
91,"Entrepreneurship, Institutions, and Economic Growth: Does the Level of
  Development Matter?","Entrepreneurship is often touted for its ability to generate economic growth.
Through the creative-destructive process, entrepreneurs are often able to
innovate and outperform incumbent organizations, all of which is supposed to
lead to higher employment and economic growth. Although some empirical evidence
supports this logic, it has also been the subject of recent criticisms.
Specifically, entrepreneurship does not lead to growth in developing countries;
it only does in more developed countries with higher income levels. Using
Global Entrepreneurship Monitor data for a panel of 83 countries from 2002 to
2014, we examine the contribution of entrepreneurship towards economic growth.
Our evidence validates earlier studies findings but also exposes previously
undiscovered findings. That is, we find that entrepreneurship encourages
economic growth but not in developing countries. In addition, our evidence
finds that the institutional environment of the country, as measured by GEM
Entrepreneurial Framework Conditions, only contributes to economic growth in
more developed countries but not in developing countries. These findings have
important policy implications. Namely, our evidence contradicts policy
proposals that suggest entrepreneurship and the adoption of pro-market
institutions that support it to encourage economic growth in developing
countries. Our evidence suggests these policy proposals will be unlikely to
generate the economic growth desired.",The Stata Journal,http://arxiv.org/abs/1903.02934v1,econ.EM
141,Optimal policy design for the sugar tax,"Healthy nutrition promotions and regulations have long been regarded as a
tool for increasing social welfare. One of the avenues taken in the past decade
is sugar consumption regulation by introducing a sugar tax. Such a tax
increases the price of extensive sugar containment in products such as soft
drinks. In this article we consider a typical problem of optimal regulatory
policy design, where the task is to determine the sugar tax rate maximizing the
social welfare. We model the problem as a sequential game represented by the
three-level mathematical program. On the upper level, the government decides
upon the tax rate. On the middle level, producers decide on the product
pricing. On the lower level, consumers decide upon their preferences towards
the products. While the general problem is computationally intractable, the
problem with a few product types is polynomially solvable, even for an
arbitrary number of heterogeneous consumers. This paper presents a simple,
intuitive and easily implementable framework for computing optimal sugar tax in
a market with a few products. This resembles the reality as the soft drinks,
for instance, are typically categorized in either regular or no-sugar drinks,
e.g. Coca-Cola and Coca-Cola Zero. We illustrate the algorithm using an example
based on the real data and draw conclusions for a specific local market.",Theoretical and Applied Economics,http://arxiv.org/abs/1810.07243v1,econ.GN
142,The Losses from Integration in Matching Markets can be Large,"Although the integration of two-sided matching markets using stable
mechanisms generates expected gains from integration, I show that there are
worst-case scenarios in which these are negative. The losses from integration
can be large enough that the average rank of an agent's spouse decreases by
37.5% of the length of their preference list in any stable matching mechanism.",Theoretical and Applied Economics,http://arxiv.org/abs/1810.10287v1,econ.GN
143,Revealed Stochastic Preference: A One-Paragraph Proof and Generalization,"McFadden and Richter (1991) and later McFadden (2005) show that the Axiom of
Revealed Stochastic Preference characterizes rationalizability of choice
probabilities through random utility models on finite universal choice spaces.
This note proves the result in one short, elementary paragraph and extends it
to set valued choice. The latter requires a different axiom than is reported in
McFadden (2005).",Theoretical and Applied Economics,http://arxiv.org/abs/1810.10604v2,econ.GN
146,"Characterizing Permissibility, Proper Rationalizability, and Iterated
  Admissibility by Incomplete Information","We characterize three interrelated concepts in epistemic game theory:
permissibility, proper rationalizability, and iterated admissibility. We define
the lexicographic epistemic model for a game with incomplete information. Based
on it, we give two groups of characterizations. The first group characterizes
permissibility and proper rationalizability. The second group characterizes
permissibility in an alternative way and iterated admissibility. In each group,
the conditions for the latter are stronger than those for the former, which
corresponds to the fact that proper rationalizability and iterated
admissibility are two (compatible) refinements of permissibility within the
complete information framework. The intrinsic difference between the two groups
are the role of rationality: the first group does not need it, while the second
group does.",Theoretical and Applied Economics,http://arxiv.org/abs/1811.01933v1,econ.GN
288,Competing to Persuade a Rationally Inattentive Agent,"Firms strategically disclose product information in order to attract
consumers, but recipients often find it costly to process all of it, especially
when products have complex features. We study a model of competitive
information disclosure by two senders, in which the receiver may garble each
sender's experiment, subject to a cost increasing in the informativeness of the
garbling. For a large class of parameters, it is an equilibrium for the senders
to provide the receiver's first best level of information - i.e. as much as she
would learn if she herself controlled information provision. Information on one
sender substitutes for information on the other, which nullifies the
profitability of a unilateral provision of less information. Thus, we provide a
novel channel through which competition with attention costs encourages
information disclosure.",Theoretical Economics,http://arxiv.org/abs/1907.09255v2,q-fin.EC
225,"Disability for HIV and Disincentives for Health: The Impact of South
  Africa's Disability Grant on HIV/AIDS Recovery","South Africa's disability grants program is tied to its HIV/AIDS recovery
program, such that individuals who are ill enough may qualify. Qualification is
historically tied to a CD4 count of 200 cells/mm3, which improve when a person
adheres to antiretroviral therapy. This creates a potential unintended
consequence where poor individuals, faced with potential loss of their income,
may choose to limit their recovery through non-adherence. To test for
manipulation caused by grant rules, we identify differences in disability grant
recipients and non-recipients' rate of CD4 recovery around the qualification
threshold, implemented as a fixed-effects difference-in-difference around the
threshold. We use data from the Africa Health Research Institute Demographic
and Health Surveillance System (AHRI DSS) in rural KwaZulu-Natal, South Africa,
utilizing DG status and laboratory CD4 count records for 8,497 individuals to
test whether there are any systematic differences in CD4 recover rates among
eligible patients. We find that disability grant threshold rules caused
recipients to have a relatively slower CD4 recovery rate of about 20-30
cells/mm3/year, or a 20% reduction in the speed of recovery around the
threshold.",Theoretical Economics ,http://arxiv.org/abs/1810.01971v1,econ.TH
11,Banks as Tanks: A Continuous-Time Model of Financial Clearing,"We present a simple continuous-time model of clearing in financial networks.
Financial firms are represented as ""tanks"" filled with fluid (money), flowing
in and out. Once ""pipes"" connecting ""tanks"" are open, the system reaches the
clearing payment vector in finite time. This approach provides a simple
recursive solution to a classical static model of financial clearing in
bankruptcy, and suggests a practical payment mechanism. With sufficient
resources, a system of mutual obligations can be restructured into an
equivalent system that has a cascade structure: there is a group of banks that
paid off their debts, another group that owes money only to banks in the first
group, and so on. Technically, we use the machinery of Markov chains to analyze
evolution of a deterministic dynamical system.",Theory Methodology Practice,http://arxiv.org/abs/1705.05943v3,econ.GN
211,Bayesian Elicitation,"How can a receiver design an information structure in order to elicit
information from a sender? We study how a decision-maker can acquire more
information from an agent by reducing her own ability to observe what the agent
transmits. Intuitively, when the two parties' preferences are not perfectly
aligned, this garbling relaxes the sender's concern that the receiver will use
her information to the sender's disadvantage. We characterize the optimal
information structure for the receiver. The main result is that under broad
conditions, the receiver can do just as well as if she could commit to a rule
mapping the sender's message to actions: information design is just as good as
full commitment. Similarly, we show that these conditions guarantee that ex
ante information acquisition always benefits the receiver, even though this
learning might actually lower the receiver's expected payoff in the absence of
garbling. We illustrate these effects in a range of economically relevant
examples.",Topology and Its Applications ,http://arxiv.org/abs/1902.00976v2,econ.TH
589,"Anonymous, non-manipulable, binary social choice","Let V be a finite society whose members express weak orderings (hence also
indifference, possibly) about two alternatives. We show a simple representation
formula that is valid for all, and only, anonymous, non-manipulable, binary
social choice functions on V . The number of such functions is $2^{n+1}$ if V
contains $n$ agents.",Tourism Management Perspectives ,http://arxiv.org/abs/2007.01552v1,cs.CL
121,"Data-Driven Investment Decision-Making: Applying Moore's Law and
  S-Curves to Business Strategies","This paper introduces a method for linking technological improvement rates
(i.e. Moore's Law) and technology adoption curves (i.e. S-Curves). There has
been considerable research surrounding Moore's Law and the generalized versions
applied to the time dependence of performance for other technologies. The prior
work has culminated with methodology for quantitative estimation of
technological improvement rates for nearly any technology. This paper examines
the implications of such regular time dependence for performance upon the
timing of key events in the technological adoption process. We propose a simple
crossover point in performance which is based upon the technological
improvement rates and current level differences for target and replacement
technologies. The timing for the cross-over is hypothesized as corresponding to
the first 'knee'? in the technology adoption ""S-curve"" and signals when the
market for a given technology will start to be rewarding for innovators. This
is also when potential entrants are likely to intensely experiment with
product-market fit and when the competition to achieve a dominant design
begins. This conceptual framework is then back-tested by examining two
technological changes brought about by the internet, namely music and video
transmission. The uncertainty analysis around the cases highlight opportunities
for organizations to reduce future technological uncertainty. Overall, the
results from the case studies support the reliability and utility of the
conceptual framework in strategic business decision-making with the caveat that
while technical uncertainty is reduced, it is not eliminated.",Transactions of The Royal Society of Tropical Medicine and Hygiene,http://arxiv.org/abs/1805.06339v1,econ.GN
545,Spatial pattern and city size distribution,"Many large cities are found at locations with certain first nature
advantages. Yet, those exogenous locational features may not be the most potent
forces governing the spatial pattern of cities. In particular, population size,
spacing and industrial composition of cities exhibit simple, persistent and
monotonic relationships. Theories of economic agglomeration suggest that this
regularity is a consequence of interactions between endogenous agglomeration
and dispersion forces. This paper reviews the extant formal models that explain
the spatial pattern together with the size distribution of cities, and
discusses the remaining research questions to be answered in this literature.
To obtain results about explicit spatial patterns of cities, a model needs to
depart from the most popular two-region and systems-of-cities frameworks in
urban and regional economics in which there is no variation in interregional
distance. This is one of the major reasons that only few formal models have
been proposed in this literature. To draw implications as much as possible from
the extant theories, this review involves extensive discussions on the behavior
of the many-region extension of these models. The mechanisms that link the
spatial pattern of cities and the diversity in city sizes are also discussed in
detail.",Transportation ,http://arxiv.org/abs/1908.09706v1,stat.AP
398,"A Multicriteria Decision Making Approach to Study the Barriers to the
  Adoption of Autonomous Vehicles","The automation technology is emerging, but the adoption rate of autonomous
vehicles (AV) will largely depend upon how policymakers and the government
address various challenges such as public acceptance and infrastructure
development. This study proposes a five-step method to understand these
barriers to AV adoption. First, based on a literature review followed by
discussions with experts, ten barriers are identified. Second, the opinions of
eighteen experts from industry and academia regarding inter-relations between
these barriers are recorded. Third, a multicriteria decision making (MCDM)
technique, the grey-based Decision-making Trial and Evaluation Laboratory
(Grey-DEMATEL), is applied to characterize the structure of relationships
between the barriers. Fourth, robustness of the results is tested using
sensitivity analysis. Fifth, the key results are depicted in a causal loop
diagram (CLD), a systems thinking approach, to comprehend cause-and-effect
relationships between the barriers. The results indicate that the lack of
customer acceptance (LCA) is the most prominent barrier, the one which should
be addressed at the highest priority. The CLD suggests that LCA can be rather
mitigated by addressing two other prominent, yet more tangible, barriers --
lack of industry standards and the absence of regulations and certifications.
The study's overarching contribution thus lies in bringing to fore multiple
barriers to AV adoption and their potential influences on each other. Moreover,
the insights from this study can help associations related to AVs prioritize
their endeavors to expedite AV adoption. From the methodological perspective,
this is the first study in transportation literature that integrates
Grey-DEMATEL with systems thinking.",Transportation Research Part A Policy and Practice,http://arxiv.org/abs/1904.12051v3,econ.GN
425,Perfect bidder collusion through bribe and request,"We study collusion in a second-price auction with two bidders in a dynamic
environment. One bidder can make a take-it-or-leave-it collusion proposal,
which consists of both an offer and a request of bribes, to the opponent. We
show that there always exists a robust equilibrium in which the collusion
success probability is one. In the equilibrium, for each type of initiator the
expected payoff is generally higher than the counterpart in any robust
equilibria of the single-option model (Es\""{o} and Schummer (2004)) and any
other separating equilibria in our model.",Transportation Research Part B Methodological,http://arxiv.org/abs/1912.03607v2,cs.CY
269,Startups and Stanford University,"Startups have become in less than 50 years a major component of innovation
and economic growth. Silicon Valley has been the place where the startup
phenomenon was the most obvious and Stanford University was a major component
of that success. Companies such as Google, Yahoo, Sun Microsystems, Cisco,
Hewlett Packard had very strong links with Stanford but even these vary famous
success stories cannot fully describe the richness and diversity of the
Stanford entrepreneurial activity. This report explores the dynamics of more
than 5000 companies founded by Stanford University alumni and staff, through
their value creation, their field of activities, their growth patterns and
more. The report also explores some features of the founders of these companies
such as their academic background or the number of years between their Stanford
experience and their company creation.",Transportation Research Part B Methodological,http://arxiv.org/abs/1711.00644v1,econ.EM
478,"Transaction costs and institutional change of trade litigations in
  Bulgaria","The methods of new institutional economics for identifying the transaction
costs of trade litigations in Bulgaria are used in the current paper. For the
needs of the research, an indicative model, measuring this type of costs on
microeconomic level, is applied in the study. The main purpose of the model is
to forecast the rational behavior of trade litigation parties in accordance
with the transaction costs in the process of enforcing the execution of the
signed commercial contract. The application of the model is related to the more
accurate measurement of the transaction costs on microeconomic level, which
fact could lead to better prediction and management of these costs in order
market efficiency and economic growth to be achieved. In addition, it is made
an attempt to be analysed the efficiency of the institutional change of the
commercial justice system and the impact of the reform of the judicial system
over the economic turnover. The augmentation or lack of reduction of the
transaction costs in trade litigations would mean inefficiency of the reform of
the judicial system. JEL Codes: O43, P48, D23, K12",Transportation Research Part B Methodological,http://arxiv.org/abs/1807.03034v1,stat.ML
329,Aggregating Google Trends: Multivariate Testing and Analysis,"Web search data are a valuable source of business and economic information.
Previous studies have utilized Google Trends web search data for economic
forecasting. We expand this work by providing algorithms to combine and
aggregate search volume data, so that the resulting data is both consistent
over time and consistent between data series. We give a brand equity example,
where Google Trends is used to analyze shopping data for 100 top ranked brands
and these data are used to nowcast economic variables. We describe the
importance of out of sample prediction and show how principal component
analysis (PCA) can be used to improve the signal to noise ratio and prevent
overfitting in nowcasting models. We give a finance example, where exploratory
data analysis and classification is used to analyze the relationship between
Google Trends searches and stock prices.","Transportation Research Part C Emerging Technologies
",http://arxiv.org/abs/1712.03152v2,econ.GN
64,Equivalent Choice Functions and Stable Mechanisms,"We study conditions for the existence of stable and group-strategy-proof
mechanisms in a many-to-one matching model with contracts if students'
preferences are monotone in contract terms. We show that ""equivalence"",
properly defined, to a choice profile under which contracts are substitutes and
the law of aggregate holds is a necessary and sufficient condition for the
existence of a stable and group-strategy-proof mechanism.
  Our result can be interpreted as a (weak) embedding result for choice
functions under which contracts are observable substitutes and the observable
law of aggregate demand holds.","Transportation Research Part C Emerging Technologies
",http://arxiv.org/abs/1812.10326v3,econ.GN
231,"Contemporary facets of business successes among leading companies,
  operating in Bulgaria","The current article unveils and analyzes some important factors, influencing
diversity in strategic decision-making approaches in local companies.
Researcher's attention is oriented to survey important characteristics of the
strategic moves, undertaken by leading companies in Bulgaria.",Transportation Research Part D Transport and Environment,http://arxiv.org/abs/1810.02622v1,econ.GN
569,Bootstrapping Structural Change Tests,"This paper analyses the use of bootstrap methods to test for parameter change
in linear models estimated via Two Stage Least Squares (2SLS). Two types of
test are considered: one where the null hypothesis is of no change and the
alternative hypothesis involves discrete change at k unknown break-points in
the sample; and a second test where the null hypothesis is that there is
discrete parameter change at l break-points in the sample against an
alternative in which the parameters change at l + 1 break-points. In both
cases, we consider inferences based on a sup-Wald-type statistic using either
the wild recursive bootstrap or the wild fixed bootstrap. We establish the
asymptotic validity of these bootstrap tests under a set of general conditions
that allow the errors to exhibit conditional and/or unconditional
heteroskedasticity, and report results from a simulation study that indicate
the tests yield reliable inferences in the sample sizes often encountered in
macroeconomics. The analysis covers the cases where the first-stage estimation
of 2SLS involves a model whose parameters are either constant or themselves
subject to discrete parameter change. If the errors exhibit unconditional
heteroskedasticity and/or the reduced form is unstable then the bootstrap
methods are particularly attractive because the limiting distributions of the
test statistics are not pivotal.","Transportation Research Part D Transport and Environment 
 ",http://arxiv.org/abs/1811.04125v1,cs.LG
236,"Aggressive Economic Incentives and Physical Activity: The Role of Choice
  and Technology Decision Aids","Aggressive incentive schemes that allow individuals to impose economic
punishment on themselves if they fail to meet health goals present a promising
approach for encouraging healthier behavior. However, the element of choice
inherent in these schemes introduces concerns that only non-representative
sectors of the population will select aggressive incentives, leaving value on
the table for those who don't opt in. In a field experiment conducted over a 29
week period on individuals wearing Fitbit activity trackers, we find modest and
short lived increases in physical activity for those provided the choice of
aggressive incentives. In contrast, we find significant and persistent
increases for those assigned (oftentimes against their stated preference) to
the same aggressive incentives. The modest benefits for those provided a choice
seems to emerge because those who benefited most from the aggressive incentives
were the least likely to choose them, and it was those who did not need them
who opted in. These results are confirmed in a follow up lab experiment. We
also find that benefits to individuals assigned to aggressive incentives were
pronounced if they also updated their step target in the Fitbit mobile
application to match the new activity goal we provided them. Our findings have
important implications for incentive based interventions to improve health
behavior. For firms and policy makers, our results suggest that one effective
strategy for encouraging sustained healthy behavior combines exposure to
aggressive incentive schemes to jolt individuals out of their comfort zones
with technology decision aids that help individuals sustain this behavior after
incentives end.",Travel Behaviour and Society ,http://arxiv.org/abs/1810.06698v2,econ.GN
548,Coase Meets Bellman: Dynamic Programming for Production Networks,"We show that competitive equilibria in a range of models related to
production networks can be recovered as solutions to dynamic programs. Although
these programs fail to be contractive, we prove that they are tractable. As an
illustration, we treat Coase's theory of the firm, equilibria in production
chains with transaction costs, and equilibria in production networks with
multiple partners. We then show how the same techniques extend to other
equilibrium and decision problems, such as the distribution of management
layers within firms and the spatial distribution of cities.",Urban Transformations ,http://arxiv.org/abs/1908.10557v3,econ.GN
25,Machine Learning for Dynamic Discrete Choice,"Dynamic discrete choice models often discretize the state vector and restrict
its dimension in order to achieve valid inference. I propose a novel two-stage
estimator for the set-identified structural parameter that incorporates a
high-dimensional state space into the dynamic model of imperfect competition.
In the first stage, I estimate the state variable's law of motion and the
equilibrium policy function using machine learning tools. In the second stage,
I plug the first-stage estimates into a moment inequality and solve for the
structural parameter. The moment function is presented as the sum of two
components, where the first one expresses the equilibrium assumption and the
second one is a bias correction term that makes the sum insensitive (i.e.,
orthogonal) to first-stage bias. The proposed estimator uniformly converges at
the root-N rate and I use it to construct confidence regions. The results
developed here can be used to incorporate high-dimensional state space into
classic dynamic discrete choice models, for example, those considered in Rust
(1987), Bajari et al. (2007), and Scott (2013).",Vanguard scientific instruments in management journal,http://arxiv.org/abs/1808.02569v2,econ.GN
26,A Unified Framework for Efficient Estimation of General Treatment Models,"This paper presents a weighted optimization framework that unifies the
binary,multi-valued, continuous, as well as mixture of discrete and continuous
treatment, under the unconfounded treatment assignment. With a general loss
function, the framework includes the average, quantile and asymmetric least
squares causal effect of treatment as special cases. For this general
framework, we first derive the semiparametric efficiency bound for the causal
effect of treatment, extending the existing bound results to a wider class of
models. We then propose a generalized optimization estimation for the causal
effect with weights estimated by solving an expanding set of equations. Under
some sufficient conditions, we establish consistency and asymptotic normality
of the proposed estimator of the causal effect and show that the estimator
attains our semiparametric efficiency bound, thereby extending the existing
literature on efficient estimation of causal effect to a wider class of
applications. Finally, we discuss etimation of some causal effect functionals
such as the treatment effect curve and the average outcome. To evaluate the
finite sample performance of the proposed procedure, we conduct a small scale
simulation study and find that the proposed estimation has practical value. To
illustrate the applicability of the procedure, we revisit the literature on
campaign advertise and campaign contributions. Unlike the existing procedures
which produce mixed results, we find no evidence of campaign advertise on
campaign contribution.",Vanguard scientific instruments in management journal,http://arxiv.org/abs/1808.04936v2,econ.GN
29,"Estimation in a Generalization of Bivariate Probit Models with Dummy
  Endogenous Regressors","The purpose of this paper is to provide guidelines for empirical researchers
who use a class of bivariate threshold crossing models with dummy endogenous
variables. A common practice employed by the researchers is the specification
of the joint distribution of the unobservables as a bivariate normal
distribution, which results in a bivariate probit model. To address the problem
of misspecification in this practice, we propose an easy-to-implement
semiparametric estimation framework with parametric copula and nonparametric
marginal distributions. We establish asymptotic theory, including root-n
normality, for the sieve maximum likelihood estimators that can be used to
conduct inference on the individual structural parameters and the average
treatment effect (ATE). In order to show the practical relevance of the
proposed framework, we conduct a sensitivity analysis via extensive Monte Carlo
simulation exercises. The results suggest that the estimates of the parameters,
especially the ATE, are sensitive to parametric specification, while
semiparametric estimation exhibits robustness to underlying data generating
processes. We then provide an empirical illustration where we estimate the
effect of health insurance on doctor visits. In this paper, we also show that
the absence of excluded instruments may result in identification failure, in
contrast to what some practitioners believe.",Vanguard scientific instruments in management journal,http://arxiv.org/abs/1808.05792v2,econ.GN
148,Constrained Information Design,"We provide tools to analyze information design problems subject to
constraints. We do so by showing that the techniques in Le Treust and Tomala
(2019) extend to the case of multiple inequality and equality constraints. This
showcases the power of the results in that paper to analyze problems of
information design subject to constraints. We illustrate our results with
applications to mechanism design with limited commitment (Doval and Skreta,
2020) and persuasion of a privately informed receiver (Kolotilin et al., 2017).",World Development,http://arxiv.org/abs/1811.03588v2,econ.EM
422,The Bretton Woods Experience and ERM,"Historical examination of the Bretton Woods system allows comparisons to be
made with the current evolution of the EMS.",World Economics ,http://arxiv.org/abs/1807.00418v1,q-fin.GN